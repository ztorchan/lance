{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"sdk_docs/","title":"SDK Documentation","text":"<p>Lance provides comprehensive documentation for all the language SDKs.  These auto-generated docs contain detailed information about  all classes, functions, and methods available in each language.</p> <ul> <li>Python</li> <li>Rust</li> <li>Java</li> </ul>"},{"location":"community/","title":"Lance Community Governance","text":"<p>The Lance community is run by volunteers in a collaborative and open way. Its governance is inspired by open source foundations and projects such as ASF, CNCF, and Substrait.</p>"},{"location":"community/#governance-structure","title":"Governance Structure","text":"<p>The Lance community recognizes three tiers of participation:</p>"},{"location":"community/#contributors","title":"Contributors","text":"<p>Everyone who has made a contribution to Lance is a contributor.</p> <p>A \"contribution\" is not limited to code changes. Adopting Lance in personal or company projects, providing bug reports and feature requests, performing code reviews, organizing or planning community gatherings, giving talks, creating and assisting in branding and design, writing documentation, and many other activities are all counted as contributions.</p> <p>All contributions, regardless of form, are valued and greatly appreciated. It is entirely possible to advance through the governance tiers without writing code.</p>"},{"location":"community/#maintainers","title":"Maintainers","text":"<p>A maintainer is a contributor who has made sustained and valuable contributions to the Lance community. Maintainers are recognized for their work and granted various rights to support their ongoing contributions. For more details of the activities, rights, roster and how to become a maintainer, see Maintainers.</p>"},{"location":"community/#project-management-committee-pmc","title":"Project Management Committee (PMC)","text":"<p>A PMC member is a maintainer who has demonstrated leadership in the project. The PMC guides the long-term direction, makes decisions on governance and project changes, and protects the Lance brand. For more details of the activities, rights, roster and how to become a PMC member, see PMC.</p>"},{"location":"community/#roster-information","title":"Roster Information","text":"<p>Maintainer and PMC rosters information follow these guidelines:</p> <ul> <li>Ordering: People in the roster are listed in alphabetical order by last name</li> <li>Self-Report: Personal information such as Affiliation and Ecosystem Roles is self-reported and updated at the individual's discretion.</li> <li>Ecosystem Roles: This field documents the individual's involvement in other open source projects if any. It helps identify potential collaboration and integration pathways with the broader open source ecosystem.</li> </ul>"},{"location":"community/#projects","title":"Projects","text":"<p>This section details the projects maintained in the Lance community.</p>"},{"location":"community/#core-projects","title":"Core Projects","text":"<p>Core projects are the foundational repositories maintained by the Lance community with strict quality and release standards. Contributing Guidelines, Community Voting Process and Release Guidelines  are all applicable to these projects.</p> <p>Here is the list of current core projects:</p> Project Name Repository Contents lance https://github.com/lance-format/lance Lance file and table format specification, Rust SDK (including Namespace Integration SDK), Python SDK, Java SDK, Website lance-namespace https://github.com/lance-format/lance-namespace Lance namespace format specification, Rust/Python/Java Codegen SDKs, Java/Python Integration SDK lance-python-docs https://github.com/lance-format/lance-python-docs Lance Python SDK generated docs and integration hook with readthedocs lance-ray https://github.com/lance-format/lance-ray Ray integration for Lance lance-spark https://github.com/lance-format/lance-spark Apache Spark connector for Lance"},{"location":"community/#subprojects","title":"Subprojects","text":"<p>Subprojects are initiatives or repositories that extend Lance's functionality. They must align with Lance's overall mission and technical direction. New subprojects can be created with PMC approval.</p> <p>Subprojects have relaxed requirements for contribution, where contributors may receive write access even if not maintainers.</p> <p>Here is the list of current subprojects:</p> Project Name Repository Contents lance-data-viewer https://github.com/lance-format/lance-data-viewer Read-only web interface for browsing Lance datasets lance-duckdb https://github.com/lance-format/lance-duckdb DuckDB extension for Lance lance-flink https://github.com/lance-format/lance-flink Apache Flink connector for Lance lance-graph https://github.com/lance-format/lance-graph Cypher-capable graph query engine on top of Lance lance-trino https://github.com/lance-format/lance-trino Trino connector for Lance pglance https://github.com/lance-format/pglance PostgreSQL extension for Lance"},{"location":"community/#graduating-a-subproject","title":"Graduating a Subproject","text":"<p>The PMC can vote to promote a subproject to a core project once the subproject has demonstrated aspects including:</p> <ul> <li>Proper repository setup including CI, issue tracking, contributing guide, etc.</li> <li>Proper code standard enforcement including lint, testing, etc.</li> <li>Automated release mechanism</li> <li>Established production use cases</li> <li>Community adoption outside the primary contributor</li> <li>Consistent contributions from the community to add new features and fix bugs</li> </ul>"},{"location":"community/#project-license","title":"Project License","text":"<p>All Lance projects hosted in the lance-format GitHub Organization are licensed under the Apache License 2.0.</p>"},{"location":"community/#external-integrations","title":"External Integrations","text":"<p>We welcome and encourage Lance integrations in external projects. These integrations are valuable contributions to the Lance community and help expand the Lance ecosystem. When integrations are developed in external projects, the integration code and licensing should follow the guidelines and license of the external project.</p>"},{"location":"community/#reporting-security-vulnerability","title":"Reporting Security Vulnerability","text":"<p>In case of any security vulnerability, please contact the PMC through the Lance Private Mailing List and refrain from public disclosure until the issue is resolved.</p>"},{"location":"community/#reporting-harassment","title":"Reporting Harassment","text":"<p>The Lance community follows the Rust Community Code of Conduct. We are committed to providing a welcoming and inspiring community for all. Harassment of participants will not be tolerated. For such cases, please report to the Lance Private Mailing List.</p>"},{"location":"community/communication/","title":"Communication","text":""},{"location":"community/communication/#discord","title":"Discord","text":"<p>Discord is used for day-to-day discussions, community support, and real-time collaboration. Use this invite link to join and say hi!</p>"},{"location":"community/communication/#github-discussions","title":"GitHub Discussions","text":"<p>GitHub Discussions is used for development discussions, design proposals, public voting, and announcements.</p>"},{"location":"community/communication/#github-issues","title":"GitHub Issues","text":"<p>GitHub Issues is used for project issue tracking and roadmap posting.</p>"},{"location":"community/communication/#github-pull-requests","title":"GitHub Pull Requests","text":"<p>GitHub Pull Requests is used for project code reviews and code contributions.</p>"},{"location":"community/communication/#mailing-lists","title":"Mailing Lists","text":"<p>There are two mailing lists used by Lance:</p> <ul> <li>dev@lance.org: Archive for discussions from GitHub Discussions (public)</li> <li>private@lance.org: Private discussions, security and harassment issues reporting, voting (private, PMC only)</li> </ul>"},{"location":"community/communication/#community-events","title":"Community Events","text":"<p>Check out Lance Community Events  for upcoming virtual or in-person events related to Lance. The community can self-organize additional meetups as well.</p>"},{"location":"community/contributing/","title":"Guidelines for Contributing","text":"<p>In general, code contributions are in the form of GitHub PRs, and require review and approval from maintainers with write access.</p>"},{"location":"community/contributing/#conventional-commits","title":"Conventional Commits","text":"<p>Lance projects use the Conventional Commits standard for commit messages. This standard helps differentiate between:</p> <ul> <li>Breaking changes vs non-breaking changes (using <code>!</code> and <code>BREAKING CHANGE:</code> footer)</li> <li>Features (<code>feat:</code>), fixes (<code>fix:</code>), documentation updates (<code>docs:</code>), and other change types</li> </ul> <p>Commit messages following this standard are used to automatically generate release notes during each release.</p>"},{"location":"community/contributing/#feature-design-proposals","title":"Feature Design Proposals","text":"<p>Designs in Lance evolve naturally with community input and consensus. Major technical changes are discussed organically through the following approach:</p> <ul> <li>Start a Discussion: Create a GitHub Discussion to publish your design proposal and gather community feedback. Use discussion threads to explore different aspects and alternatives</li> <li>Iterate on Design: Engage with the community to refine the approach based on their input and expertise</li> <li>Draft PRs for Details: Once the general direction is acceptable to the community, publish draft PRs to help hash out implementation details. Draft PRs are encouraged as they facilitate concrete discussions</li> <li>Break Down Changes: Split large draft PRs into smaller, incremental PRs for easier review and to demonstrate progress</li> <li>Formal Voting: Maintainers with write access can approve code modifications related to the design. If the design requires Lance format spec changes, a separate vote will be conducted on GitHub Discussions following the voting requirements</li> </ul>"},{"location":"community/contributing/#ai-tooling-integrations","title":"AI Tooling Integrations","text":"<p>We encourage contributors to continuously improve integrations with AI tools, including:</p> <ul> <li>Enhancing coding agent guidelines such as <code>AGENTS.md</code> and <code>CLAUDE.md</code></li> <li>Providing feedback to AI code reviewers</li> <li>Developing and improving AI-driven GitHub actions</li> </ul>"},{"location":"community/contributing/#project-specific-contributing-guidelines","title":"Project Specific Contributing Guidelines","text":"<p>Each project maintains its own detailed contributing guidelines in files named <code>CONTRIBUTING.md</code>.</p>"},{"location":"community/maintainers/","title":"Maintainers","text":"<p>A maintainer is a contributor who has made sustained and valuable contributions to the Lance community. Maintainers are recognized for their work and granted various rights to support their ongoing contributions.</p>"},{"location":"community/maintainers/#rights","title":"Rights","text":"<p>All maintainers have the following rights:</p> <ul> <li>GitHub triage permissions on all Lance repositories</li> <li>Publish preview/beta releases of all Lance projects at any time</li> <li>Propose stable releases of all Lance projects</li> <li>Execute stable releases after the vote has passed</li> </ul> <p>Some maintainers may be granted GitHub write access to Lance repositories, which allows them to review and merge pull requests. Maintainers with write access may refer to themselves as \"committers\" when communicating outside the project, as this is a widely recognized term in the open source ecosystem, though \"committer\" is not part of the official Lance governance structure.</p> <p>Maintainers with GitHub write access additionally have the following rights:</p> <ul> <li>GitHub write access to all Lance repositories</li> <li>Approve and merge code modifications in all Lance projects (except changes to format specifications)</li> </ul>"},{"location":"community/maintainers/#activities","title":"Activities","text":"<p>Maintainers are encouraged to:</p> <ul> <li>Continue making valuable contributions to the Lance projects</li> <li>Help review PRs and provide feedback when possible</li> <li>Volunteer as release manager for stable releases when proposed</li> <li>Participate in community discussions and support other users</li> <li>Mentor and grow new maintainers</li> </ul> <p>Maintainers with GitHub write access are additionally encouraged to:</p> <ul> <li>Actively review and merge PRs while maintaining project code quality</li> <li>Participate in technical discussions and decisions</li> <li>Maintain project code standards and best practices</li> </ul>"},{"location":"community/maintainers/#roster","title":"Roster","text":"Name GitHub Handle Affiliation GitHub Write Access Ecosystem Roles Matt Basta mattbasta Runway AI Giuseppe Battista giusedroid AWS Timothy Carambat timothycarambat Anything LLM Ayush Chaurasia AyushExel LanceDB Chongchen Chen chenkovsky MiraclePlus Akela Drissner-Schmid akelad dltHub Ty Dunn TyDunn Continue Enwei Jiao jiaoew1991 Luma.ai \u2713 Milvus Maintainer Bryan Keller bryanck Netflix Apache Iceberg Committer Aman Kishore AmanKishore Harvey.ai Sangwu Lee RE-N-Y Krea.ai Jeremy Leibs jleibs Rerun.io Haocheng Liu HaochengLIU Seven Research \u2713 Nathan Ma majin1102 ByteDance \u2713 Apache Amoro (incubating) PPMC Member ChanChan Mao ccmao1130 LanceDB Lu Qiu LuQQiu LanceDB \u2713 Alluxio PMC Member Rong Rong walterddr Google DeepMind Apache Pinot PMC Member, Apache Flink Committer Nat Roth nrothGIT Meta AI Kevin Shaffer-Morrison kevinshaffermorrison AWS Noah Shpak noahshpak Thinking Machines Ankit Vij ankitvij-db Databricks Jiacheng Yang jiachengdb Google AI"},{"location":"community/maintainers/#becoming-a-maintainer","title":"Becoming a Maintainer","text":"<p>To become a maintainer:</p> <ul> <li>Make sustained and valuable contributions</li> <li>Demonstrate active participation in the community</li> <li>Get nominated by a PMC member and approved through a passing vote</li> </ul> <p>Here are some example areas of valuable contributions:</p> <ul> <li>Code Contributions:<ul> <li>Submit pull requests implementing features or fixing bugs</li> <li>Review code and provide constructive feedback on PRs</li> <li>Tackle issues on GitHub, especially those marked \"good first issue\"</li> </ul> </li> <li>Community Support:<ul> <li>Help other users in Discord and GitHub discussions</li> <li>Answer questions and provide guidance to newcomers</li> <li>Host or participate in tech discussions and special interest groups</li> </ul> </li> <li>Documentation and Content:<ul> <li>Write or improve documentation, tutorials, and guides</li> <li>Create blog posts, demos, or case studies showcasing Lance</li> <li>Share your Lance journey through articles and technical content</li> </ul> </li> <li>Advocacy and Outreach:<ul> <li>Present Lance at meetups, conferences, or community events</li> <li>Spread awareness on social media and developer platforms</li> <li>Beta test new features and provide detailed feedback</li> <li>Champion Lance adoption in your organization or community</li> </ul> </li> </ul> <p>To be granted GitHub write access, the maintainer should:</p> <ul> <li>Demonstrate deep understanding of the Lance codebase</li> <li>Have a history of high-quality contributions</li> <li>Have earned trust from the community for code reviews</li> <li>Get nominated by a PMC member and approval through a passing vote</li> <li>Sign the Contributor License Agreement (CLA)</li> </ul>"},{"location":"community/pmc/","title":"Project Management Committee (PMC)","text":"<p>A PMC member is a maintainer who has demonstrated leadership in the project. The PMC guides the long-term direction, makes decisions on governance and project changes, and protects the Lance brand.</p>"},{"location":"community/pmc/#rights","title":"Rights","text":"<p>In addition to the rights of maintainers, PMC members have the following rights:</p> <ul> <li>Binding vote on governance process and structure modifications</li> <li>Binding vote on changes in maintainers and PMC rosters</li> <li>Binding vote on core and subproject management decisions</li> <li>Binding vote on stable releases of core and subprojects</li> <li>Binding vote on Lance format specification modifications</li> <li>Access to the private mailing list</li> </ul>"},{"location":"community/pmc/#activities","title":"Activities","text":"<p>In addition to the activities of maintainers, PMC members are encouraged to:</p> <ul> <li>Provide guidance on the long-term direction of the project</li> <li>Make decisions on governance and project changes</li> <li>Protect the Lance brand</li> <li>Execute binding votes for stable releases</li> <li>Evaluate and respond to security vulnerabilities</li> </ul>"},{"location":"community/pmc/#roster","title":"Roster","text":"Name GitHub Handle Affiliation Ecosystem Roles Yang Cen BubbleCal LanceDB Milvus Contributor Pablo Delgado pablete Netflix Hao Ding Xuanwo LanceDB Apache OpenDAL PMC Chair, Apache Iceberg Committer, Apache Member and more Zhaowei Huang SaintBacchus Alibaba Apache Doris Committer Will Jones wjones127 LanceDB Apache Arrow PMC Member, Apache DataFusion PMC Member, Delta Lake Maintainer Matt Kafonek kafonek Runway AI Denny Lee dennyglee Databricks Unity Catalog Maintainer, Delta Lake Maintainer, Apache Spark Contributor, MLflow Contributor Rob Meng chebbyChefNEQ Jump Trading Dao Mi dowjones226 Netflix Weston Pace westonpace LanceDB Apache Arrow PMC Member, Substrait SMC Member Calvin Qi calvinqi Harvey.ai Prashanth Rao prrao87 LanceDB Ethan Rosenthal EthanRosenthal Runway AI Tim Saucer timsaucer Rerun.io Apache DataFusion PMC Member Chang She changhiskhan LanceDB Pandas Co-Author Jasmine Wang onigiriisabunny LanceDB Alluxio PMC Community Manager Lei Xu eddyxu LanceDB Apache Hadoop PMC Member Vino Yang yanghua Bytedance Apache Hudi PMC Member, Apache Kyuubi PMC Member, Apache Kylin Committer, Apache Incubation Program Committer Jack Ye jackye1995 LanceDB Apache Iceberg PMC Member, Apache Polaris (incubating) PPMC Member, Apache Incubation Program Committer"},{"location":"community/pmc/#becoming-a-pmc-member","title":"Becoming a PMC Member","text":"<p>PMC membership is earned through:</p> <ul> <li>Significant leadership in the project</li> <li>Long-term sustained contributions to Lance</li> <li>Active mentorship of other contributors</li> <li>Demonstrated commitment to project values</li> <li>Get nominated by a PMC member and approved through a passing vote</li> </ul>"},{"location":"community/release/","title":"Guidelines for Releases","text":"<p>Lance project releases should be automated as much as possible through GitHub Actions. Such automation includes bumping versions, marking breaking changes, publishing artifacts, and generating release notes. Overall, our goal is to minimize human interaction beyond initiating the release through GitHub Actions and voting on the release in GitHub Discussions.</p>"},{"location":"community/release/#release-types","title":"Release Types","text":"<p>Lance projects follow two types of releases:</p> <ul> <li> <p>Preview Releases (a.k.a. beta releases): Maintainers can publish preview releases at any time to consume the latest changes.   Preview releases have no stability guarantees and are intended for early testing and feedback.</p> </li> <li> <p>Stable Releases: Maintainers with write access can initiate stable releases at any time in GitHub Discussions.   Stable releases must go through a voting process.   After a stable release is initiated, the community is encouraged to verify the release for any potential bugs and vote for it.   The PMC is responsible for officially casting binding votes for the stable release. Once the vote has passed,   a maintainer can continue and finish the stable release.</p> </li> </ul>"},{"location":"community/release/#release-versioning","title":"Release Versioning","text":"<p>All Lance projects follow semantic versioning spec for release versioning:</p> <ul> <li>Major version (<code>X.0.0</code>): Incremented for breaking changes that are not backwards compatible</li> <li>Minor version (<code>0.X.0</code>): Incremented for new features that are backwards compatible</li> <li>Patch version (<code>0.0.X</code>): Incremented for critical fixes</li> </ul> <p>Preview releases use the <code>-beta.X</code> prerelease suffix appended to the target stable version (e.g., <code>1.2.3-beta.1</code>, <code>1.2.3-beta.2</code>).</p> <p>Note that unlike major and minor version releases that are cut from the main branch, patch version releases should be applied on top of an existing major, minor or patch release commit. Patch releases should only contain critical fixes for cases such as security vulnerabilities, major correctness issues, major performance regressions, or reverts of unintended breaking changes. Any fixes applied in a patch release should have corresponding fixes applied to the main branch. It is strongly discouraged to continue adding patch releases to old versions.</p> <p>For major version releases, it is recommended to include a migration guide for users to understand how to handle any breaking changes introduced in the major version.</p>"},{"location":"community/release/#project-specific-release-process","title":"Project Specific Release Process","text":"<p>Each project maintains its own detailed release process in a file named <code>release_process.md</code>. Changes to any project-specific release process are treated as normal code modifications and can be approved by a maintainer with write access.</p>"},{"location":"community/voting/","title":"Lance Community Voting Process","text":"<p>Lance uses a consensus-based voting process for decision-making.</p>"},{"location":"community/voting/#expressing-votes","title":"Expressing Votes","text":"<p>Votes are expressed as the following:</p> <ul> <li>+1: Yes</li> <li>0: Abstain</li> <li>-1: No</li> </ul> <p>When voting, it is recommended that voters indicate whether their vote is binding or not (e.g., <code>+1 (non-binding)</code>, <code>-1 (binding)</code>) to ease the counting of binding votes.</p> <p>In addition to the vote, voters can also express their justification as part of the comment. -1 votes must include justification to allow meaningful discussion. Any -1 vote not accompanied by justification is considered invalid.</p> <p>For votes conducted on GitHub Discussions, each vote should be cast as an independent comment instead of as a reply within a comment. This ensures that people can discuss the vote as replies to that specific comment if needed (e.g., to discuss -1 vetoes or address concerns).</p>"},{"location":"community/voting/#binding-votes","title":"Binding Votes","text":"<p>Only votes from the binding voters are counted for each decision, but other people in the community are also encouraged to cast non-binding votes. Binding voters should consider any concern from non-binding voters during the vote process.</p>"},{"location":"community/voting/#vetoes","title":"Vetoes","text":"<p>A -1 binding vote is considered a veto for all decision types. Vetoes:</p> <ul> <li>Stop the proposal until the concerns are resolved</li> <li>Cannot be overruled</li> <li>Trigger consensus gathering to address concerns</li> </ul>"},{"location":"community/voting/#voting-requirements","title":"Voting Requirements","text":"Decision Type +1 Votes Required Binding Voters Location Minimum Period Governance process and structure modifications 3 PMC Private Mailing List 1 week Changes in maintainers and PMC rosters 3 (excluding the people proposed for change) PMC Private Mailing List 1 week Subproject creation and management 3 PMC GitHub Discussions 3 days Subproject graduation to core project 3 PMC GitHub Discussions 1 week Release a new stable major version of core projects 3 PMC GitHub Discussions 1 week Release a new stable minor version of core projects 3 PMC GitHub Discussions 3 days Release a new stable patch version of core projects 3 PMC GitHub Discussions N/A Lance Format Specification modifications 3 (excluding proposer) PMC GitHub Discussions (with a GitHub PR) 1 week Code modifications in core projects (except changes to format specifications) 1 (excluding proposer) Maintainers with write access GitHub PR N/A Release a new stable version of subprojects 1 PMC GitHub Discussions N/A Code modifications in subprojects 1 (excluding proposer) Contributors with write access GitHub PR N/A"},{"location":"community/project-specific/","title":"Project Specific Guidelines","text":"<p>This section contains contributing and release guidelines from different Lance core projects.</p> <p>Each project maintains its own detailed guidelines that are automatically pulled from their respective repositories during the documentation build process.</p>"},{"location":"community/project-specific/namespace-impls/","title":"Contributing to Lance Namespace Implementations","text":"<p>The Lance Namespace Implementations codebase is at lance-format/lance-namespace-impls. This repository contains namespace implementations for various catalog systems that integrate with the Lance format.</p> <p>For contributing changes to the Lance Namespace specification or generated clients, please go to the lance-namespace repo.</p> <p>For contributing changes to directory and REST namespaces, please go to the lance repo.</p>"},{"location":"community/project-specific/namespace-impls/#repository-structure","title":"Repository structure","text":"<p>This repository currently contains the following components:</p> Component Language Path Description Glue Namespace Java java/lance-namespace-glue AWS Glue catalog implementation Hive2 Namespace Java java/lance-namespace-hive2 Hive Metastore 2.x implementation Hive3 Namespace Java java/lance-namespace-hive3 Hive Metastore 3.x implementation Unity Namespace Java java/lance-namespace-unity Databricks Unity Catalog implementation Polaris Namespace Java java/lance-namespace-polaris Apache Polaris catalog implementation Glue Namespace Python python/src/lance_namespace_impls/glue.py AWS Glue catalog implementation Hive Namespace Python python/src/lance_namespace_impls/hive.py Hive Metastore implementation Unity Namespace Python python/src/lance_namespace_impls/unity.py Unity Catalog implementation"},{"location":"community/project-specific/namespace-impls/#development-setup","title":"Development Setup","text":""},{"location":"community/project-specific/namespace-impls/#install-uv","title":"Install uv","text":"<p>We use uv for Python development. Make sure it is installed.</p>"},{"location":"community/project-specific/namespace-impls/#java-development","title":"Java Development","text":"<pre><code>cd java\n./mvnw install\n</code></pre>"},{"location":"community/project-specific/namespace-impls/#python-development","title":"Python Development","text":"<pre><code>cd python\nuv sync\nuv run pytest\n</code></pre>"},{"location":"community/project-specific/namespace-impls/#build","title":"Build","text":"<p>Top-level commands available:</p> <ul> <li><code>make clean</code>: Remove all build artifacts</li> <li><code>make build</code>: Build all modules</li> <li><code>make test</code>: Run all tests</li> </ul> <p>Language-specific commands:</p> <ul> <li><code>make clean-java</code>: Clean Java modules</li> <li><code>make build-java</code>: Build Java modules</li> <li><code>make test-java</code>: Test Java modules</li> <li><code>make clean-python</code>: Clean Python modules</li> <li><code>make build-python</code>: Build Python modules</li> <li><code>make test-python</code>: Test Python modules</li> </ul>"},{"location":"community/project-specific/namespace-impls/#documentation","title":"Documentation","text":"<p>The documentation is built using mkdocs-material.</p> <pre><code>cd docs\nmake serve  # Start local server\nmake build  # Build static site\n</code></pre>"},{"location":"community/project-specific/namespace-impls/#release-process","title":"Release Process","text":"<p>This section describes the CI/CD workflows for automated version management, releases, and publishing.</p>"},{"location":"community/project-specific/namespace-impls/#version-scheme","title":"Version Scheme","text":"<ul> <li>Stable releases: <code>X.Y.Z</code> (e.g., 1.2.3)</li> <li>Preview releases: <code>X.Y.Z-beta.N</code> (e.g., 1.2.3-beta.1)</li> </ul>"},{"location":"community/project-specific/namespace-impls/#creating-a-release","title":"Creating a Release","text":"<ol> <li>Create Release Draft</li> <li>Go to Actions -&gt; \"Create Release\"</li> <li>Select parameters:<ul> <li>Release type (major/minor/patch)</li> <li>Release channel (stable/preview)</li> <li>Dry run (test without pushing)</li> </ul> </li> <li> <p>Run workflow (creates a draft release)</p> </li> <li> <p>Review and Publish</p> </li> <li>Go to the Releases page to review the draft</li> <li>Edit release notes if needed</li> <li>Click \"Publish release\" to:<ul> <li>For stable releases: Trigger automatic publishing for Java and Python</li> <li>For preview releases: Create a beta release (not published)</li> </ul> </li> </ol>"},{"location":"community/project-specific/namespace/","title":"Contributing to Lance Namespace","text":"<p>The Lance Namespace codebase is at lance-format/lance-namespace. This codebase contains:</p> <ul> <li>The Lance Namespace specification</li> <li>The core <code>LanceNamespace</code> interface and generic connect functionality for all languages except Rust   (for Rust, these are located in the lance-format/lance repo)</li> <li>Generated clients and servers using OpenAPI generator</li> </ul> <p>This project should only be used to make spec and interface changes to Lance Namespace, or to add new clients and servers to be generated based on community demand. In general, we welcome more generated components to be added as long as  the contributor is willing to set up all the automations for generation and publication.</p> <p>For contributing changes to directory and REST namespaces, please go to the lance repo.</p> <p>For contributing changes to implementations other than the directory and REST namespace,  or for adding new namespace implementations, please go to the lance-namespace-impls repo.</p>"},{"location":"community/project-specific/namespace/#project-dependency","title":"Project Dependency","text":"<p>This project contains the core Lance Namespace specification, interface and generated modules across all languages. The dependency structure varies by language due to different build and distribution models.</p>"},{"location":"community/project-specific/namespace/#rust","title":"Rust","text":"<p>For Rust, the interface module <code>lance-namespace</code> and implementations (<code>lance-namespace-impls</code> for REST and directory namespaces) are located in the core lance-format/lance repository. This is because Rust uses source code builds, and separating modules across repositories makes dependency management complicated.</p> <p>The dependency chain is: <code>lance-namespace</code> \u2192 <code>lance</code> \u2192 <code>lance-namespace-impls</code></p>"},{"location":"community/project-specific/namespace/#other-languages-eg-python-java","title":"Other Languages (e.g. Python, Java)","text":"<p>For Python, Java, and other languages, the core <code>LanceNamespace</code> interface and generic connect functionality are maintained in this repository (e.g., <code>lance-namespace</code> for Python, <code>lance-namespace-core</code> for Java). The core lance-format/lance repository then imports these modules.</p> <p>The reason for this import direction is that <code>lance-namespace-impls</code> (REST and directory namespace implementations) are used in the Lance Python and Java bindings, and are exposed back through the corresponding language interfaces. These language interfaces can also be imported dynamically without the need to have a dependency of the Lance core library bindings in those languages.</p>"},{"location":"community/project-specific/namespace/#other-implementations","title":"Other Implementations","text":"<p>For namespace implementations other than directory and REST namespaces, those are stored in the lance-format/lance-namespace-impls repository, with one implementation per language.</p>"},{"location":"community/project-specific/namespace/#dependency-diagram","title":"Dependency Diagram","text":"<pre><code>flowchart TB\n    subgraph this_repo[\"lance-namespace repo\"]\n        spec[\"Spec &amp; Generated Clients\"]\n        py_core[\"Python: lance-namespace\"]\n        java_core[\"Java: lance-namespace-core\"]\n    end\n\n    subgraph lance_repo[\"lance repo\"]\n        subgraph rust_modules[\"Rust Modules\"]\n            rs_ns[\"lance-namespace\"]\n            rs_lance[\"lance\"]\n            rs_impls[\"lance-namespace-impls&lt;br/&gt;(dir, rest)\"]\n        end\n        py_lance[\"Python: lance\"]\n        java_lance[\"Java: lance\"]\n    end\n\n    subgraph impls_repo[\"namespace-impls repo\"]\n        polaris[\"Apache Polaris\"] ~~~ hive[\"Apache Hive\"] ~~~ iceberg_rest[\"Apache Iceberg REST\"] ~~~ unity[\"Unity Catalog\"] ~~~ glue[\"AWS Glue\"]\n    end\n\n    %% Rust dependencies (source build)\n    rs_ns --&gt; rs_lance\n    rs_lance --&gt; rs_impls\n\n    %% Python/Java dependencies\n    py_core --&gt; py_lance\n    java_core --&gt; java_lance\n    rs_impls -.-&gt; py_lance\n    rs_impls -.-&gt; java_lance\n\n    %% Other implementations depend on core interfaces and lance bindings\n    py_core -.-&gt; impls_repo\n    java_core -.-&gt; impls_repo\n    py_lance -.-&gt; impls_repo\n    java_lance -.-&gt; impls_repo\n\n    style this_repo fill:#1565c0,color:#fff\n    style lance_repo fill:#e65100,color:#fff\n    style impls_repo fill:#7b1fa2,color:#fff\n    style rust_modules fill:#ff8a65,color:#000</code></pre>"},{"location":"community/project-specific/namespace/#repository-structure","title":"Repository structure","text":"<p>This repository currently contains the following components:</p> Component Language Path Description Spec docs/src Lance Namespace Specification Python Core Python python/lance_namespace Core LanceNamespace interface and connect functionality Python UrlLib3 Client Python python/lance_namespace_urllib3_client Generated Python urllib3 client for Lance REST Namespace Java Core Java java/lance-namespace-core Core LanceNamespace interface and connect functionality Java Apache Client Java java/lance-namespace-apache-client Generated Java Apache HTTP client for Lance REST Namespace Java SpringBoot Server Java java/lance-namespace-springboot-server Generated Java SpringBoot server for Lance REST Namespace Rust Reqwest Client Rust rust/lance-namespace-reqwest-client Generated Rust reqwest client for Lance REST Namespace"},{"location":"community/project-specific/namespace/#install-uv","title":"Install uv","text":"<p>We use uv for development. Make sure it is installed, and run:</p> <pre><code>make sync\n</code></pre>"},{"location":"community/project-specific/namespace/#lint","title":"Lint","text":"<p>To ensure the OpenAPI definition is valid, you can use the lint command to check it.</p> <pre><code>make lint\n</code></pre>"},{"location":"community/project-specific/namespace/#build","title":"Build","text":"<p>There are 3 commands that is available at top level as well as inside each language folder:</p> <ul> <li><code>make clean</code>: remove all codegen modules</li> <li><code>make gen</code>: codegen and lint all modules (depends on <code>clean</code>)</li> <li><code>make build</code>: build all modules (depends on <code>gen</code>)</li> </ul> <p>You can also run <code>make &lt;command&gt;-&lt;language&gt;</code> to only run the command in the specific language, for example:</p> <ul> <li><code>make gen-python</code>: codegen and lint all Python modules</li> <li><code>make build-rust</code>: build all Rust modules</li> </ul> <p>You can also run <code>make &lt;command&gt;-&lt;language&gt;-&lt;module&gt;</code> inside a language folder to run the command against a specific module, for example:</p> <ul> <li><code>make gen-rust-reqwest-client</code>: codegen and lint the Rust reqwest client module</li> <li><code>make build-java-springboot-server</code>: build the Java Spring Boot server module</li> </ul>"},{"location":"community/project-specific/namespace/#documentation","title":"Documentation","text":""},{"location":"community/project-specific/namespace/#setup","title":"Setup","text":"<p>The documentation website is built using mkdocs-material. Start the server with:</p> <pre><code>make serve-docs\n</code></pre>"},{"location":"community/project-specific/namespace/#generated-model-documentation","title":"Generated Model Documentation","text":"<p>The operation request and response model documentation is generated from the Java Apache Client. When building or serving docs, the Java client must be generated first to produce the model Markdown files, which are then copied to <code>docs/src/operations/models/</code>.</p> <p>This happens automatically when running:</p> <pre><code>make build-docs  # or make serve-docs\n</code></pre> <p>These commands depend on <code>gen-java</code> to ensure the Java client docs are up-to-date before building the documentation.</p>"},{"location":"community/project-specific/namespace/#understanding-the-build-process","title":"Understanding the Build Process","text":"<p>The contents in <code>lance-namespace/docs</code> are for the ease of contributors to edit and preview. After code merge, the contents are added to the  main Lance documentation  during the Lance doc CI build time, and is presented in the Lance website under  Lance Namespace Spec.</p>"},{"location":"community/project-specific/namespace/#release-process","title":"Release Process","text":"<p>This section describes the CI/CD workflows for automated version management, releases, and publishing.</p>"},{"location":"community/project-specific/namespace/#version-scheme","title":"Version Scheme","text":"<ul> <li>Stable releases: <code>X.Y.Z</code> (e.g., 1.2.3)</li> <li>Preview releases: <code>X.Y.Z-beta.N</code> (e.g., 1.2.3-beta.1)</li> </ul>"},{"location":"community/project-specific/namespace/#creating-a-release","title":"Creating a Release","text":"<ol> <li>Create Release Draft</li> <li>Go to Actions \u2192 \"Create Release\"</li> <li>Select parameters:<ul> <li>Release type (major/minor/patch)</li> <li>Release channel (stable/preview)</li> <li>Dry run (test without pushing)</li> </ul> </li> <li> <p>Run workflow (creates a draft release)</p> </li> <li> <p>Review and Publish</p> </li> <li>Go to the Releases page to review the draft</li> <li>Edit release notes if needed</li> <li>Click \"Publish release\" to:<ul> <li>For stable releases: Trigger automatic publishing for Java, Python, Rust</li> <li>For preview releases: Create a beta release (not published)</li> </ul> </li> </ol>"},{"location":"community/project-specific/ray/","title":"Contributing to Lance-Ray","text":"<p>Thank you for your interest in contributing to Lance-Ray!  This document provides guidelines and instructions for contributing to the project.</p>"},{"location":"community/project-specific/ray/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python &gt;= 3.10</li> <li>UV package manager</li> <li>Git</li> </ul>"},{"location":"community/project-specific/ray/#setting-up-your-development-environment","title":"Setting Up Your Development Environment","text":"<ol> <li>Fork and clone the repository</li> </ol> <pre><code># Fork the repository on GitHub, then clone your fork\ngit clone https://github.com/YOUR_USERNAME/lance-ray.git\ncd lance-ray\n</code></pre> <ol> <li>Install UV (if not already installed)</li> </ol> <pre><code>pip install uv\n</code></pre> <ol> <li>Install the project in development mode</li> </ol> <pre><code># Install with all development dependencies\nuv pip install -e \".[dev]\"\n\n# To work on documentation, also install docs dependencies\nuv pip install -e \".[dev,docs]\"\n</code></pre>"},{"location":"community/project-specific/ray/#running-tests","title":"Running Tests","text":"<p>Run the test suite to ensure everything is working:</p> <pre><code># Run all tests\nuv run pytest\n\n# Run with coverage report\nuv run pytest --cov=lance_ray\n\n# Run specific test file\nuv run pytest tests/test_basic_read_write.py -vv\n</code></pre>"},{"location":"community/project-specific/ray/#check-styles","title":"Check Styles","text":"<p>We use <code>ruff</code> for both linting and formatting:</p> <pre><code># Format code\nuv run ruff format lance_ray/ tests/ examples/\n\n# Check linting\nuv run ruff check lance_ray/ tests/ examples/\n\n# Fix linting issues automatically\nuv run ruff check --fix lance_ray/ tests/ examples/\n</code></pre>"},{"location":"community/project-specific/ray/#building-documentation-locally","title":"Building Documentation Locally","text":"<pre><code># Serve documentation locally\ncd docs\nuv run mkdocs serve\n\n# Documentation will be available at http://localhost:8000\n</code></pre>"},{"location":"community/project-specific/ray/#release-process","title":"Release Process","text":"<p>This section describes the CI/CD workflows for automated version management, releases, and publishing.</p>"},{"location":"community/project-specific/ray/#version-scheme","title":"Version Scheme","text":"<ul> <li>Stable releases: <code>X.Y.Z</code> (e.g., 1.2.3)</li> <li>Preview releases: <code>X.Y.Z-beta.N</code> (e.g., 1.2.3-beta.1)</li> </ul>"},{"location":"community/project-specific/ray/#creating-a-release","title":"Creating a Release","text":"<ol> <li>Create Release Draft</li> <li>Go to Actions \u2192 \"Create Release\"</li> <li>Select parameters:<ul> <li>Release type (major/minor/patch)</li> <li>Release channel (stable/preview)</li> <li>Dry run (test without pushing)</li> </ul> </li> <li> <p>Run workflow (creates a draft release)</p> </li> <li> <p>Review and Publish</p> </li> <li>Go to the Releases page to review the draft</li> <li>Edit release notes if needed</li> <li>Click \"Publish release\" to:<ul> <li>For stable releases: Trigger automatic PyPI publishing</li> <li>For preview releases: Create a beta release (not published to PyPI)</li> </ul> </li> </ol>"},{"location":"community/project-specific/spark/","title":"Contributing to Spark Lance Connector","text":"<p>The Spark Lance connector codebase is at lancedb/lance-spark.</p>"},{"location":"community/project-specific/spark/#build-commands","title":"Build Commands","text":"<p>This connector is built using Maven. You can run the following make commands:</p> <pre><code># Install a specific Spark/Scala version (defaults: SPARK_VERSION=3.5, SCALA_VERSION=2.12)\nmake install\nmake install SPARK_VERSION=3.4 SCALA_VERSION=2.13\n\n# Run tests for a specific Spark/Scala version\nmake test\nmake test SPARK_VERSION=4.0 SCALA_VERSION=2.13\n\n# Build (lint + install) for a specific version\nmake build\n\n# Build the runtime bundle for a specific version\nmake bundle\nmake bundle SPARK_VERSION=3.5 SCALA_VERSION=2.13\n\n# Install all modules (without tests)\nmake install-all\n\n# Run all tests\nmake test-all\n\n# Clean a specific module\nmake clean-module\n\n# Clean all modules\nmake clean\n\n# Show all available commands\nmake help\n</code></pre>"},{"location":"community/project-specific/spark/#styling-guide","title":"Styling Guide","text":"<p>We use checkstyle and spotless to lint the code.</p> <p>To verify style, run:</p> <pre><code>make lint\n</code></pre> <p>To auto-format the code, run:</p> <pre><code>make format\n</code></pre>"},{"location":"community/project-specific/spark/#documentation","title":"Documentation","text":""},{"location":"community/project-specific/spark/#setup","title":"Setup","text":"<p>The documentation website is built using mkdocs-material. The build system require uv.</p> <p>Start the server with:</p> <pre><code>make serve-docs\n</code></pre>"},{"location":"community/project-specific/spark/#understanding-the-build-process","title":"Understanding the Build Process","text":"<p>The contents in <code>lance-spark/docs</code> are for the ease of contributors to edit and preview. After code merge, the contents are added to the  main Lance documentation  during the Lance doc CI build time, and is presented in the Lance website under  Apache Spark integration.</p> <p>The CONTRIBUTING.md document is auto-built to the Lance Contributing Guide</p>"},{"location":"community/project-specific/spark/#release-process","title":"Release Process","text":"<p>This section describes the CI/CD workflows for automated version management, releases, and publishing.</p>"},{"location":"community/project-specific/spark/#version-scheme","title":"Version Scheme","text":"<ul> <li>Stable releases: <code>X.Y.Z</code> (e.g., 1.2.3)</li> <li>Preview releases: <code>X.Y.Z-beta.N</code> (e.g., 1.2.3-beta.1)</li> </ul>"},{"location":"community/project-specific/spark/#creating-a-release","title":"Creating a Release","text":"<ol> <li>Create Release Draft</li> <li>Go to Actions \u2192 \"Create Release\"</li> <li>Select parameters:<ul> <li>Release type (major/minor/patch)</li> <li>Release channel (stable/preview)</li> <li>Dry run (test without pushing)</li> </ul> </li> <li> <p>Run workflow (creates a draft release)</p> </li> <li> <p>Review and Publish</p> </li> <li>Go to the Releases page to review the draft</li> <li>Edit release notes if needed</li> <li>Click \"Publish release\" to:<ul> <li>For stable releases: Trigger automatic Maven Central publishing</li> <li>For preview releases: Create a beta release (not published to Maven Central)</li> </ul> </li> </ol>"},{"location":"community/project-specific/lance/docs/","title":"Docs","text":""},{"location":"community/project-specific/lance/docs/#contributing-to-documentation","title":"Contributing to Documentation","text":""},{"location":"community/project-specific/lance/docs/#main-website","title":"Main website","text":"<p>The main documentation website is built using mkdocs-material. To build the docs, first install requirements:</p> <pre><code>cd docs\nuc sync --dev\n</code></pre> <p>Then build and start the docs server:</p> <pre><code>uv run mkdocs serve\n</code></pre>"},{"location":"community/project-specific/lance/docs/#python-generated-doc","title":"Python Generated Doc","text":"<p>Python code documentation is built using Sphinx in lance-python-doc, and published through Github Pages in ReadTheDocs style.</p>"},{"location":"community/project-specific/lance/docs/#rust-generated-doc","title":"Rust Generated Doc","text":"<p>Rust code documentation is built and published to the Rust official docs website as a part of the release process.</p>"},{"location":"community/project-specific/lance/docs/#java-generated-doc","title":"Java Generated Doc","text":"<p>Java code documentation is built and published to Maven Central. You can find the doc page for the specific project at javadoc.io.</p>"},{"location":"community/project-specific/lance/general/","title":"Guide for New Contributors","text":"<p>This is a guide for new contributors to the Lance project. Even if you have no previous experience with python, rust, and open source, you can still make an non-trivial impact by helping us improve documentation, examples, and more. For experienced developers, the issues you can work on run the gamut from warm-ups to serious challenges in python and rust.</p> <p>If you have any questions, please join our Discord for real-time support. Your feedback is always welcome!</p>"},{"location":"community/project-specific/lance/general/#getting-started","title":"Getting Started","text":"<ol> <li>Join our Discord and say hi</li> <li>Setup your development environment</li> <li>Pick an issue to work on. See https://github.com/lancedb/lance/contribute for good first issues.</li> <li>Have fun!</li> </ol>"},{"location":"community/project-specific/lance/general/#development-environment","title":"Development Environment","text":"<p>Currently Lance is implemented in Rust and comes with a Python wrapper. So you'll want to make sure you setup both.</p> <ol> <li>Install Rust: https://www.rust-lang.org/tools/install</li> <li>Install Python 3.9+: https://www.python.org/downloads/</li> <li>Install protoctol buffers: https://grpc.io/docs/protoc-installation/ (make sure you have version 3.20 or higher)</li> <li>Install commit hooks:     a. Install pre-commit: https://pre-commit.com/#install     b. Run <code>pre-commit install</code> in the root of the repo</li> </ol>"},{"location":"community/project-specific/lance/general/#sample-workflow","title":"Sample Workflow","text":"<ol> <li>Fork the repo</li> <li>Pick Github issue</li> <li>Create a branch for the issue</li> <li>Make your changes</li> <li>Create a pull request from your fork to lancedb/lance</li> <li>Get feedback and iterate</li> <li>Merge!</li> <li>Go back to step 2</li> </ol>"},{"location":"community/project-specific/lance/general/#example-notebooks","title":"Example Notebooks","text":"<p>Example notebooks are under <code>examples</code>. These are standalone notebooks you should be able to download and run.</p>"},{"location":"community/project-specific/lance/general/#benchmarks","title":"Benchmarks","text":"<p>Our Rust benchmarks are run multiple times a day and the history can be found here.</p> <p>Separately, we have vector index benchmarks that test against the sift1m dataset, as well as benchmarks for tpch. These live under <code>benchmarks</code>.</p>"},{"location":"community/project-specific/lance/general/#reviewing-issues-and-pull-requests","title":"Reviewing issues and pull requests","text":"<p>Please consider the following when reviewing code contributions.</p>"},{"location":"community/project-specific/lance/general/#rust-api-design","title":"Rust API design","text":"<ul> <li>Design public APIs so they can be evolved easily in the future without breaking   changes. Often this means using builder patterns or options structs instead of   long argument lists.</li> <li>For public APIs, prefer inputs that use <code>Into&lt;T&gt;</code> or <code>AsRef&lt;T&gt;</code> traits to allow   more flexible inputs. For example, use <code>name: Into&lt;String&gt;</code> instead of <code>name: String</code>,   so we don't have to write <code>func(\"my_string\".to_string())</code>.</li> </ul>"},{"location":"community/project-specific/lance/general/#testing","title":"Testing","text":"<ul> <li>Ensure all new public APIs have documentation and examples.</li> <li>Ensure that all bugfixes and features have corresponding tests. We do not merge   code without tests.</li> </ul>"},{"location":"community/project-specific/lance/general/#important-labels","title":"Important Labels","text":"<p>There are two important labels to apply to relevant issues and PRs:</p> <ol> <li><code>breaking-change</code>: Any PR that introduces a breaking change to the public API   (Rust or Python) must be labelled as such. This is used to determine how to   bump the version number when releasing. You can still add this label even   after merging a PR.</li> <li><code>critical-fix</code>: Any PR that fixes a critical bug (e.g., security issue, data   corruption, crash) should be labelled as such. These are bugs that users might   have without realizing. Fixes that aren't critical include bugs that return   an error message. These labels are used to determine whether a patch release   is needed.</li> </ol>"},{"location":"community/project-specific/lance/general/#code-of-conduct","title":"Code of Conduct","text":"<p>We follow the Code of Conduct of Python Foundation and Rust Foundation.</p>"},{"location":"community/project-specific/lance/python/","title":"Contributing to Python","text":"<p>The python integration is done via pyo3 + custom python code:</p> <ol> <li>The Rust code that directly supports the Python bindings are under <code>python/src</code> while the pure Python code lives under <code>python/python</code>.</li> <li>We make wrapper classes in Rust for Dataset/Scanner/RecordBatchReader that's exposed to python.</li> <li>These are then used by LanceDataset / LanceScanner implementations that extend pyarrow Dataset/Scanner for duckdb compat.</li> <li>Data is delivered via the Arrow C Data Interface</li> </ol> <p>To build the Python bindings, first install requirements:</p> <pre><code>pip install maturin\n</code></pre> <p>To make a dev install:</p> <pre><code>cd python\nmaturin develop\n</code></pre> <p>After installing, you can run <code>import lance</code> in a Python shell within the virtual environment.</p> <p>To run tests and integration tests: <pre><code>make test\nmake integtest\n</code></pre></p> <p>To run the tests on OS X, you may need to increase the default limit on the number of open files: <code>ulimit -n 2048</code></p>"},{"location":"community/project-specific/lance/release/","title":"Release Process","text":"<p>Lance uses semantic versioning and maintains a linear commit history. * All pull requests are merged into the <code>main</code> branch. * Beta releases (or preview releases) are created on-demand from the <code>main</code> branch. * Stable releases (non-prereleases) are created only after a voting process and come from a release branch <code>release/vX.Y</code>. These are typically created once every two weeks. * Release Candidates (RC) are created from release branches prior to voting. * Patch releases are created by committing fixes directly to the release branch, voting on a new RC, and releasing.</p> <pre><code>gitGraph\n    commit\n    branch feature\n    checkout feature\n    commit\n    checkout main\n    merge feature\n    branch bugfix\n    checkout bugfix\n    commit id: \"bugfix\"\n    checkout main\n    branch \"release/v1.4\"\n    checkout \"release/v1.4\"\n    commit tag: \"1.4.0-rc.1\"\n    commit tag: \"1.4.0\"\n    checkout main\n    merge bugfix\n    commit id: \"merged\"\n    checkout \"release/v1.4\"\n    cherry-pick id: \"merged\"\n    commit tag: \"1.4.1-rc.1\"\n    commit tag: \"1.4.1\"\n    checkout main\n    commit tag: \"1.5.0-beta.1\"\n</code></pre>"},{"location":"community/project-specific/lance/release/#version-semantics","title":"Version Semantics","text":""},{"location":"community/project-specific/lance/release/#version-format","title":"Version Format","text":"<p>Lance uses semantic versioning with prerelease identifiers: - Stable: <code>X.Y.Z</code> (e.g., <code>1.3.0</code>) - Beta: <code>X.Y.Z-beta.N</code> (e.g., <code>1.3.0-beta.1</code>, <code>1.3.0-beta.2</code>) - RC: <code>X.Y.Z-rc.N</code> (e.g., <code>1.3.0-rc.1</code>, <code>1.3.0-rc.2</code>)</p>"},{"location":"community/project-specific/lance/release/#beta-version-states","title":"Beta Version States","text":"<ul> <li>beta.0: Unreleased version (exists on branch but not published)</li> <li>Created after cutting an RC to mark the next unreleased version</li> <li>Indicates no preview has been published yet</li> <li>beta.1+: Published preview releases</li> <li>Created when publishing beta preview artifacts</li> </ul>"},{"location":"community/project-specific/lance/release/#publishing-channels","title":"Publishing Channels","text":"Language Stable release RC release Beta release Rust crates.io Not published (use git tag) Not published (use git tag) Python PyPI fury.io fury.io Java Maven Central Maven Central Maven Central Protobuf Buf Schema Registry Buf Schema Registry Buf Schema Registry"},{"location":"community/project-specific/lance/release/#github-releases-and-release-notes","title":"GitHub Releases and Release Notes","text":"Release Type GitHub Release Type Start Commit (exclusive) End Commit (inclusive) Explanation Stable (Major/Minor) Release <code>release-root/X.Y.0-beta.N</code> <code>vX.Y.0</code> All changes from main + RC fixes Stable (Patch) Release <code>vX.Y.(Z-1)</code> <code>vX.Y.Z</code> Only changes in this patch release RC (Major/Minor) Pre-Release <code>release-root/X.Y.0-beta.N</code> <code>vX.Y.0-rc.N</code> All changes for the release RC (Patch) Pre-Release <code>vX.Y.(Z-1)</code> <code>vX.Y.Z-rc.N</code> Only changes in this patch release RC (Iterations) Pre-Release <code>vX.Y.(Z-1)</code> <code>vX.Y.Z-rc.N</code> Only changes in this patch release (not changes against previous RC) Beta (Main branch) Pre-Release <code>release-root/X.Y.Z-beta.N</code> <code>vX.Y.Z-beta.N</code> Changes since last stable release RC cut in main branch Beta (Release branch) Pre-Release <code>vX.Y.(Z-1)</code> <code>vX.Y.Z-beta.N</code> Changes since last stable release"},{"location":"community/project-specific/lance/release/#branching-strategy","title":"Branching Strategy","text":""},{"location":"community/project-specific/lance/release/#main-branch","title":"Main Branch","text":"<ul> <li>Always contains the latest development work</li> <li>Version format: <code>X.Y.Z-beta.N</code></li> <li>After RC creation, bumped to next minor version with <code>-beta.0</code> (unreleased)</li> <li>Beta previews published by bumping to <code>-beta.1+</code></li> </ul>"},{"location":"community/project-specific/lance/release/#release-branches","title":"Release Branches","text":"<ul> <li>Format: <code>release/v{major}.{minor}</code> (e.g., <code>release/v1.3</code>)</li> <li>Created when cutting initial RC for major/minor release</li> <li>Maintained for patch releases</li> <li>Version progression: <code>rc.1</code> \u2192 <code>rc.2</code> \u2192 stable \u2192 <code>beta.0</code> \u2192 <code>rc.1</code> (for patches)</li> </ul>"},{"location":"community/project-specific/lance/release/#version-flow","title":"Version Flow","text":"<pre><code>%%{init: {'theme':'base', 'themeVariables': { 'fontSize':'14px'}}}%%\nflowchart LR\n    subgraph main[\"Main Branch\"]\n        direction LR\n        M0[\"1.3.0-beta.2&lt;br/&gt;\ud83d\udccd release-root/1.4.0-beta.N&lt;br/&gt;\ud83d\udccd release-root/2.0.0-beta.N\"] --&gt; M1[\"1.4.0-beta.0\"]\n        M1 --&gt; M2[\"1.4.0-beta.1&lt;br/&gt;\ud83c\udff7\ufe0f v1.4.0-beta.1\"]\n        M2 --&gt; M3[\"2.0.0-beta.1&lt;br/&gt;\ud83c\udff7\ufe0f v2.0.0-beta.1\"]\n    end\n\n    subgraph release[\"Release Branch: release/v1.3\"]\n        direction LR\n        R1[\"1.3.0-rc.1&lt;br/&gt;\ud83c\udff7\ufe0f v1.3.0-rc.1\"] --&gt; R2[\"1.3.0&lt;br/&gt;\ud83c\udff7\ufe0f v1.3.0\"]\n        R2 --&gt; R3[\"1.3.1-beta.0\"]\n        R3 --&gt; R4[\"1.3.1-rc.1&lt;br/&gt;\ud83c\udff7\ufe0f v1.3.1-rc.1\"]\n        R4 --&gt; R5[\"1.3.1&lt;br/&gt;\ud83c\udff7\ufe0f v1.3.1\"]\n        R5 --&gt; R6[\"1.3.2-beta.0\"]\n    end\n\n    M0 -.-&gt;|\"create RC&lt;br/&gt;(no breaking changes)\"| R1</code></pre> <p>Flow explanation: - Main branch: Commit M0 at <code>1.3.0-beta.2</code> has <code>release-root/1.4.0-beta.N</code> (created when cutting v1.3.0-rc.1, pointing to this commit) and <code>release-root/2.0.0-beta.N</code> (created when breaking changes bumped major version, pointing to same commit) \u2192 M1 bumps to <code>1.4.0-beta.0</code> (unreleased) \u2192 M2 publishes <code>1.4.0-beta.1</code> (preview, tagged) \u2192 M3 publishes <code>2.0.0-beta.1</code> after detecting breaking changes (tagged) - Release branch <code>release/v1.3</code> created from M0, starts at <code>1.3.0-rc.1</code> (tagged) \u2192 <code>1.3.0</code> (stable, tagged) \u2192 <code>1.3.1-beta.0</code> \u2192 <code>1.3.1-rc.1</code> (tagged) \u2192 <code>1.3.1</code> (stable, tagged) \u2192 <code>1.3.2-beta.0</code> - Tags: \ud83c\udff7\ufe0f = version tag (points to tagged commit), \ud83d\udccd = release-root tag (points to commit before RC was created, used for breaking change detection) - Breaking changes: Both <code>release-root/1.4.0-beta.N</code> and <code>release-root/2.0.0-beta.N</code> point to M0 (same commit), showing that 2.0.0 is a major version bump from the 1.3.0-rc.1 baseline</p> <p>Note: All commits are linear on their respective branches. <code>beta.0</code> = unreleased, <code>beta.1+</code> = published previews.</p>"},{"location":"community/project-specific/lance/release/#workflows","title":"Workflows","text":""},{"location":"community/project-specific/lance/release/#user-facing-workflows","title":"User-Facing Workflows","text":"<ol> <li>publish-beta.yml - Publish beta preview releases from any branch</li> <li>create-release-branch.yml - Create release branch with initial RC for new major/minor version</li> <li>create-rc.yml - Create RC on existing release branch (for new patch release RC or iterations of an existing RC)</li> <li>approve-rc.yml - Approve any RC to stable (works for all release types)</li> </ol>"},{"location":"community/project-specific/lance/release/#create-a-beta-preview-release","title":"Create a Beta / Preview Release","text":"<p>Purpose: Publish preview releases for testing before creating release candidates.</p> <p>Steps: 1. Trigger \"Publish Beta\" workflow 2. Set branch: <code>main</code> (or any release branch) 3. Set dry_run: <code>true</code> (test first) 4. Review results, then run with dry_run: <code>false</code></p> <p>Result: Creates a beta tag (e.g., <code>v1.4.0-beta.1</code>) and publishes preview artifacts to fury.io, Maven Central, and Buf Schema Registry.</p> How beta versioning works  **For main branch**: Automatically checks for breaking changes and bumps version: - **No breaking changes**: Increments beta (e.g., `1.4.0-beta.0` \u2192 `1.4.0-beta.1`) - **Breaking changes found**: Bumps major and resets beta (e.g., `1.4.0-beta.1` \u2192 `2.0.0-beta.1`) - **Already bumped**: Just increments beta (e.g., `2.0.0-beta.1` \u2192 `2.0.0-beta.2`)  **For release branches**: Bumps beta number (`beta.N` \u2192 `beta.N+1`)  **Use cases**: - Testing features before RC - Regular preview releases for early adopters - Automatic breaking change detection"},{"location":"community/project-specific/lance/release/#breaking-change-detection","title":"Breaking Change Detection","text":"<p>How it works: Mark PRs with the <code>breaking-change</code> label in GitHub. The workflow automatically detects these and bumps the major version when publishing beta releases from main.</p> <p>What counts as breaking: - Upgrading pinned dependencies in public API (DataFusion, Arrow) - Changing signatures of public functions/methods - Removing public functions/methods - Changing public data structures - Exception: Experimental APIs (marked as such in docs) are not considered breaking</p> Technical details: Release root tags and version bumping  ### Release Root Tag  Release root tags mark the base commits for breaking change detection. The tag naming reflects the **beta version series on main**, while the tag points to the **RC commit being compared against**.  **Tag Format**: `release-root/{major}.{minor}.{patch}-beta.N` - The tag name indicates which beta version series uses this base - The tag points to the commit on main branch before the RC was created (the comparison base) - The tag message stores the base RC version (e.g., \"Base: 1.3.0-rc.1\") - this is what we compare against to detect major version bumps - The base RC version in the message stays constant even when multiple release-root tags point to the same commit  **When created**: 1. **When creating a major/minor RC**: After bumping main to the next version    - Example: After cutting v1.3.0-rc.1, create `release-root/1.4.0-beta.N` pointing to the commit before the RC branch was created 2. **When breaking changes bump major version**: When major version is bumped during beta publish    - Example: When bumping 1.4.0-beta.5 \u2192 2.0.0-beta.1, create `release-root/2.0.0-beta.N` pointing to the SAME commit with the SAME base RC version  **Key properties**: - **Multiple tags, same commit**: `release-root/1.4.0-beta.N` and `release-root/2.0.0-beta.N` point to the same commit on main (the commit before the RC branch was created) - **Major version bumped once**: Both tags store same base RC version (1.3.0-rc.1), so we know 2.x is already a major bump from 1.3.0 - **No additional bumps**: When at 2.0.0-beta.1, we detect breaking changes but see major already bumped (2 &gt; 1), so just increment beta - **Beta reset on major bump**: When bumping major version, beta number resets to 1 (e.g., 1.4.0-beta.5 \u2192 2.0.0-beta.1)  ### Detection Process  Breaking change detection happens **on every beta publish from main branch**:  1. **Find release-root tag**: Look for `release-root/{current_version}-beta.N`    - If NOT found \u2192 Bump minor only (no comparison base exists, skip breaking change detection) 2. **Extract base RC version**: Read from tag message (e.g., \"Base: 1.3.0-rc.1\" \u2192 base major is `1`) 3. **Compare**: Check for breaking changes since the commit pointed to by the release-root tag 4. **Determine action**:    - If breaking changes AND current_major == base_major \u2192 bump to next major    - If breaking changes AND current_major &gt; base_major \u2192 no bump (already bumped)    - If no breaking changes \u2192 no major bump  ### Examples  Starting from v1.3.0-rc.1 cut, main at 1.4.0-beta.0 with `release-root/1.4.0-beta.N` (Base: 1.3.0-rc.1): - `1.4.0-beta.0` + no breaking \u2192 `1.4.0-beta.1` - `1.4.0-beta.1` + breaking \u2192 `2.0.0-beta.1`   - Creates `release-root/2.0.0-beta.N` pointing to same commit, message still \"Base: 1.3.0-rc.1\"   - Base major from tag message is 1, current major is 1, so bump to 2 - `2.0.0-beta.1` + breaking \u2192 `2.0.0-beta.2`   - Base major is 1, current major is 2 (already bumped), so just increment beta - `2.0.0-beta.2` + no breaking \u2192 `2.0.0-beta.3`  **Key insight**: Multiple beta version series can share the same release-root commit, with major version bumped only once when first detected."},{"location":"community/project-specific/lance/release/#create-a-major-minor-release","title":"Create a Major / Minor Release","text":"<p>Purpose: Create a new major or minor release from the main branch.</p> <p>Steps: 1. Ensure CI on main is green 2. Trigger \"Create Release Branch\" workflow with dry_run: <code>true</code> 3. Review the RC version and changes 4. Run with dry_run: <code>false</code> to create release branch and RC 5. Test RC artifacts (published to fury.io, Maven Central) 6. Vote in the GitHub Discussion thread (created automatically) 7. If issues found: Fix on release branch, run \"Create RC\" workflow to create <code>rc.2</code>, <code>rc.3</code>, etc. 8. If approved: Trigger \"Approve RC\" workflow with rc_tag (e.g., <code>v1.3.0-rc.2</code>)</p> <p>Result: - Creates release branch (e.g., <code>release/v1.3</code>) with RC tag (e.g., <code>v1.3.0-rc.1</code>) - Bumps main to next minor (e.g., <code>1.4.0-beta.0</code>) - After approval: Creates stable tag (e.g., <code>v1.3.0</code>) and publishes to PyPI, crates.io, Maven Central</p> What happens under the hood  **Create Release Branch workflow**: - Reads current version from main (e.g., `1.3.0-beta.2`) - Checks for breaking changes since release-root tag - If breaking changes: Creates RC with bumped major (e.g., `2.0.0-rc.1`), bumps main to `2.1.0-beta.0` - If no breaking changes: Creates RC with current version (e.g., `1.3.0-rc.1`), bumps main to `1.4.0-beta.0` - Creates `release/v{major}.{minor}` branch from main HEAD - Creates GitHub Discussion for voting  **Approve RC workflow**: - Bumps version from `rc.N` to stable - Generates release notes comparing against `release-root/{version}-beta.N` tag - Creates GitHub Release and publishes stable artifacts - Auto-bumps release branch to next patch `beta.0` (e.g., `1.3.0` \u2192 `1.3.1-beta.0`) - Main branch is NOT affected (already bumped in step 1)"},{"location":"community/project-specific/lance/release/#create-a-patch-bugfix-release","title":"Create a Patch / Bugfix Release","text":"<p>Purpose: Release critical bug fixes for an existing release.</p> <p>Steps: 1. Checkout the release branch (e.g., <code>release/v1.3</code>) 2. Create and test your fix (ensure no breaking changes) 3. Create a PR to merge into the release branch 4. Trigger \"Create RC\" workflow with release_branch (e.g., <code>release/v1.3</code>) and dry_run: <code>true</code> 5. Review the patch RC version 6. Run with dry_run: <code>false</code> to create the patch RC 7. Test RC artifacts and vote in the GitHub Discussion 8. If issues found: Fix and run \"Create RC\" again to create <code>rc.2</code>, <code>rc.3</code>, etc. 9. If approved: Trigger \"Approve RC\" workflow with rc_tag (e.g., <code>v1.3.1-rc.1</code>)</p> <p>Result: - Creates patch RC tag (e.g., <code>v1.3.1-rc.1</code>) on release branch - After approval: Creates stable tag (e.g., <code>v1.3.1</code>) and publishes to PyPI, crates.io, Maven Central - Auto-bumps release branch to next patch <code>beta.0</code> (e.g., <code>1.3.2-beta.0</code>) - Main branch is NOT affected</p> Important notes  - **Breaking changes not allowed**: Release branches are for patch releases only - **Beta versions**: Release branches stay at `X.Y.Z-beta.N` between releases (auto-bumped after stable) - **Release notes**: Compares against previous stable tag (e.g., `v1.3.0`) - **Allowed changes**: Correctness bugs, security fixes, major performance regressions, unintentional breaking change reverts"},{"location":"community/project-specific/lance/release/#example-workflows","title":"Example Workflows","text":""},{"location":"community/project-specific/lance/release/#beta-preview-release","title":"Beta Preview Release","text":"<pre><code># 1. Main at 1.4.0-beta.0 (unreleased after RC cut for v1.3.0)\n# 2. Want to publish preview for testing\nWorkflow: Publish Beta\n  branch: main\n  Result:\n    - Looks for release-root/1.4.0-beta.N \u2192 found\n    - Extracts base: 1.3.0-rc.1 (major: 1) from tag message\n    - No breaking changes detected\n    - Bumped to 1.4.0-beta.1\n    - Tagged v1.4.0-beta.1\n    - Created GitHub Pre-Release with release notes from release-root/1.4.0-beta.N to v1.4.0-beta.1\n    - Published artifacts to fury.io\n\n# 3. More changes, publish again (with breaking changes)\nWorkflow: Publish Beta\n  branch: main\n  Result:\n    - Looks for release-root/1.4.0-beta.N \u2192 found\n    - Extracts base: 1.3.0-rc.1 (major: 1) from tag message\n    - Breaking changes detected, current major (1) == base major (1)\n    - Bumped to 2.0.0-beta.1 (beta resets on major bump)\n    - Created release-root/2.0.0-beta.N \u2192 same commit, message \"Base: 1.3.0-rc.1\"\n    - Tagged v2.0.0-beta.1\n    - Created GitHub Pre-Release with release notes from release-root/2.0.0-beta.N to v2.0.0-beta.1\n    - Published artifacts to fury.io\n\n# 4. More changes, publish again (still has breaking changes)\nWorkflow: Publish Beta\n  branch: main\n  Result:\n    - Looks for release-root/2.0.0-beta.N \u2192 found\n    - Extracts base: 1.3.0-rc.1 (major: 1) from tag message\n    - Breaking changes detected, but current major (2) &gt; base major (1)\n    - No major bump needed (already bumped)\n    - Bumped to 2.0.0-beta.2\n    - Tagged v2.0.0-beta.2\n    - Created GitHub Pre-Release with release notes from release-root/2.0.0-beta.N to v2.0.0-beta.2\n    - Published artifacts to fury.io\n</code></pre>"},{"location":"community/project-specific/lance/release/#standard-majorminor-release","title":"Standard Major/Minor Release","text":"<pre><code># 1. Main is at 1.3.0-beta.2\n# 2. Create release branch (version auto-determined from main)\nWorkflow: Create Release Branch\n  Result:\n    - Checks for breaking changes since release-root/1.3.0-beta.N\n    - No breaking changes detected\n    - Created release/v1.3 at 1.3.0-rc.1\n    - Tagged v1.3.0-rc.1\n    - Created GitHub Pre-Release with release notes from release-root/1.3.0-beta.N to v1.3.0-rc.1\n    - Bumped main to 1.4.0-beta.0 (unreleased)\n    - Tagged release-root/1.4.0-beta.N \u2192 points to commit before RC branch, message \"Base: 1.3.0-rc.1\"\n    - GitHub Discussion created\n\n# 3. Vote on RC\n  - Navigate to Discussion thread\n  - Test RC artifacts\n  - Vote with +1, 0, -1\n\n# 4. Approve RC\nWorkflow: Approve RC\n  rc_tag: v1.3.0-rc.1\n  Result:\n    - release/v1.3 @ 1.3.0 (stable)\n    - Tagged v1.3.0\n    - Generated release notes comparing v1.3.0 vs release-root/1.3.0-beta.N\n    - Created GitHub Release (not pre-release)\n    - Stable artifacts published\n    - Release branch auto-bumped to 1.3.1-beta.0\n    - Main unchanged (already at 1.4.0-beta.0)\n\n# 5. Later: Publish first beta after RC (no breaking changes)\nWorkflow: Publish Beta\n  branch: main\n  Result:\n    - Looks for release-root/1.4.0-beta.N \u2192 found\n    - Extracts base: 1.3.0-rc.1 (major: 1) from tag message\n    - No breaking changes detected\n    - Bumped from 1.4.0-beta.0 to 1.4.0-beta.1\n    - Tagged v1.4.0-beta.1\n    - Created GitHub Pre-Release with release notes from release-root/1.4.0-beta.N to v1.4.0-beta.1\n    - Published artifacts to fury.io\n\n# 6. More changes, publish second beta (breaking changes introduced!)\nWorkflow: Publish Beta\n  branch: main\n  Result:\n    - Looks for release-root/1.4.0-beta.N \u2192 found\n    - Extracts base: 1.3.0-rc.1 (major: 1) from tag message\n    - Breaking changes detected, current major (1) == base major (1)\n    - Bumped from 1.4.0-beta.1 to 2.0.0-beta.1 (beta resets on major bump)\n    - Created release-root/2.0.0-beta.N \u2192 same commit, message \"Base: 1.3.0-rc.1\"\n    - Tagged v2.0.0-beta.1\n    - Created GitHub Pre-Release with release notes from release-root/2.0.0-beta.N to v2.0.0-beta.1\n    - Published artifacts to fury.io\n\n# 7. More changes, publish third beta (still has breaking changes)\nWorkflow: Publish Beta\n  branch: main\n  Result:\n    - Looks for release-root/2.0.0-beta.N \u2192 found\n    - Extracts base: 1.3.0-rc.1 (major: 1) from tag message\n    - Breaking changes detected, but current major (2) &gt; base major (1)\n    - No major bump needed (already bumped from base)\n    - Bumped to 2.0.0-beta.2\n    - Tagged v2.0.0-beta.2\n    - Published artifacts\n\n# 8. Eventually: Cut RC for v2.0.0\nWorkflow: Create Release Branch\n  Result:\n    - Main at 2.0.0-beta.2\n    - Checks for breaking changes since release-root/2.0.0-beta.N\n    - No additional breaking changes (major already bumped)\n    - Created release/v2.0 at 2.0.0-rc.1\n    - Tagged v2.0.0-rc.1\n    - Bumped main to 2.1.0-beta.0\n    - Tagged release-root/2.1.0-beta.N \u2192 points to commit before RC branch, message \"Base: 2.0.0-rc.1\"\n</code></pre>"},{"location":"community/project-specific/lance/release/#patch-release","title":"Patch Release","text":"<pre><code># 1. Start with release/v1.3 @ 1.3.1-beta.0 (auto-bumped after previous stable release)\n# 2. Critical bug found in 1.3.0\n# 3. Fix committed to release/v1.3\n# 4. Create patch RC\nWorkflow: Create RC\n  release_branch: release/v1.3\n  Result:\n    - Branch at 1.3.1-beta.0\n    - Created 1.3.1-rc.1\n    - Tagged v1.3.1-rc.1\n    - Created GitHub Pre-Release with release notes from v1.3.0 to v1.3.1-rc.1\n    - GitHub Discussion created\n\n# 5. Vote passes\n# 6. Approve patch RC\nWorkflow: Approve RC\n  rc_tag: v1.3.1-rc.1\n  Result:\n    - release/v1.3 @ 1.3.1\n    - Tagged v1.3.1\n    - Generated release notes comparing v1.3.1 vs v1.3.0\n    - Created GitHub Release (not pre-release)\n    - Stable artifacts published\n    - Auto-bumped to 1.3.2-beta.0 (ready for next patch)\n    - Main unchanged\n</code></pre>"},{"location":"community/project-specific/lance/release/#rc-iteration-due-to-issues","title":"RC Iteration Due to Issues","text":"<pre><code># 1. Create initial RC\nRC: v1.3.0-rc.1 on release/v1.3\n\n# 2. Issue found during testing\n# 3. Fix committed to release/v1.3 branch\n# 4. Create new RC\nWorkflow: Create RC\n  release_branch: release/v1.3\n  Result:\n    - Branch at 1.3.0-rc.1\n    - Auto-incremented to 1.3.0-rc.2\n    - Tagged v1.3.0-rc.2\n    - Created GitHub Pre-Release with release notes from release-root/1.3.0-beta.N to v1.3.0-rc.2 (same comparison as rc.1, showing all changes)\n    - GitHub Discussion created\n\n# 5. Vote passes\n# 6. Approve rc.2\nWorkflow: Approve RC\n  rc_tag: v1.3.0-rc.2\n  Result:\n    - release/v1.3 @ 1.3.0\n    - Tagged v1.3.0\n    - Generated release notes comparing v1.3.0 vs release-root/1.3.0-beta.N (includes fixes from rc.1 and rc.2)\n    - Created GitHub Release (not pre-release)\n    - Stable artifacts published\n    - Release branch auto-bumped to 1.3.1-beta.0\n    - Main unchanged\n</code></pre>"},{"location":"community/project-specific/lance/rust/","title":"Contributing to Rust","text":"<p>To format and lint Rust code:</p> <pre><code>cargo fmt --all\ncargo clippy --all-features --tests --benches\n</code></pre>"},{"location":"community/project-specific/lance/rust/#core-format","title":"Core Format","text":"<p>The core format is implemented in Rust under the <code>rust</code> directory. Once you've setup Rust you can build the core format with:</p> <pre><code>cargo build\n</code></pre> <p>This builds the debug build. For the optimized release build:</p> <pre><code>cargo build -r\n</code></pre> <p>To run the Rust unit tests:</p> <pre><code>cargo test\n</code></pre> <p>If you're working on a performance related feature, benchmarks can be run via:</p> <pre><code>cargo bench\n</code></pre> <p>If you want detailed logging and full backtraces, set the following environment variables. More details can be found here.</p> <pre><code>LANCE_LOG=info RUST_BACKTRACE=FULL &lt;cargo-commands&gt;\n</code></pre>"},{"location":"examples/python/artifact_management/","title":"Deep Learning Artifact Management using Lance","text":"<p>Along with datasets, Lance file format can also be used for saving and versioning deep learning model weights.  In fact deep learning artifact management can be made more streamlined (compared to vanilla weight saving methods) using Lance file format for PyTorch model weights.</p> <p>In this example we will be demonstrating how you save, version and load a PyTorch model's weights using Lance. More specifically we will be loading a pre-trained ResNet model, saving it in Lance file format, loading it back to PyTorch and verifying if the weights are still indeed the same. We will also be demonstrating how you can version your model weights in a single lance dataset thanks to our Zero-copy, automatic versioning.</p> <p>Key Idea: When you save a model's weights (read: state dictionary) in PyTorch, weights are stored as key-value pairs in an <code>OrderedDict</code> with the keys representing the weight's name and the value representing the corresponding weight tensor. To emulate this as closely as possible, we will be saving the weights in three columns. The first column will have the name of the weight, the second will have the weight itself but flattened in a list and the third will have the original shape of the weights so they can be reconstructed for loading into a model.</p>"},{"location":"examples/python/artifact_management/#imports-and-setup","title":"Imports and Setup","text":"<p>We will start by importing and loading all the necessary modules.</p> <pre><code>import os\nimport shutil\nimport lance\nimport pyarrow as pa\nimport torch\nfrom collections import OrderedDict\n</code></pre> <p>We will also define a <code>GLOBAL_SCHEMA</code> that will dictate how the weights table will look like.</p> <pre><code>GLOBAL_SCHEMA = pa.schema(\n    [\n        pa.field(\"name\", pa.string()),\n        pa.field(\"value\", pa.list_(pa.float64(), -1)),\n        pa.field(\"shape\", pa.list_(pa.int64(), -1)), # Is a list with variable shape because weights can have any number of dims\n    ]\n)\n</code></pre> <p>As we covered earlier, the weights table will have three columns - one for storing the weight name, one for storing the flattened weight value and one for storing the original weight shape for loading them back.</p>"},{"location":"examples/python/artifact_management/#saving-and-versioning-models","title":"Saving and Versioning Models","text":"<p>First we will focus on the model saving part. Let's start by writing a utility function that will take a model's state dict, goes over each weight, flatten it and then return the weight name, flattened weight and weight's original shape in a pyarrow <code>RecordBatch</code>.</p> <pre><code>def _save_model_writer(state_dict):\n    \"\"\"Yields a RecordBatch for each parameter in the model state dict\"\"\"\n    for param_name, param in state_dict.items():\n        param_shape = list(param.size())\n        param_value = param.flatten().tolist()\n        yield pa.RecordBatch.from_arrays(\n            [\n                pa.array(\n                    [param_name],\n                    pa.string(),\n                ),\n                pa.array(\n                    [param_value],\n                    pa.list_(pa.float64(), -1),\n                ),\n                pa.array(\n                    [param_shape],\n                    pa.list_(pa.int64(), -1),\n                ),\n            ],\n            [\"name\", \"value\", \"shape\"],\n        )\n</code></pre> <p>Now about versioning: Let's say you trained your model on some new data but don't want to overwrite your old checkpoint, you can now just save these newly trained model weights as a version in Lance weights dataset. This will allow you to load specific version of weights from one lance weight dataset instead of making separate folders for each model checkpoint to make.</p> <p>Let's write a function that handles the work for saving the model, whether with versions or without them.</p> <pre><code>def save_model(state_dict: OrderedDict, file_name: str, version=False):\n    \"\"\"Saves a PyTorch model in lance file format\n\n    Args:\n        state_dict (OrderedDict): Model state dict\n        file_name (str): Lance model name\n        version (bool): Whether to save as a new version or overwrite the existing versions,\n            if the lance file already exists\n    \"\"\"\n    # Create a reader\n    reader = pa.RecordBatchReader.from_batches(\n        GLOBAL_SCHEMA, _save_model_writer(state_dict)\n    )\n\n    if os.path.exists(file_name):\n        if version:\n            # If we want versioning, we use the overwrite mode to create a new version\n            lance.write_dataset(\n                reader, file_name, schema=GLOBAL_SCHEMA, mode=\"overwrite\"\n            )\n        else:\n            # If we don't want versioning, we delete the existing file and write a new one\n            shutil.rmtree(file_name)\n            lance.write_dataset(reader, file_name, schema=GLOBAL_SCHEMA)\n    else:\n        # If the file doesn't exist, we write a new one\n        lance.write_dataset(reader, file_name, schema=GLOBAL_SCHEMA)\n</code></pre> <p>The above function will take in the model state dict, the lance saved file name and the weights version. The function will start by making a <code>RecordBatchReader</code> using the global schema and the utility function we wrote above. If the weights lance dataset already exists in the directory, we will just save it as a new version (if versioning is enabled) or delete the old file and save the weights as new. Otherwise the weights saving will be done normally.</p>"},{"location":"examples/python/artifact_management/#loading-models","title":"Loading Models","text":"<p>Loading weights from a Lance weight dataset into a model is just the reverse of saving them. The key part is to reshape the flattened weights back to their original shape, which is easier thanks to the shape that you saved corresponding to the weights. We will divide this into three functions for better readability.</p> <p>The first function will be the <code>_load_weight</code> function which will take a \"weight\" retrieved from the Lance weight dataset and return the weight as a torch tensor in its original shape. The \"weight\" that we retrieve from the Lance weight dataset will be a dict with value corresponding to each column in form of a key.</p> <pre><code>def _load_weight(weight: dict) -&gt; torch.Tensor:\n    \"\"\"Converts a weight dict to a torch tensor\"\"\"\n    return torch.tensor(weight[\"value\"], dtype=torch.float64).reshape(weight[\"shape\"])\n</code></pre> <p>Optionally, you could also add an option to specify the datatype of the weights.</p> <p>The next function will be on loading all the weights from the lance weight dataset into a state dictionary, which is what PyTorch will expect when we load the weights into our model.</p> <pre><code>def _load_state_dict(file_name: str, version: int = 1, map_location=None) -&gt; OrderedDict:\n    \"\"\"Reads the model weights from lance file and returns a model state dict\n    If the model weights are too large, this function will fail with a memory error.\n\n    Args:\n        file_name (str): Lance model name\n        version (int): Version of the model to load\n        map_location (str): Device to load the model on\n\n    Returns:\n        OrderedDict: Model state dict\n    \"\"\"\n    ds = lance.dataset(file_name, version=version)\n    weights = ds.take([x for x in range(ds.count_rows())]).to_pylist()\n    state_dict = OrderedDict()\n\n    for weight in weights:\n        state_dict[weight[\"name\"]] = _load_weight(weight).to(map_location)\n\n    return state_dict\n</code></pre> <p>The <code>load_state_dict</code> function will expect a lance weight dataset file name, a version and a device where the weights will be loaded into.  We essentially load all the weights from the lance weight dataset into our memory and iteratively convert them into weights using the utility function we wrote earlier and then put them on the device.</p> <p>One thing to note here is that this function will fail if the saved weights are larger than memory. For the sake of simplicity, we assume the weights to be loaded can fit in the memory and we don't have to deal with any sharding.</p> <p>Finally, we will write a higher level function is the only one we will call to load the weights.</p> <pre><code>def load_model(\n    model: torch.nn.Module, file_name: str, version: int = 1, map_location=None\n):\n    \"\"\"Loads the model weights from lance file and sets them to the model\n\n    Args:\n        model (torch.nn.Module): PyTorch model\n        file_name (str): Lance model name\n        version (int): Version of the model to load\n        map_location (str): Device to load the model on\n    \"\"\"\n    state_dict = _load_state_dict(file_name, version=version, map_location=map_location)\n    model.load_state_dict(state_dict)\n</code></pre> <p>The <code>load_model</code> function will require the model, the lance weight dataset name, the version of weights to load in and the map location. This will just call the <code>_load_state_dict</code> utility to get the state dict and then load that state dict into the model.</p>"},{"location":"examples/python/artifact_management/#conclusion","title":"Conclusion","text":"<p>In conclusion, you only need to call the two functions: <code>save_model</code> and <code>load_model</code> to save and load the models respectively and as long as the weights can be fit in the memory and are in PyTorch, it should be fine.</p> <p>Although experimental, this approach defines a new way of doing deep learning artifact management. </p>"},{"location":"examples/python/clip_training/","title":"Training Multi-Modal models using a Lance dataset","text":"<p>In this example we will be training a CLIP model for natural image based search using a Lance image-text dataset.  In particular, we will be using the flickr_8k Lance dataset.</p> <p>The model architecture and part of the training code are adapted from Manan Goel's Implementing CLIP with PyTorch Lightning with necessary changes to for a minimal, lance-compatible training example.</p>"},{"location":"examples/python/clip_training/#imports-and-setup","title":"Imports and Setup","text":"<p>Along with Lance, we will be needing PyTorch and timm for our CLIP model to train.</p> <pre><code>import cv2\nimport lance\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nimport timm\nfrom transformers import AutoModel, AutoTokenizer\n\nimport itertools\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.simplefilter('ignore')\n</code></pre> <p>Now, we will define a Config class that will house all the hyper-parameters required for training.</p> <pre><code>class Config:\n    img_size = (128, 128)\n    bs = 32\n    head_lr = 1e-3\n    img_enc_lr = 1e-4\n    text_enc_lr = 1e-5\n    max_len = 18\n    img_embed_dim = 2048\n    text_embed_dim = 768\n    projection_dim = 256\n    temperature = 1.0\n    num_epochs = 2\n    img_encoder_model = 'resnet50'\n    text_encoder_model = 'bert-base-cased'\n</code></pre> <p>And also two utility functions that will help us load the images and texts from the lance dataset.  Remember, our Lance dataset has images, image names and all the captions for a given image. We only need the images and one of those captions.  For simplicity, when loading captions, we will be choosing the one that is the longest (with the rather naive assumption that it has more information about the image).</p> <pre><code>def load_image(ds, idx):\n    # Utility function to load an image at an index and convert it from bytes format to img format\n    raw_img = ds.take([idx], columns=['image']).to_pydict()\n    raw_img = np.frombuffer(b''.join(raw_img['image']), dtype=np.uint8)\n    img = cv2.imdecode(raw_img, cv2.IMREAD_COLOR)\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n    return img\n\ndef load_caption(ds, idx):\n    # Utility function to load an image's caption. Currently we return the longest caption of all\n    captions = ds.take([idx], columns=['captions']).to_pydict()['captions'][0]\n    return max(captions, key=len)\n</code></pre> <p>Since the images are stored as bytes in the lance dataset, the <code>load_image()</code> function will load the bytes corresponding to an image and then use numpy and opencv to convert it into an image.</p>"},{"location":"examples/python/clip_training/#dataset-and-augmentations","title":"Dataset and Augmentations","text":"<p>Since our CLIP model will expect images of same size and tokenized captions, we will define a custom PyTorch dataset that will take the lance dataset path along with any augmentation (for the image) and return a pre-processed image and a tokenized caption (as a dictionary).</p> <pre><code>class CLIPLanceDataset(Dataset):\n    \"\"\"Custom Dataset to load images and their corresponding captions\"\"\"\n    def __init__(self, lance_path, max_len=18, tokenizer=None, transforms=None):\n        self.ds = lance.dataset(lance_path)\n        self.max_len = max_len\n        # Init a new tokenizer if not specified already\n        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-cased') if not tokenizer else tokenizer\n        self.transforms = transforms\n\n    def __len__(self):\n        return self.ds.count_rows()\n\n    def __getitem__(self, idx):\n        # Load the image and caption\n        img = load_image(self.ds, idx)\n        caption = load_caption(self.ds, idx)\n\n        # Apply transformations to the images\n        if self.transforms:\n            img = self.transforms(img)\n\n        # Tokenize the caption\n        caption = self.tokenizer(\n            caption,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n        # Flatten each component of tokenized caption otherwise they will cause size mismatch errors during training\n        caption = {k: v.flatten() for k, v in caption.items()}\n\n        return img, caption\n</code></pre> <p>Now that our custom dataset is ready, we also define some very basic augmentations for our images.</p> <pre><code>train_augments = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Resize(Config.img_size),\n        transforms.Normalize([0.5], [0.5]),\n    ]\n)\n</code></pre> <p>The transformations are very basic: resizing all the images to be of the same shape and then normalizing them to stabilize the training later on.</p>"},{"location":"examples/python/clip_training/#model-and-setup","title":"Model and Setup","text":"<p>Since we our training a CLIP model, we have the following: * <code>ImageEncoder</code> that uses a pre-trained vision model (<code>resnet50</code> in this case) to convert images into feature vectors. * <code>TextEncoder</code> that uses a pre-trained language model (<code>bert-base-cased</code> in this case) to transform text captions into feature vectors. * <code>Head</code> which is a Projection module projects these feature vectors into a common embedding space.</p> <p>Going into deeper details of the CLIP model and its architectural nuances are out of the scope of this example, however if you wish to read more on it, you can read the official paper here.</p> <p>Now that we have understood the general summary of the model, let's define all the required modules.</p> <pre><code>class ImageEncoder(nn.Module):\n    \"\"\"Encodes the Image\"\"\"\n    def __init__(self, model_name, pretrained = True):\n        super().__init__()\n        self.backbone = timm.create_model(\n            model_name,\n            pretrained=pretrained,\n            num_classes=0,\n            global_pool=\"avg\"\n        )\n\n        for param in self.backbone.parameters():\n            param.requires_grad = True\n\n    def forward(self, img):\n        return self.backbone(img)\n\nclass TextEncoder(nn.Module):\n    \"\"\"Encodes the Caption\"\"\"\n    def __init__(self, model_name):\n        super().__init__()\n\n        self.backbone = AutoModel.from_pretrained(model_name)\n\n        for param in self.backbone.parameters():\n            param.requires_grad = True\n\n    def forward(self, captions):\n        output = self.backbone(**captions)\n        return output.last_hidden_state[:, 0, :]\n\nclass Head(nn.Module):\n    \"\"\"Projects both into Embedding space\"\"\"\n    def __init__(self, embedding_dim, projection_dim):\n        super().__init__()\n        self.projection = nn.Linear(embedding_dim, projection_dim)\n        self.gelu = nn.GELU()\n        self.fc = nn.Linear(projection_dim, projection_dim)\n\n        self.dropout = nn.Dropout(0.3)\n        self.layer_norm = nn.LayerNorm(projection_dim)\n\n    def forward(self, x):\n        projected = self.projection(x)\n        x = self.gelu(projected)\n        x = self.fc(x)\n        x = self.dropout(x)\n        x += projected\n\n        return self.layer_norm(x)\n</code></pre> <p>Along with the model definition, we will be defining two utility functions to simplify the training: <code>forward()</code> which will do one forward pass through the combined models and <code>loss_fn()</code> which will take the image and text embeddings output from <code>forward</code> function and then calculate the loss using them.</p> <pre><code>def loss_fn(img_embed, text_embed, temperature=0.2):\n    \"\"\"\n    https://arxiv.org/abs/2103.00020/\n    \"\"\"\n    # Calculate logits, image similarity and text similarity\n    logits = (text_embed @ img_embed.T) / temperature\n    img_sim = img_embed @ img_embed.T\n    text_sim = text_embed @ text_embed.T\n    # Calculate targets by taking the softmax of the similarities\n    targets = F.softmax(\n        (img_sim + text_sim) / 2 * temperature, dim=-1\n    )\n    img_loss = (-targets.T * nn.LogSoftmax(dim=-1)(logits.T)).sum(1)\n    text_loss = (-targets * nn.LogSoftmax(dim=-1)(logits)).sum(1)\n    return (img_loss + text_loss) / 2.0\n\ndef forward(img, caption):\n    # Transfer to device\n    img = img.to('cuda')\n    for k, v in caption.items():\n        caption[k] = v.to('cuda')\n\n    # Get embeddings for both img and caption\n    img_embed = img_head(img_encoder(img))\n    text_embed = text_head(text_encoder(caption))\n\n    return img_embed, text_embed\n</code></pre> <p>In order for us to train, we will define the models, tokenizer and the optimizer to be used in the next section</p> <pre><code># Define image encoder, image head, text encoder, text head and a tokenizer for tokenizing the caption\nimg_encoder = ImageEncoder(model_name=Config.img_encoder_model).to('cuda')\nimg_head = Head(Config.img_embed_dim, Config.projection_dim).to('cuda')\n\ntokenizer = AutoTokenizer.from_pretrained(Config.text_encoder_model)\ntext_encoder = TextEncoder(model_name=Config.text_encoder_model).to('cuda')\ntext_head = Head(Config.text_embed_dim, Config.projection_dim).to('cuda')\n\n# Since we are optimizing two different models together, we will define parameters manually\nparameters = [\n    {\"params\": img_encoder.parameters(), \"lr\": Config.img_enc_lr},\n    {\"params\": text_encoder.parameters(), \"lr\": Config.text_enc_lr},\n    {\n        \"params\": itertools.chain(\n            img_head.parameters(),\n            text_head.parameters(),\n        ),\n        \"lr\": Config.head_lr,\n    },\n]\n\noptimizer = torch.optim.Adam(parameters)\n</code></pre>"},{"location":"examples/python/clip_training/#training","title":"Training","text":"<p>Before we actually train the model, one last step remains: which is to initialize our Lance dataset and a dataloader.</p> <pre><code># We assume the flickr8k.lance dataset is in the same directory\ndataset = CLIPLanceDataset(\n    lance_path=\"flickr8k.lance\",\n    max_len=Config.max_len,\n    tokenizer=tokenizer,\n    transforms=train_augments\n)\n\ndataloader = DataLoader(\n    dataset,\n    shuffle=False,\n    batch_size=Config.bs,\n    pin_memory=True\n)\n</code></pre> <p>Now that our dataloader is initialized, let's train the model.</p> <pre><code>img_encoder.train()\nimg_head.train()\ntext_encoder.train()\ntext_head.train()\n\nfor epoch in range(Config.num_epochs):\n    print(f\"{'='*20} Epoch: {epoch+1} / {Config.num_epochs} {'='*20}\")\n\n    prog_bar = tqdm(dataloader)\n    for img, caption in prog_bar:\n        optimizer.zero_grad(set_to_none=True)\n\n        img_embed, text_embed = forward(img, caption)\n        loss = loss_fn(img_embed, text_embed, temperature=Config.temperature).mean()\n\n        loss.backward()\n        optimizer.step()\n\n        prog_bar.set_description(f\"loss: {loss.item():.4f}\")\n    print()\n</code></pre> <p>The training loop is quite self-explanatory. We set image encoder, image head, text encoder and text head models to training mode.  Then in each epoch, we iterate over our lance dataset, training the model and reporting the lance to the progress bar.</p> <pre><code>==================== Epoch: 1 / 2 ====================\nloss: 2.0799: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 253/253 [02:14&lt;00:00,  1.88it/s]\n\n==================== Epoch: 2 / 2 ====================\nloss: 1.3064: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 253/253 [02:10&lt;00:00,  1.94it/s]\n</code></pre> <p>And that's basically it! Using Lance dataset for training any type of model is very similar to using any other type of dataset but it also comes with increased speed and ease of use! </p>"},{"location":"examples/python/flickr8k_dataset_creation/","title":"Creating Multi-Modal datasets using Lance","text":"<p>Thanks to Lance file format's ability to store data of different modalities, one of the important use-cases that Lance shines in is storing Multi-modal datasets. In this brief example we will be going over how you can take a Multi-modal dataset and store it in Lance file format. </p> <p>The dataset of choice here is Flickr8k dataset. Flickr8k is a benchmark collection for sentence-based image description and search, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events.  The images were chosen from six different Flickr groups, and tend not to contain any well-known people or locations, but were manually selected to depict a variety of scenes and situations.</p> <p>We will be creating an Image-caption pair dataset for Multi-modal model training by using the above mentioned Flickr8k dataset, saving it in form of a Lance dataset with image file names, all captions for every image (order preserved) and the image itself (in binary format).</p>"},{"location":"examples/python/flickr8k_dataset_creation/#imports-and-setup","title":"Imports and Setup","text":"<p>We assume that you downloaded the dataset, more specifically the \"Flickr8k.token.txt\" file and the \"Flicker8k_Dataset/\" folder and both are present in the current directory. These can be downloaded from here (download both the dataset and text zip files).</p> <p>We also assume you have pyarrow and pylance installed as well as opencv (for reading in images) and tqdm (for pretty progress bars).</p> <p>Now let's start with imports and defining the caption file and image dataset folder.</p> <pre><code>import os\nimport cv2\nimport random\n\nimport lance\nimport pyarrow as pa\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm.auto import tqdm\n\ncaptions = \"Flickr8k.token.txt\"\nimage_folder = \"Flicker8k_Dataset/\"\n</code></pre>"},{"location":"examples/python/flickr8k_dataset_creation/#loading-and-processing","title":"Loading and Processing","text":"<p>In flickr8k dataset, each image has multiple corresponding captions that are ordered.  We are going to put all these captions in a list corresponding to each image with their position in the list representing the order in which they originally appear. Let's load the annotations (the image path and corresponding captions) in a list with each element of the list being a tuple consisting of image name, caption number and caption itself.</p> <pre><code>with open(captions, \"r\") as fl:\n    annotations = fl.readlines()\n\n# Converts the annotations where each element of this list is a tuple consisting of image file name, caption number and caption itself\nannotations = list(map(lambda x: tuple([*x.split('\\t')[0].split('#'), x.split('\\t')[1]]), annotations))\n</code></pre> <p>Now, for all captions of the same image, we will put them in a list sorted by their ordering.</p> <pre><code>captions = []\nimage_ids = set(ann[0] for ann in annotations)\nfor img_id in tqdm(image_ids):\n    current_img_captions = []\n    for ann_img_id, num, caption in annotations:\n        if img_id == ann_img_id:\n            current_img_captions.append((num, caption))\n\n    # Sort by the annotation number\n    current_img_captions.sort(key=lambda x: x[0])\n    captions.append((img_id, tuple([x[1] for x in current_img_captions])))\n</code></pre>"},{"location":"examples/python/flickr8k_dataset_creation/#converting-to-a-lance-dataset","title":"Converting to a Lance Dataset","text":"<p>Now that our captions list is in a proper format, we will write a <code>process()</code> function that will take the said captions as argument and yield a Pyarrow record batch consisting of the <code>image_id</code>, <code>image</code> and <code>captions</code>. The image in this record batch will be in binary format and all the captions for an image will be in a list with their ordering preserved.</p> <pre><code>def process(captions):\n    for img_id, img_captions in tqdm(captions):\n        try:\n            with open(os.path.join(image_folder, img_id), 'rb') as im:\n                binary_im = im.read()\n\n        except FileNotFoundError:\n            print(f\"img_id '{img_id}' not found in the folder, skipping.\")\n            continue\n\n        img_id = pa.array([img_id], type=pa.string())\n        img = pa.array([binary_im], type=pa.binary())\n        capt = pa.array([img_captions], pa.list_(pa.string(), -1))\n\n        yield pa.RecordBatch.from_arrays(\n            [img_id, img, capt], \n            [\"image_id\", \"image\", \"captions\"]\n        )\n</code></pre> <p>Let's also define the same schema to tell Pyarrow the type of data it should be expecting in the table.</p> <pre><code>schema = pa.schema([\n    pa.field(\"image_id\", pa.string()),\n    pa.field(\"image\", pa.binary()),\n    pa.field(\"captions\", pa.list_(pa.string(), -1)),\n])\n</code></pre> <p>We are including the <code>image_id</code> (which is the original image name) so it can be easier to reference and debug in the future.</p> <p>Finally, we define a reader to iteratively read those record batches and then write them to a lance dataset on the disk.</p> <pre><code>reader = pa.RecordBatchReader.from_batches(schema, process(captions))\nlance.write_dataset(reader, \"flickr8k.lance\", schema)\n</code></pre> <p>And that's basically it! If you want to execute this in a notebook form, you can check out this example in our deeplearning-recipes repository here.</p> <p>For more Deep learning related examples using Lance dataset, be sure to check out the lance-deeplearning-recipes repository! </p>"},{"location":"examples/python/llm_dataset_creation/","title":"Creating text dataset for LLM training using Lance","text":"<p>Lance can be used for creating and caching a text (or code) dataset for pre-training / fine-tuning of Large Language Models. The need for this arises when one needs to train a model on a subset of data or process the data in chunks without downloading all of it on the disk at once. This becomes a considerable problem when you just want a subset of a Terabyte or Petabyte-scale dataset.</p> <p>In this example, we will be bypassing this problem by downloading a text dataset in parts, tokenizing it and saving it as a Lance dataset.  This can be done for as many or as few data samples as you wish with average memory consumption approximately 3-4 GBs!</p> <p>For this example, we are working with the wikitext dataset, which is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia.</p>"},{"location":"examples/python/llm_dataset_creation/#preparing-and-pre-processing-the-raw-dataset","title":"Preparing and pre-processing the raw dataset","text":"<p>Let's first define the dataset and the tokenizer</p> <pre><code>import lance\nimport pyarrow as pa\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom tqdm.auto import tqdm  # optional for progress tracking\n\ntokenizer = AutoTokenizer.from_pretrained('gpt2')\n\ndataset = load_dataset('wikitext', 'wikitext-103-raw-v1', streaming=True)['train']\ndataset = dataset.shuffle(seed=1337)\n</code></pre> <p>The <code>streaming</code> argument in <code>load_dataset</code> is especially important because if you run it without setting it to  <code>True</code>, the datasets library will download the entire dataset first, even though you only wish to use a subset of it. With <code>streaming</code> set to <code>True</code>, the samples will be downloaded as they are needed.</p> <p>Now we will define a function to help us with tokenizing our samples, one-by-one.</p> <pre><code>def tokenize(sample, field='text'):\n    return tokenizer(sample[field])['input_ids']\n</code></pre> <p>This function will receive a sample from a huggingface dataset and tokenize the values in the <code>field</code> column. This is the main text you want  to tokenize.</p>"},{"location":"examples/python/llm_dataset_creation/#creating-a-lance-dataset","title":"Creating a Lance dataset","text":"<p>Now that we have set up our raw dataset and pre-processing code,  let's define the main function that takes in the dataset, number of samples and field, and returns a pyarrow batch that will later be written into a lance dataset.</p> <pre><code>def process_samples(dataset, num_samples=100_000, field='text'):\n    current_sample = 0\n    for sample in tqdm(dataset, total=num_samples):\n        # If we have added all 5M samples, stop\n        if current_sample == num_samples:\n            break\n        if not sample[field]:\n            continue\n        # Tokenize the current sample\n        tokenized_sample = tokenize(sample, field)\n        # Increment the counter\n        current_sample += 1\n        # Yield a PyArrow RecordBatch\n        yield pa.RecordBatch.from_arrays(\n            [tokenized_sample], \n            names=[\"input_ids\"]\n        )\n</code></pre> <p>This function will be iterating over the huggingface dataset, one sample at a time, tokenizing the sample and yielding a pyarrow <code>RecordBatch</code> with all the tokens. We will do this until we have reached the <code>num_samples</code> number of samples or the end of the dataset, whichever comes first.</p> <p>Please note that by 'sample', we mean one example (row) in the original dataset. What one example exactly means will depend on the dataset itself as it could  be one line or an entire file of text. In this example, it varies in length between a line and a paragraph of text.</p> <p>We also need to define a schema to tell Lance what type of data we are expecting in our table. Since our dataset consists only of tokens which are long integers, <code>int64</code> is the suitable datatype.</p> <pre><code>schema = pa.schema([\n    pa.field(\"input_ids\", pa.int64())\n])\n</code></pre> <p>Finally, we need to define a <code>reader</code> that will be reading a stream of record batches from our <code>process_samples</code> function that yields  said record batches consisting of individual tokenized samples.</p> <pre><code>reader = pa.RecordBatchReader.from_batches(\n    schema, \n    process_samples(dataset, num_samples=500_000, field='text') # For 500K samples\n)\n</code></pre> <p>And finally we use the <code>lance.write_dataset</code> which will write the dataset to the disk.</p> <pre><code># Write the dataset to disk\nlance.write_dataset(\n    reader, \n    \"wikitext_500K.lance\",\n    schema\n)\n</code></pre> <p>If you want to apply some other pre-processing to the tokens before saving it to the disk (like masking, etc), you may add it in the  <code>process_samples</code> function.</p> <p>And that's it! Your dataset has been tokenized and saved to the disk! </p>"},{"location":"examples/python/llm_training/","title":"Training LLMs using a Lance text dataset","text":"<p>Using a Lance text dataset for pre-training / fine-tuning a Large Language model is straightforward and memory-efficient. This example follows up on the Creating text dataset for LLM training using Lance example. Check it out if you haven't already.</p> <p>In this example, we will be training an LLM using \ud83e\udd17 transformers on the tokenized \"wikitext_500K\" lance dataset we created in the aforementioned example.</p>"},{"location":"examples/python/llm_training/#imports-and-setup","title":"Imports and Setup","text":"<p>Let's setup our environment by doing all the necessary imports and defining a few basic things.</p> <pre><code>import numpy as np\nimport lance\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, Sampler\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom tqdm.auto import tqdm\n\n# We'll be training the pre-trained GPT2 model in this example\nmodel_name = 'gpt2'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Also define some hyperparameters\nlr = 3e-4\nnb_epochs = 10\nblock_size = 1024\nbatch_size = 8\ndevice = 'cuda:0'\ndataset_path = 'wikitext_500K.lance'\n</code></pre> <p>Now that the basic setup is out of the way, let's define our custom Dataset and a Sampler for streaming the tokens from our Lance dataset.</p>"},{"location":"examples/python/llm_training/#data-loading-setup","title":"Data-loading Setup","text":"<p>We start by defining a utility function that will help us load any number of tokens from our lance dataset in a 'chunk'.</p> <pre><code>def from_indices(dataset, indices):\n    \"\"\"Load the elements on given indices from the dataset\"\"\"\n    chunk = dataset.take(indices).to_pylist()\n    chunk = list(map(lambda x: x['input_ids'], chunk))\n    return chunk\n</code></pre> <p>Now let's define our custom dataset and sampler for loading the tokens.</p> <pre><code>class LanceDataset(Dataset):\n    def __init__(\n        self,\n        dataset_path,\n        block_size,\n    ):\n        # Load the lance dataset from the saved path\n        self.ds = lance.dataset(dataset_path)\n        self.block_size = block_size\n\n        # Doing this so the sampler never asks for an index at the end of text\n        self.length = self.ds.count_rows() - block_size\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Generate a window of indices starting from the current idx to idx+block_size\n        and return the tokens at those indices\n        \"\"\"\n        window = np.arange(idx, idx + self.block_size)\n        sample = from_indices(self.ds, window)\n\n        return {\"input_ids\": torch.tensor(sample), \"labels\": torch.tensor(sample)}\n</code></pre> <p>When given a random index by the sampler, the dataset will load the next <code>block_size</code> number of tokens starting from current index. This would in-essence form a sample as the loaded tokens would be causal.</p> <p>However we also need to make sure that the tokens we get from the dataset aren't overlapping. Let's understand this from an example:</p> <p>Let's say, for some arbitrary block size, during the training loop the dataset return the following tokens:</p> <p><code>\"Vienna is the capital of Austria\"</code> at index = 12 for sample #1, and,</p> <p><code>\"is the capital of Austria and\"</code> at index = 13 for sample #2, and so on</p> <p>The problem here is that if we allow the dataloader to fetch the 'samples' for any arbitrary number of indices, they may overlap (as we see above). This is not good for the model as it may start to overfit after seeing sufficient overlapping tokens.</p> <p>To solve this problem, we define a custom Sampler that only returns the indices that are 'block_size' apart from each other, ensuring that we don't see any overlapping samples.</p> <pre><code>class LanceSampler(Sampler):\n    r\"\"\"Samples tokens randomly but `block_size` indices apart.\n\n    Args:\n        data_source (Dataset): dataset to sample from\n        block_size (int): minimum index distance between each random sample\n    \"\"\"\n\n    def __init__(self, data_source, block_size=512):\n        self.data_source = data_source\n        self.num_samples = len(self.data_source)\n        self.available_indices = list(range(0, self.num_samples, block_size))\n        np.random.shuffle(self.available_indices)\n\n    def __iter__(self):\n        yield from self.available_indices\n\n    def __len__(self) -&gt; int:\n        return len(self.available_indices)\n</code></pre> <p>Now when we fetch the tokens from our dataset with sampler being the <code>LanceSampler</code>, all samples in all the batches that our model sees during the training are guaranteed to be non-overlapping.</p> <p>This is done by generating a list of indices starting from 0 to the end of the dataset (which if you remember is lance dataset length - block size) with each index 'block_size' apart from the other. We then shuffle this list and yield indices from it.</p> <p>And that's basically it for the Dataloading! Now all we are left is to train the model!</p>"},{"location":"examples/python/llm_training/#model-training","title":"Model Training","text":"<p>Now you train the model just like you would with any other dataset!</p> <pre><code># Define the dataset, sampler and dataloader\ndataset = LanceDataset(dataset_path, block_size)\nsampler = LanceSampler(dataset, block_size)\ndataloader = DataLoader(\n    dataset,\n    shuffle=False,\n    batch_size=batch_size,\n    sampler=sampler,\n    pin_memory=True\n)\n\n# Define the optimizer, training loop and train the model!\nmodel = model.to(device)\nmodel.train()\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n\nfor epoch in range(nb_epochs):\n    print(f\"========= Epoch: {epoch+1} / {nb_epochs} =========\")\n    epoch_loss = []\n    prog_bar = tqdm(dataloader, total=len(dataloader))\n    for batch in prog_bar:\n        optimizer.zero_grad(set_to_none=True)\n\n        # Put both input_ids and labels to the device\n        for k, v in batch.items():\n            batch[k] = v.to(device)\n\n        # Perform one forward pass and get the loss\n        outputs = model(**batch)\n        loss = outputs.loss\n\n        # Perform backward pass\n        loss.backward()\n        optimizer.step()\n\n        prog_bar.set_description(f\"loss: {loss.item():.4f}\")\n\n        epoch_loss.append(loss.item())\n\n    # Calculate training perplexity for this epoch\n    try:\n        perplexity = np.exp(np.mean(epoch_loss))\n    except OverflowError:\n        perplexity = float(\"-inf\")\n\n    print(f\"train_perplexity: {perplexity}\")\n</code></pre> <p>One tip: If your lance dataset is huge (like the wikitext_500K is), and you want to debug the model to look out for errors, you may want to wrap the dataloader in an <code>iter()</code> function and only run it for a couple batches.</p> <p>And that's basically it!</p> <p>The best part about using Lance, the custom Dataset and Sampler is that you get a whopping 95% average GPU utilisation and minimal CPU overhead thanks to the lightning fast random access that Lance provides \ud83d\ude80 </p>"},{"location":"examples/rust/hnsw/","title":"Indexing a dataset with HNSW (Hierarchical Navigable Small World)","text":"<p>HNSW is a graph based algorithm for approximate neighbor search in high-dimensional spaces. In this example, we will demonstrate how to build an HNSW vector index against a Lance dataset.</p> <p>This example will show how to:</p> <ol> <li>Generate synthetic test data of specified dimensions</li> <li>Build a hierarchical graph structure for efficient vector search using Lance API</li> <li>Perform vector search with different parameters and compute the ground truth using L2 distance search</li> </ol>"},{"location":"examples/rust/hnsw/#complete-example","title":"Complete Example","text":"<pre><code>use std::collections::HashSet;\nuse std::sync::Arc;\n\nuse arrow::array::{types::Float32Type, Array, FixedSizeListArray};\nuse arrow::array::{AsArray, FixedSizeListBuilder, Float32Builder};\nuse arrow::datatypes::{DataType, Field, Schema};\nuse arrow::record_batch::RecordBatch;\nuse arrow::record_batch::RecordBatchIterator;\nuse arrow_select::concat::concat;\nuse futures::stream::StreamExt;\nuse lance::Dataset;\nuse lance_index::vector::v3::subindex::IvfSubIndex;\nuse lance_index::vector::{\n    flat::storage::FlatFloatStorage,\n    hnsw::{builder::HnswBuildParams, HNSW},\n};\nuse lance_linalg::distance::DistanceType;\n\nfn ground_truth(fsl: &amp;FixedSizeListArray, query: &amp;[f32], k: usize) -&gt; HashSet&lt;u32&gt; {\n    let mut dists = vec![];\n    for i in 0..fsl.len() {\n        let dist = lance_linalg::distance::l2_distance(\n            query,\n            fsl.value(i).as_primitive::&lt;Float32Type&gt;().values(),\n        );\n        dists.push((dist, i as u32));\n    }\n    dists.sort_by(|a, b| a.0.partial_cmp(&amp;b.0).unwrap());\n    dists.truncate(k);\n    dists.into_iter().map(|(_, i)| i).collect()\n}\n\npub async fn create_test_vector_dataset(output: &amp;str, num_rows: usize, dim: i32) {\n    let schema = Arc::new(Schema::new(vec![Field::new(\n        \"vector\",\n        DataType::FixedSizeList(Arc::new(Field::new(\"item\", DataType::Float32, true)), dim),\n        false,\n    )]));\n\n    let mut batches = Vec::new();\n\n    // Create a few batches\n    for _ in 0..2 {\n        let v_builder = Float32Builder::new();\n        let mut list_builder = FixedSizeListBuilder::new(v_builder, dim);\n\n        for _ in 0..num_rows {\n            for _ in 0..dim {\n                list_builder.values().append_value(rand::random::&lt;f32&gt;());\n            }\n            list_builder.append(true);\n        }\n        let array = Arc::new(list_builder.finish());\n        let batch = RecordBatch::try_new(schema.clone(), vec![array]).unwrap();\n        batches.push(batch);\n    }\n    let batch_reader = RecordBatchIterator::new(batches.into_iter().map(Ok), schema.clone());\n    println!(\"Writing dataset to {}\", output);\n    Dataset::write(batch_reader, output, None).await.unwrap();\n}\n\n#[tokio::main]\nasync fn main() {\n    let uri: Option&lt;String&gt; = None; // None means generate test data\n    let column = \"vector\";\n    let ef = 100;\n    let max_edges = 30;\n    let max_level = 7;\n\n    // 1. Generate a synthetic test data of specified dimensions\n    let dataset = if uri.is_none() {\n        println!(\"No uri is provided, generating test dataset...\");\n        let output = \"test_vectors.lance\";\n        create_test_vector_dataset(output, 1000, 64).await;\n        Dataset::open(output).await.expect(\"Failed to open dataset\")\n    } else {\n        Dataset::open(uri.as_ref().unwrap())\n            .await\n            .expect(\"Failed to open dataset\")\n    };\n\n    println!(\"Dataset schema: {:#?}\", dataset.schema());\n    let batches = dataset\n        .scan()\n        .project(&amp;[column])\n        .unwrap()\n        .try_into_stream()\n        .await\n        .unwrap()\n        .then(|batch| async move { batch.unwrap().column_by_name(column).unwrap().clone() })\n        .collect::&lt;Vec&lt;_&gt;&gt;()\n        .await;\n    let arrs = batches.iter().map(|b| b.as_ref()).collect::&lt;Vec&lt;_&gt;&gt;();\n    let fsl = concat(&amp;arrs).unwrap().as_fixed_size_list().clone();\n    println!(\"Loaded {:?} batches\", fsl.len());\n\n    let vector_store = Arc::new(FlatFloatStorage::new(fsl.clone(), DistanceType::L2));\n\n    let q = fsl.value(0);\n    let k = 10;\n    let gt = ground_truth(&amp;fsl, q.as_primitive::&lt;Float32Type&gt;().values(), k);\n\n    for ef_construction in [15, 30, 50] {\n        let now = std::time::Instant::now();\n        // 2. Build a hierarchical graph structure for efficient vector search using Lance API\n        let hnsw = HNSW::index_vectors(\n            vector_store.as_ref(),\n            HnswBuildParams::default()\n                .max_level(max_level)\n                .num_edges(max_edges)\n                .ef_construction(ef_construction),\n        )\n        .unwrap();\n        let construct_time = now.elapsed().as_secs_f32();\n        let now = std::time::Instant::now();\n        // 3. Perform vector search with different parameters and compute the ground truth using L2 distance search\n        let results: HashSet&lt;u32&gt; = hnsw\n            .search_basic(q.clone(), k, ef, None, vector_store.as_ref())\n            .unwrap()\n            .iter()\n            .map(|node| node.id)\n            .collect();\n        let search_time = now.elapsed().as_micros();\n        println!(\n            \"level={}, ef_construct={}, ef={} recall={}: construct={:.3}s search={:.3} us\",\n            max_level,\n            ef_construction,\n            ef,\n            results.intersection(&amp;gt).count() as f32 / k as f32,\n            construct_time,\n            search_time\n        );\n    }\n}\n</code></pre>"},{"location":"examples/rust/llm_dataset_creation/","title":"Creating text dataset for LLM training using Lance in Rust","text":"<p>In this example, we will demonstrate how to achieve the Python example - LLM dataset creation shown in the Python examples in Rust.</p> <p>Note</p> <p>The huggingface Python API supports loading data in streaming mode and shuffling is provided as a builtin feature. Rust API lacks these feature thus the data are manually downloaded and shuffled within each batch.</p> <p>This example will show how to:</p> <ol> <li>Download and process a text dataset in parts from huggingface</li> <li>Tokenize the text data with a custom RecordBatchReader</li> <li>Save it as a Lance dataset using Lance API</li> </ol> <p>The implementation details in Rust will follow similar concepts as the Python version, but with Rust-specific APIs and patterns which are significantly more verbose.</p>"},{"location":"examples/rust/llm_dataset_creation/#complete-example","title":"Complete Example","text":"<pre><code>use arrow::array::{Array, Int64Builder, ListBuilder, UInt32Array};\nuse arrow::datatypes::{DataType, Field, Schema};\nuse arrow::record_batch::RecordBatch;\nuse arrow::record_batch::RecordBatchReader;\nuse futures::StreamExt;\nuse hf_hub::{api::sync::Api, Repo, RepoType};\nuse lance::dataset::WriteParams;\nuse parquet::arrow::arrow_reader::ParquetRecordBatchReaderBuilder;\nuse rand::seq::SliceRandom;\nuse rand::SeedableRng;\nuse std::error::Error;\nuse std::fs::File;\nuse std::io::Write;\nuse std::sync::Arc;\nuse tempfile::NamedTempFile;\nuse tokenizers::Tokenizer;\n\n// Implement a custom stream batch reader\nstruct WikiTextBatchReader {\n    schema: Arc&lt;Schema&gt;,\n    parquet_readers: Vec&lt;Option&lt;ParquetRecordBatchReaderBuilder&lt;File&gt;&gt;&gt;,\n    current_reader_idx: usize,\n    current_reader: Option&lt;Box&lt;dyn RecordBatchReader + Send&gt;&gt;,\n    tokenizer: Tokenizer,\n    num_samples: u64,\n    cur_samples_cnt: u64,\n}\n\nimpl WikiTextBatchReader {\n    fn new(\n        parquet_readers: Vec&lt;ParquetRecordBatchReaderBuilder&lt;File&gt;&gt;,\n        tokenizer: Tokenizer,\n        num_samples: Option&lt;u64&gt;,\n    ) -&gt; Result&lt;Self, Box&lt;dyn Error + Send + Sync&gt;&gt; {\n        let schema = Arc::new(Schema::new(vec![Field::new(\n            \"input_ids\",\n            DataType::List(Arc::new(Field::new(\"item\", DataType::Int64, true))),\n            false,\n        )]));\n\n        Ok(Self {\n            schema,\n            parquet_readers: parquet_readers.into_iter().map(Some).collect(),\n            current_reader_idx: 0,\n            current_reader: None,\n            tokenizer,\n            num_samples: num_samples.unwrap_or(100_000),\n            cur_samples_cnt: 0,\n        })\n    }\n\n    fn process_batch(\n        &amp;mut self,\n        input_batch: &amp;RecordBatch,\n    ) -&gt; Result&lt;RecordBatch, arrow::error::ArrowError&gt; {\n        let num_rows = input_batch.num_rows();\n        let mut token_builder = ListBuilder::new(Int64Builder::with_capacity(num_rows * 1024)); // Pre-allocate space\n        let mut should_break = false;\n\n        let column = input_batch.column_by_name(\"text\").unwrap();\n        let string_array = column\n            .as_any()\n            .downcast_ref::&lt;arrow::array::StringArray&gt;()\n            .unwrap();\n        for i in 0..num_rows {\n            if self.cur_samples_cnt &gt;= self.num_samples {\n                should_break = true;\n                break;\n            }\n            if !Array::is_null(string_array, i) {\n                let text = string_array.value(i);\n                // Split paragraph into lines\n                for line in text.split('\\n') {\n                    if let Ok(encoding) = self.tokenizer.encode(line, true) {\n                        let tb_values = token_builder.values();\n                        for &amp;id in encoding.get_ids() {\n                            tb_values.append_value(id as i64);\n                        }\n                        token_builder.append(true);\n                        self.cur_samples_cnt += 1;\n                        if self.cur_samples_cnt % 5000 == 0 {\n                            println!(\"Processed {} rows\", self.cur_samples_cnt);\n                        }\n                        if self.cur_samples_cnt &gt;= self.num_samples {\n                            should_break = true;\n                            break;\n                        }\n                    }\n                }\n            }\n        }\n\n        // Create array and shuffle it\n        let input_ids_array = token_builder.finish();\n\n        // Create shuffled array by randomly sampling indices\n        let mut rng = rand::rngs::StdRng::seed_from_u64(1337);\n        let len = input_ids_array.len();\n        let mut indices: Vec&lt;u32&gt; = (0..len as u32).collect();\n        indices.shuffle(&amp;mut rng);\n\n        // Take values in shuffled order\n        let indices_array = UInt32Array::from(indices);\n        let shuffled = arrow::compute::take(&amp;input_ids_array, &amp;indices_array, None)?;\n\n        let batch = RecordBatch::try_new(self.schema.clone(), vec![Arc::new(shuffled)]);\n        if should_break {\n            println!(\"Stop at {} rows\", self.cur_samples_cnt);\n            self.parquet_readers.clear();\n            self.current_reader = None;\n        }\n\n        batch\n    }\n}\n\nimpl RecordBatchReader for WikiTextBatchReader {\n    fn schema(&amp;self) -&gt; Arc&lt;Schema&gt; {\n        self.schema.clone()\n    }\n}\n\nimpl Iterator for WikiTextBatchReader {\n    type Item = Result&lt;RecordBatch, arrow::error::ArrowError&gt;;\n    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {\n        loop {\n            // If we have a current reader, try to get next batch\n            if let Some(reader) = &amp;mut self.current_reader {\n                if let Some(batch_result) = reader.next() {\n                    return Some(batch_result.and_then(|batch| self.process_batch(&amp;batch)));\n                }\n            }\n\n            // If no current reader or current reader is exhausted, try to get next reader\n            if self.current_reader_idx &lt; self.parquet_readers.len() {\n                if let Some(builder) = self.parquet_readers[self.current_reader_idx].take() {\n                    match builder.build() {\n                        Ok(reader) =&gt; {\n                            self.current_reader = Some(Box::new(reader));\n                            self.current_reader_idx += 1;\n                            continue;\n                        }\n                        Err(e) =&gt; {\n                            return Some(Err(arrow::error::ArrowError::ExternalError(Box::new(e))))\n                        }\n                    }\n                }\n            }\n\n            // No more readers available\n            return None;\n        }\n    }\n}\n\nfn main() -&gt; Result&lt;(), Box&lt;dyn Error + Send + Sync&gt;&gt; {\n    let rt = tokio::runtime::Runtime::new()?;\n    rt.block_on(async {\n        // Load tokenizer\n        let tokenizer = load_tokenizer(\"gpt2\")?;\n\n        // Set up Hugging Face API\n        // Download from https://huggingface.co/datasets/Salesforce/wikitext/tree/main/wikitext-103-raw-v1\n        let api = Api::new()?;\n        let repo = api.repo(Repo::with_revision(\n            \"Salesforce/wikitext\".into(),\n            RepoType::Dataset,\n            \"main\".into(),\n        ));\n\n        // Define the parquet files we want to download\n        let train_files = vec![\n            \"wikitext-103-raw-v1/train-00000-of-00002.parquet\",\n            \"wikitext-103-raw-v1/train-00001-of-00002.parquet\",\n        ];\n\n        let mut parquet_readers = Vec::new();\n        for file in &amp;train_files {\n            println!(\"Downloading file: {}\", file);\n            let file_path = repo.get(file)?;\n            let data = std::fs::read(file_path)?;\n\n            // Create a temporary file in the system temp directory and write the downloaded data to it\n            let mut temp_file = NamedTempFile::new()?;\n            temp_file.write_all(&amp;data)?;\n\n            // Create the parquet reader builder with a larger batch size\n            let builder = ParquetRecordBatchReaderBuilder::try_new(temp_file.into_file())?\n                .with_batch_size(8192); // Increase batch size for better performance\n            parquet_readers.push(builder);\n        }\n\n        if parquet_readers.is_empty() {\n            println!(\"No parquet files found to process.\");\n            return Ok(());\n        }\n\n        // Create batch reader\n        let num_samples: u64 = 500_000;\n        let batch_reader = WikiTextBatchReader::new(parquet_readers, tokenizer, Some(num_samples))?;\n\n        // Save as Lance dataset\n        println!(\"Writing to Lance dataset...\");\n        let lance_dataset_path = \"rust_wikitext_lance_dataset.lance\";\n\n        let write_params = WriteParams::default();\n        lance::Dataset::write(batch_reader, lance_dataset_path, Some(write_params)).await?;\n\n        // Verify the dataset\n        let ds = lance::Dataset::open(lance_dataset_path).await?;\n        let scanner = ds.scan();\n        let mut stream = scanner.try_into_stream().await?;\n\n        let mut total_rows = 0;\n        while let Some(batch_result) = stream.next().await {\n            let batch = batch_result?;\n            total_rows += batch.num_rows();\n        }\n\n        println!(\n            \"Lance dataset created successfully with {} rows\",\n            total_rows\n        );\n        println!(\"Dataset location: {}\", lance_dataset_path);\n\n        Ok(())\n    })\n}\n\nfn load_tokenizer(model_name: &amp;str) -&gt; Result&lt;Tokenizer, Box&lt;dyn Error + Send + Sync&gt;&gt; {\n    let api = Api::new()?;\n    let repo = api.repo(Repo::with_revision(\n        model_name.into(),\n        RepoType::Model,\n        \"main\".into(),\n    ));\n\n    let tokenizer_path = repo.get(\"tokenizer.json\")?;\n    let tokenizer = Tokenizer::from_file(tokenizer_path)?;\n\n    Ok(tokenizer)\n}\n</code></pre>"},{"location":"examples/rust/write_read_dataset/","title":"Writing and reading a dataset using Lance","text":"<p>In this example, we will write a simple lance dataset to disk. Then we will read it and print out some basic properties like the schema and sizes for each record batch in the dataset. The example uses only one record batch, however it should work for larger datasets (multiple record batches) as well.</p>"},{"location":"examples/rust/write_read_dataset/#writing-the-raw-dataset","title":"Writing the raw dataset","text":"<pre><code>// Writes sample dataset to the given path\nasync fn write_dataset(data_path: &amp;str) {\n    // Define new schema\n    let schema = Arc::new(Schema::new(vec![\n        Field::new(\"key\", DataType::UInt32, false),\n        Field::new(\"value\", DataType::UInt32, false),\n    ]));\n\n    // Create new record batches\n    let batch = RecordBatch::try_new(\n        schema.clone(),\n        vec![\n            Arc::new(UInt32Array::from(vec![1, 2, 3, 4, 5, 6])),\n            Arc::new(UInt32Array::from(vec![6, 7, 8, 9, 10, 11])),\n        ],\n    )\n    .unwrap();\n\n    let batches = RecordBatchIterator::new([Ok(batch)], schema.clone());\n\n    // Define write parameters (e.g. overwrite dataset)\n    let write_params = WriteParams {\n        mode: WriteMode::Overwrite,\n        ..Default::default()\n    };\n\n    Dataset::write(batches, data_path, Some(write_params))\n        .await\n        .unwrap();\n} // End write dataset\n</code></pre> <p>First we define a schema for our dataset, and create a record batch from that schema. Next we iterate over the record batches (only one in this case) and write them to disk. We also define the write parameters (set to overwrite) and then write the dataset to disk.</p>"},{"location":"examples/rust/write_read_dataset/#reading-a-lance-dataset","title":"Reading a Lance dataset","text":"<p>Now that we have written the dataset to a new directory, we can read it back and print out some basic properties.</p> <pre><code>// Reads dataset from the given path and prints batch size, schema for all record batches. Also extracts and prints a slice from the first batch\nasync fn read_dataset(data_path: &amp;str) {\n    let dataset = Dataset::open(data_path).await.unwrap();\n    let scanner = dataset.scan();\n\n    let mut batch_stream = scanner.try_into_stream().await.unwrap().map(|b| b.unwrap());\n\n    while let Some(batch) = batch_stream.next().await {\n        println!(\"Batch size: {}, {}\", batch.num_rows(), batch.num_columns()); // print size of batch\n        println!(\"Schema: {:?}\", batch.schema()); // print schema of recordbatch\n\n        println!(\"Batch: {:?}\", batch); // print the entire recordbatch (schema and data)\n    }\n} // End read dataset\n</code></pre> <p>First we open the dataset, and create a scanner object. We use it to create a <code>batch_stream</code> that will let us access each record batch in the dataset. Then we iterate over the record batches and print out the size and schema of each one.</p>"},{"location":"examples/rust/write_read_dataset/#complete-example","title":"Complete Example","text":"<pre><code>use arrow::array::UInt32Array;\nuse arrow::datatypes::{DataType, Field, Schema};\nuse arrow::record_batch::{RecordBatch, RecordBatchIterator};\nuse futures::StreamExt;\nuse lance::dataset::{WriteMode, WriteParams};\nuse lance::Dataset;\nuse std::sync::Arc;\n\n#[tokio::main]\nasync fn main() {\n    let data_path: &amp;str = \"./temp_data.lance\";\n\n    write_dataset(data_path).await;\n    read_dataset(data_path).await;\n}\n</code></pre>"},{"location":"format/","title":"Lance Format Specification","text":"<p>Lance is a Lakehouse Format that spans three specification layers: file format, table format, and catalog spec.</p>"},{"location":"format/#understanding-the-lakehouse-stack","title":"Understanding the Lakehouse Stack","text":"<p>To understand where Lance fits in the data ecosystem, let's first map out the complete lakehouse technology stack. The modern lakehouse architecture consists of six distinct layers:</p> <p></p>"},{"location":"format/#1-object-store","title":"1. Object Store","text":"<p>At the foundation lies the object store\u2014storage systems characterized by their object-based simple hierarchy,  typically providing highly durable guarantees with HTTP-based communication protocols for data transfer. This includes systems like S3, GCS, and Azure Blob Storage.</p>"},{"location":"format/#2-file-format","title":"2. File Format","text":"<p>Above the storage layer, the file format describes how a single file should be stored on disk. This is where formats like Apache Parquet operate, defining the internal structure, encoding, and compression of individual data files.</p>"},{"location":"format/#3-table-format","title":"3. Table Format","text":"<p>The table format layer describes how multiple files work together to form a logical table. The key feature that modern table formats enable is transactional commits and read isolation to allow multiple writers and readers to safely operate against the same table. All major open source table formats including Iceberg and Lance implement these features through MVCC (Multi-Version Concurrency Control),  where each commit atomically produces a new table version, and all table versions form a serializable history for the specific table. This also unlocks features like time travel and makes features like schema evolution easy to develop.</p>"},{"location":"format/#4-catalog-spec","title":"4. Catalog Spec","text":"<p>The catalog spec defines how any system can discover and manage a collection of tables within storage. This is where the lower storage and format stack meets the upper service and compute stack.</p> <p>Table formats require at least a way to list all available tables and to describe, add and drop tables in the list. This is necessary for actually building the so-called connectors in compute engines so they can discover and start working on the table according to the format. Historically, Hive has defined the Hive MetaStore spec that is sufficient for most table formats including Delta Lake, Hudi, Paimon, and also Lance. Iceberg offers its unique Iceberg REST Catalog spec.</p> <p>From the top down, projects like Apache Polaris, Unity Catalog, and Apache Gravitino usually offer additional specification for operating against table derivatives  (e.g. views, materialized views, user-defined table functions) and objects used in table operations (e.g. user-defined functions, policies).</p> <p>This intersection between top and bottom stack is also why typically a catalog service would provide both the catalog specifications offered by the format side for easy connectivity to compute engines,  as well as providing their own APIs for extended management features.</p> <p>Another key differentiation of a catalog spec versus a catalog service is that there can be multiple different vendors implementing the same spec. For example, for Polaris REST spec we have open source Apache Polaris server, Snowflake Horizon Catalog, and Polaris-compatible services in AWS Glue, Azure OneLake, etc.</p>"},{"location":"format/#5-catalog-service","title":"5. Catalog Service","text":"<p>A catalog service implements one or more catalog specifications to provide both table metadata and optionally continuous background maintenance (compaction, optimization, index updates) that table formats require to stay performant. Catalog services typically implement multiple specifications to support different table formats. For example, Polaris, Unity and Gravitino all support the Iceberg REST catalog specification for Iceberg tables, and have their own generic table API for other table formats.</p> <p>Since table formats are static specifications, catalog services supply the active operational work needed for production deployments. This is often where open source transitions to commercial offerings, as open source projects typically provide metadata functionality, while commercial solutions offer the full operational experience including automated maintenance. There are also open source solutions like Apache Amoro emerging to fill this gap with complete open source catalog service implementations that offer both table metadata access and continuous optimization.</p>"},{"location":"format/#6-compute-engine","title":"6. Compute Engine","text":"<p>Finally, compute engines are the workhorses that visit catalog services and leverage their knowledge of file formats, table formats, and catalog specifications to perform complex data workflows, including SQL queries, analytics processing, vector search, full-text search, and machine learning training. All sorts of applications can be built on top of compute engines to serve more concrete analytics, ML and AI use cases.</p>"},{"location":"format/#the-overall-lakehouse-architecture","title":"The Overall Lakehouse Architecture","text":"<p>In the lakehouse architecture, compute power resides in the object store, catalog services, and compute engines. The middle three layers (file format, table format, catalog spec) are specifications without compute. This separation enables portability and interoperability.</p>"},{"location":"format/#understanding-lance-as-a-lakehouse-format","title":"Understanding Lance as a Lakehouse Format","text":"<p>Lance spans all three specification layers:</p> <ol> <li>File Format: The Lance columnar file format, read specification \u2192</li> <li>Table Format: The Lance table format, read specification \u2192</li> <li>Catalog Spec: The Lance Namespace specification, read specification \u2192</li> </ol> <p>For comparison:</p> <ul> <li>Apache Iceberg operates at the table format and catalog spec layers, using Apache Parquet, Apache Avro and Apache ORC as the file format</li> <li>Delta Lake and Apache Hudi operate at only the table format layer, using Apache Parquet as the file format</li> </ul>"},{"location":"format/file/","title":"Lance File Format","text":""},{"location":"format/file/#file-structure","title":"File Structure","text":"<p>A Lance file is a container for tabular data. The data is stored in \"disk pages\". Each disk page contains some rows for a single column. There may be one or more disk pages per column. Different columns may have different numbers of disk pages. Metadata at the end of the file describes where the pages are located and how the data is encoded.</p> <p></p> <p>Note</p> <p>This page describes the container specification. We also have a set of default encodings that are used to encode data into disk pages. See the Encoding Strategy page for more details.</p>"},{"location":"format/file/#disk-pages","title":"Disk Pages","text":"<p>Disk pages are designed to be large enough to justify a dedicated I/O operation, even on cloud storage, typically several megabytes. Using a larger page size may reduce the number of I/O operations required to read a file, but it also increases the amount of memory required to write the file. In practice, very large page sizes are not useful when high speed reads are required because large contiguous reads need to be broken into smaller reads for performance (particularly on cloud storage). As a result, a default of 8MB is recommended for the page size and should yield ideal performance on all storage systems.</p> <p>Disk pages should not generally be opaque. It is possible to read a portion of a disk page when a subset of the rows are required. However, the specifics of this process depend on the column encoding which is described in a later section.</p>"},{"location":"format/file/#no-row-groups","title":"No Row Groups","text":"<p>Unlike similar formats, there is no \"row group\" concept, only pages. We believe the concept of row groups to be fundamentally harmful to performance. If the row group size is too small then columns will be split into \"runt pages\" which yield poor read performance on cloud storage. If the row group size is too large then a file writer will need a large amount of RAM since an entire row group must be buffered in memory before it can be written. Instead, to split a file amongst multiple readers we rely on the fact that partial page reads are possible and have minimal read amplification. As a result, you can split the file at whatever row boundary you want.</p>"},{"location":"format/file/#buffer-alignment","title":"Buffer Alignment","text":"<p>The file format does not require that buffers be contiguous as buffers are referenced by absolute offsets. In practice, we always align buffers to 64 byte boundaries.</p>"},{"location":"format/file/#external-buffers","title":"External Buffers","text":"<p>Every page in the file is referenced by an absolute offset. This means that non-page data may be inserted amongst the pages. This can be useful for storing extremely large data types which might only fit a few rows per page otherwise. We can instead store the data out-of-line and store the locations in a page.</p> <p>In addition, the file format supports \"global buffers\" which can be used for auxiliary data. This may be used to store a file schema, file indexes, column statistics, or other metadata. References to the global buffers are stored in a special spot in the footer.</p>"},{"location":"format/file/#column-descriptors","title":"Column Descriptors","text":"<p>At the tail of the file is metadata that describes each page in the file, particularly the encoding strategy used. This metadata consists of a series of \"column descriptors\", which are standalone protobuf messages for each column in the file. Since each column has its own message there is no need to read all file metadata if you are only interested in a subset of the columns. However, in many cases, the column descriptors are small enough that it is cheaper to read the entire footer in a single read than split it into multiple reads.</p>"},{"location":"format/file/#offsets-footer","title":"Offsets &amp; Footer","text":"<p>After the column descriptors there are offset arrays for the column descriptors and global buffers. These simply point to the locations of each item. Finally, there is a fixed-size footer which describes the position of the offset arrays and start of the metadata section.</p>"},{"location":"format/file/#identifiers-and-type-systems","title":"Identifiers and Type Systems","text":"<p>This basic container format has no concept of types. These are added later by the encoding layer. All columns are referenced by an integer \"column index\". All global buffers are referenced by an integer \"global buffer index\". The schema is typically stored in the global buffers, but the file format is unaware of this.</p>"},{"location":"format/file/#reading-strategy","title":"Reading Strategy","text":"<p>The file metadata will need to be known before reading the data. A simple approach for loading the footer is to read one sector from the end (sector depends on the filesystem, 4KiB for local disk, larger for cloud storage). Then parse the footer and read the rest of the metadata (at this point the size will be known). This requires 1-2 IOPS. By storing the metadata size in some other location (e.g. table manifest) it is possible to always read the footer in a single IOP. If there are many columns in the file and only some are desired then it may be better to read individual columns instead of reading all column metadata, increasing the number of IOPS but decreasing the amount of data read.</p> <p>Next, to read the data, scan through the pages for each column to determine which pages are needed. Each page stores the row offset of the first row in the page. This makes it easy to quickly determine the required pages. The encoding information for the page can then be used to determine exactly which byte ranges are needed from the page.</p> <p>Disk pages should be large enough that there should no significant benefit to sequentially reading the file. However, if such a use case is desired then the file can be read sequentially once the metadata is known, assuming you want to read all columns in the file.</p>"},{"location":"format/file/#detailed-overview","title":"Detailed Overview","text":"<p>A detailed description of the file layout follows:</p> <pre><code>// Note: the number of buffers (BN) is independent of the number of columns (CN)\n//       and pages.\n//\n//       Buffers often need to be aligned.  64-byte alignment is common when\n//       working with SIMD operations.  4096-byte alignment is common when\n//       working with direct I/O.  In order to ensure these buffers are aligned\n//       writers may need to insert padding before the buffers.\n//\n//       If direct I/O is required then most (but not all) fields described\n//       below must be sector aligned.  We have marked these fields with an\n//       asterisk for clarity.  Readers should assume there will be optional\n//       padding inserted before these fields.\n//\n//       All footer fields are unsigned integers written with little endian\n//       byte order.\n//\n// \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n// | Data Pages                       |\n// |   Data Buffer 0*                 |\n// |   ...                            |\n// |   Data Buffer BN*                |\n// \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n// | Column Metadatas                 |\n// | |A| Column 0 Metadata*           |\n// |     Column 1 Metadata*           |\n// |     ...                          |\n// |     Column CN Metadata*          |\n// \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n// | Column Metadata Offset Table     |\n// | |B| Column 0 Metadata Position*  |\n// |     Column 0 Metadata Size       |\n// |     ...                          |\n// |     Column CN Metadata Position  |\n// |     Column CN Metadata Size      |\n// \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n// | Global Buffers Offset Table      |\n// | |C| Global Buffer 0 Position*    |\n// |     Global Buffer 0 Size         |\n// |     ...                          |\n// |     Global Buffer GN Position    |\n// |     Global Buffer GN Size        |\n// \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n// | Footer                           |\n// | A u64: Offset to column meta 0   |\n// | B u64: Offset to CMO table       |\n// | C u64: Offset to GBO table       |\n// |   u32: Number of global bufs     |\n// |   u32: Number of columns         |\n// |   u16: Major version             |\n// |   u16: Minor version             |\n// |   \"LANC\"                         |\n// \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n//\n// File Layout-End\n</code></pre>"},{"location":"format/file/#column-metadata","title":"Column Metadata","text":"<p>The protobuf messages for the column metadata are as follows:</p> <pre><code>message ColumnMetadata {\n\n  // This describes a page of column data.\n  message Page {\n    // The file offsets for each of the page buffers\n    //\n    // The number of buffers is variable and depends on the encoding.  There\n    // may be zero buffers (e.g. constant encoded data) in which case this\n    // could be empty.\n    repeated uint64 buffer_offsets = 1;\n    // The size (in bytes) of each of the page buffers\n    //\n    // This field will have the same length as `buffer_offsets` and\n    // may be empty.\n    repeated uint64 buffer_sizes = 2;\n    // Logical length (e.g. # rows) of the page\n    uint64 length = 3;\n    // The encoding used to encode the page\n    Encoding encoding = 4;\n    // The priority of the page\n    //\n    // For tabular data this will be the top-level row number of the first row\n    // in the page (and top-level rows should not split across pages).\n    uint64 priority = 5;\n  }\n  // Encoding information about the column itself.  This typically describes\n  // how to interpret the column metadata buffers.  For example, it could\n  // describe how statistics or dictionaries are stored in the column metadata.\n  Encoding encoding = 1;\n  // The pages in the column\n  repeated Page pages = 2;   \n  // The file offsets of each of the column metadata buffers\n  //\n  // There may be zero buffers.\n  repeated uint64 buffer_offsets = 3;\n  // The size (in bytes) of each of the column metadata buffers\n  //\n  // This field will have the same length as `buffer_offsets` and\n  // may be empty.\n  repeated uint64 buffer_sizes = 4;\n\n}\n</code></pre>"},{"location":"format/file/encoding/","title":"Lance Encoding Strategy","text":"<p>The encoding strategy determines how array data is encoded into a disk page. The encoding strategy tends to evolve more quickly than the file format itself.</p>"},{"location":"format/file/encoding/#older-encoding-strategies","title":"Older Encoding Strategies","text":"<p>The 0.1 and 2.0 encoding strategies are no longer documented. They were significantly different from future encoding strategies and describing them in detail would be a distraction.</p>"},{"location":"format/file/encoding/#terminology","title":"Terminology","text":"<p>An array is a sequence of values. An array has a data type which describes the semantic interpretation of the values. A layout is a way to encode an array into a set of buffers and child arrays. A buffer is a contiguous sequence of bytes. An encoding describes how the semantic interpretation of data is mapped to the layout. An encoder converts data from one layout to another.</p> <p>Data types and layouts are orthogonal concepts. An integer array might be encoded into two completely different layouts which represent the same data.</p> <p></p>"},{"location":"format/file/encoding/#data-types","title":"Data Types","text":"<p>Lance uses a subset of Arrow's type system for data types. An Arrow data type is both a data type and an encoding. When writing data Lance will often normalize Arrow data types. For example, a string array and a large string array might end up traveling down the same path (variable width data). In fact, most types fall into two general paths. One for fixed-width data and one for variable-width data (where we recognize both 32-bit and 64-bit offsets).</p> <p>At read time, the Arrow data type is used to determine the target encoding. For example, a string array and large string array might both be stored in the same layout but, at read time, we will use the Arrow data type to determine the size of the offsets returned to the user. There is no requirement the output Arrow type matches the input Arrow type. For example, it is acceptable to write an array as \"large string\" and then read it back as \"string\".</p>"},{"location":"format/file/encoding/#search-cache","title":"Search Cache","text":"<p>The search cache is a key component of the Lance file reader. Random access requires that we locate the physical location of the data in the file. To do so we need to know information such as the encoding used for a column, the location of the page, and potentially other information. This information is collectively known as the \"search cache\" and is implemented as a basic LRU cache. We define a \"initialization phase\" which is when we load the various indexing information into the search cache. The cost of initialization is assumed to be amortized over the lifetime of the reader.</p> <p>When performing full scans (i.e. not random access), we should be able to ignore the search cache and sometimes can avoid loading it entirely. We do want to optimize for cold scans as the initialization phase is often not amortized over the lifetime of the reader.</p>"},{"location":"format/file/encoding/#structural-encoding","title":"Structural Encoding","text":"<p>The first step in encoding an array is to determine the structural encoding of the array. A structural encoding breaks the data into smaller units which can be independently decoded. Structural encodings are also responsible for encoding the \"structure\" (struct validity, list validity, list offsets, etc.) typically utilizing repetition levels and definition levels.</p> <p>Structural encoding is fairly complicated! However, the goal is to suck out all the details related to I/O scheduling so that compression libraries can focus on compression. This keeps our compression traits simple without sacrificing our ability to perform random access.</p> <p>There are only a few structural encodings. The structural encoding is described by the <code>PageLayout</code> message and is the top-level message for the encoding.</p> <pre><code>message PageLayout {\n  oneof layout {\n    // A layout used for pages where the data is small\n    MiniBlockLayout mini_block_layout = 1;\n    // A layout used for pages where all values are null\n    AllNullLayout all_null_layout = 2;\n    // A layout used for pages where the data is large\n    FullZipLayout full_zip_layout = 3;\n    // A layout where large binary data is encoded externally\n    // and only the descriptions are put in the page\n    BlobLayout blob_layout = 4;\n  }\n\n}\n</code></pre>"},{"location":"format/file/encoding/#repetition-and-definition-levels","title":"Repetition and Definition Levels","text":"<p>Repetition and definition levels are an alternative to validity bitmaps and offset arrays for expressing struct and list information. They have a significant advantage in that they combine all of these buffers into a single buffer which allows us to avoid multiple IOPS.</p> <p>A more extensive explanation of repetition and definition levels can be found in the code. One particular note is that we use 0 to represent the \"inner-most\" item and Parquet uses 0 to represent the \"outer-most\" item. Here is an example:</p>"},{"location":"format/file/encoding/#definition-levels","title":"Definition Levels","text":"<p>Consider the following array:</p> <pre><code>[{\"middle\": {\"inner\": 1]}}, NULL, {\"middle\": NULL}, {\"middle\": {\"inner\": NULL}}]\n</code></pre> <p>In Arrow we would have the following validity arrays:</p> <pre><code>Outer validity : 1, 0, 1, 1\nMiddle validity: 1, ?, 0, 1\nInner validity : 1, ?, ?, 0\nValues         : 1, ?, ?, ?\n</code></pre> <p>The ? values are undefined in the Arrow format. We can convert these into definition levels as follows:</p> Values Definition Notes 1 0 Valid at all levels ? 3 Null at outer level ? 2 Null at middle level ? 1 Null at inner level"},{"location":"format/file/encoding/#repetition-levels","title":"Repetition Levels","text":"<p>Consider the following list array with 3 rows</p> <pre><code>[{&lt;0,1&gt;, &lt;&gt;, &lt;2&gt;}, {&lt;3&gt;}, {}], [], [{&lt;4&gt;}]\n</code></pre> <p>We would have three offsets arrays in Arrow:</p> <pre><code>Outer-most ([]): [0, 3, 3, 4]\nMiddle     ({}): [0, 3, 4, 4, 5]\nInner      (&lt;&gt;): [0, 2, 2, 3, 4, 5]\nValues         : [0, 1, 2, 3, 4]\n</code></pre> <p>We can convert these into repetition levels as follows:</p> Values Repetition Notes 0 3 Start of outer-most list 1 0 Continues inner-most list (no new lists) ? 1 Start of new inner-most list (empty list) 2 1 Start of new inner-most list 3 2 Start of new middle list ? 2 Start of new inner-most list (empty list) ? 3 Start of new outer-most list (empty list) 4 0 Start of new outer-most list"},{"location":"format/file/encoding/#mini-block-page-layout","title":"Mini Block Page Layout","text":"<p>The mini block page layout is the default layout for smallish types. This fits most of the classical data types (integers, floats, booleans, small strings, etc.) that Parquet and related formats already handle well. As is no surprise, the approach used is pretty similar to those formats.</p> <p></p> <p>The data is divided into small mini-blocks. Each mini-block should contain a power-of-two number of values (except for the last mini-block) and should be less than 32KiB of compressed data. We have to read an entire mini-block to get a single value so we want to keep the mini-block size small. Mini blocks are padded to 8 byte boundaries. This helps to avoid alignment issues. Each mini-block starts with a small header which helps us figure out how much padding has been applied.</p> <p>The repetition and definition levels are sliced up and stored in the mini-blocks along with the compressed buffers. Since we need to read an entire mini-block there is no need to zip up the various buffers and they are stored one after the other (repetition, definition, values, ...).</p>"},{"location":"format/file/encoding/#buffer-1-mini-blocks","title":"Buffer 1 (Mini Blocks)","text":"Bytes Meaning 1 Number of buffers in the mini-block 2 Size of buffer 0 2 Size of buffer 1 ... ... 2 Size of buffer N 0-7 Padding to ensure 8 byte alignment * Buffer 0 0-7 Padding to ensure 8 byte alignment * Buffer 1 ... ... 0-7 Padding to ensure 8 byte alignment * Buffer N 0-7 Padding to ensure 8 byte alignment <p>Note: It is natural to explain this buffer first but it is actually the second buffer in the page.</p>"},{"location":"format/file/encoding/#buffer-0-mini-block-metadata","title":"Buffer 0 (Mini Block Metadata)","text":"<p>To enable random access we have a small metadata lookup which contains two bytes per mini-block. This lookup tells us how many bytes are in each mini block and how many items are in the mini block. This metadata lookup must be loaded at initialization time and placed in the search cache.</p> Bits (not bytes) Meaning 12 Number of 8-byte words in block 0 4 Log2 of number of values in block 0 12 Number of 8-byte words in block 1 4 Log2 of number of values in block 1 ... ... 12 Number of 8-byte words in block N 4 Log2 of number of values in block N <p>The last 4 bits are special and we just store 0 today. This is because the protobuf contains the number of values (not required to be a power of 2) in the entire disk page. We can subtract the values in the other blocks to get the number of values in the last block.</p>"},{"location":"format/file/encoding/#buffer-2-dictionary-optional","title":"Buffer 2 (Dictionary, optional)","text":"<p>Dictionary encoding is an encoding that can be applied at many different levels throughout a file. For example, it could be used as a compressive encoding or it could even be entirely external to the file. We've found the most convenient simple place to apply dictionary encoding is at the structural level. Since dictionary indices are small we always use the mini block layout for dictionary encoding. When we use dictionary encoding we store the dictionary in the buffer at index 2. We require the dictionary to be full loaded and decoded at initialization time. This means we don't have to load the dictionary during random access but it does require the dictionary be placed in the search cache.</p> <p>Dictionary encoding is one of the few spots today where we have no rules on how it is encoded and compressed. We treat the entire dictionary as a single opaque buffer. As a result we rely on the block compression trait to handle dictionary compression.</p>"},{"location":"format/file/encoding/#buffer-2-or-3-repetition-index-optional","title":"Buffer 2 (or 3) (Repetition Index, optional)","text":"<p>If there is repetition (list levels) then we need some way to translate row offsets into item offsets. The mini blocks always store items. During a full scan the list offsets are restored when we decode the repetition levels. However, to support random access, we don't have the repetition levels available. Instead we store a repetition index in the next available buffer (index 2 or 3 depending on whether the dictionary is present).</p> <p>The repetition index is a flat buffer of u64 values. We have N * D values where N is the number of mini blocks and D Is the desired depth of random access plus one. For example, to support 1-dimensional lookups (random access by rows) then D is 2. To support two-dimensional lookups (e.g. rows[50][17]) then we could set D to 3.</p> <p>Currently we only support 1-dimensional random access. Currently we do not compress the repetition index.</p> <p>This may change in future versions.</p> Bytes Meaning 8 Number of rows in block 0 8 Number of partial items in block 0 8 Number of rows in block 1 8 Number of partial items in block 1 ... ... 8 Number of rows in block N 8 Number of partial items in block N <p>The last 8 bytes of each block stores the number of \"partial\" items. These are items leftover after the last complete row. We don't require rows to be bounded by mini-blocks so we need to keep track of this. For example, if we have 10,000 items per row then we might have several mini-blocks with only partial items and 0 rows.</p> <p>At read time we can use this repetition index to translate row offsets into item offsets.</p>"},{"location":"format/file/encoding/#mini-block-compression","title":"Mini Block Compression","text":"<p>The mini block layout relies on the compression algorithm to handle the splitting of data into mini-blocks. This is because the number of values per block will depend on the compressibility of the data. As a result, there is a special trait for mini block compression.</p> <p>The data compression algorithm is the algorithm that decides chunk boundaries. The repetition and definition levels are then sliced appropriately and sent to a block compressor. This means there are no constraints on how the repetition and definition levels are compressed.</p> <p>Beyond splitting the data into mini-blocks, there are no additional constraints. We expect to fully decode mini blocks as opaque chunks. This means we can use any compression algorithm that we deem suitable.</p>"},{"location":"format/file/encoding/#protobuf","title":"Protobuf","text":"<pre><code>message MiniBlockLayout {\n  // Description of the compression of repetition levels (e.g. how many bits per rep)\n  //\n  // Optional, if there is no repetition then this field is not present\n  CompressiveEncoding rep_compression = 1;\n  // Description of the compression of definition levels (e.g. how many bits per def)\n  //\n  // Optional, if there is no definition then this field is not present\n  CompressiveEncoding def_compression = 2;\n  // Description of the compression of values\n  CompressiveEncoding value_compression = 3;\n  // Description of the compression of the dictionary data\n  //\n  // Optional, if there is no dictionary then this field is not present\n  CompressiveEncoding dictionary = 4;\n  // Number of items in the dictionary\n  uint64 num_dictionary_items = 5;\n  // The meaning of each repdef layer, used to interpret repdef buffers correctly\n  repeated RepDefLayer layers = 6;\n  // The number of buffers in each mini-block, this is determined by the compression and does\n  // NOT include the repetition or definition buffers (the presence of these buffers can be determined\n  // by looking at the rep_compression and def_compression fields)\n  uint64 num_buffers = 7;\n  // The depth of the repetition index.\n  //\n  // If there is repetition then the depth must be at least 1.  If there are many layers\n  // of repetition then deeper repetition indices will support deeper nested random access.  For\n  // example, given 5 layers of repetition then the repetition index depth must be at least\n  // 3 to support access like `rows[50][17][3]`.\n  //\n  // We require `repetition_index_depth + 1` u64 values per mini-block to store the repetition\n  // index if the `repetition_index_depth` is greater than 0.  The +1 is because we need to store\n  // the number of \"leftover items\" at the end of the chunk.  Otherwise, we wouldn't have any way\n  // to know if the final item in a chunk is valid or not.\n  uint32 repetition_index_depth = 8;\n  // The page already records how many rows are in the page.  For mini-block we also need to know how\n  // many \"items\" are in the page.  A row and an item are the same thing unless the page has lists.\n  uint64 num_items = 9;\n\n  // Since Lance 2.2, miniblocks have larger chunk sizes (&gt;= 64KB)\n  bool has_large_chunk = 10;\n\n}\n</code></pre> <p>The protobuf for the mini block layout describes the compression of the various buffers. It also tells us some information about the dictionary (if present) and the repetition index (if present).</p>"},{"location":"format/file/encoding/#full-zip-page-layout","title":"Full Zip Page Layout","text":"<p>The full zip page layout is a layout for larger values (e.g. vector embeddings) which are large but not so large that we can justify a single IOP per value. In this case we are trying to avoid storing a large amount of \"chunk overhead\" (both in terms of buffer space and the RAM space in the search cache that we would need to store the repetition index). As a tradeoff, we are introducing a second IOP per-range for random access reads (unless the data is fixed-width such as vector embeddings).</p> <p>We currently use 256 bytes as the cutoff for the full zip layout. At this point we would only be fitting 16 values in a 4KiB disk sector and so creating a mini-block descriptor for every 16 values would be too much overhead.</p> <p>As a further consequence, we must ensure that the compression algorithm is \"transparent\" so that we can index individual values after compression has been applied. This prevents us from using compression algorithms such as delta encoding. If we want to apply general compression we have to apply them on a per-value basis. The way we enforce this is by requiring the compression to return either a flat fixed-width or variable-width layout so that we know the location of each element.</p> <p>The repetition and definition levels, along with all compressed buffers, are all zipped together into a single buffer.</p>"},{"location":"format/file/encoding/#data-buffer-buffer-0","title":"Data Buffer (Buffer 0)","text":"<p>The data buffer is a single buffer that contains the repetition, definition, and value data, all zipped into a single buffer. The repetition and definition information are combined and byte packed. This is referred to as a control word. If the value is null or an empty list, then the control word is all that is serialized. If there is no validity or repetition information then control words are not serialized. If the value is variable-width then we encode the size of the value. This is either a 4-byte or 8-byte integer depending on the width used in the offsets returned by the compression (in future versions this will likely be encoded with some kind of variable-width integer encoding). Finally the value buffers themselves are appended.</p> Bytes Meaning 0-4 Control word 0 0/4/8 Value 0 size * Value 0 data ... ... 0-4 Control word N 0/4/8 Value N size * Value N data <p>Note: a fixed-width data type that has no validity information (e.g. non-nullable vector embeddings) is simply a flat buffer of data.</p>"},{"location":"format/file/encoding/#repetition-index-buffer-1","title":"Repetition Index (Buffer 1)","text":"<p>If there is repetition information or the values are variable width then we need additional help to locate values in the disk page. The repetition index is an array of u64 values. There is one value per row and the value is an offset to the start of that row in the data buffer. To perform random access we require two IOPS. First we issue an IOP into the repetition index to determine the location and then a second IOP into the data buffer to load the data. Alternatively, the entire repetition index can be loaded into memory in the initialization phase though this can lead to high RAM usage by the search cache.</p> <p>The repetition index must have a fixed width (or else we would need a repetition index to read the repetition index!) and be transparent. As a result the compression options are limited. That being said, there is little value (in terms of performance) in compressing the repetition index. It is never read in its entirety as it is not needed for full scans. Currently the repetition index is always compressed with simple (non-chunked) byte packing into 1,2,4, or 8 byte values.</p>"},{"location":"format/file/encoding/#protobuf_1","title":"Protobuf","text":"<pre><code>message FullZipLayout {\n  // The number of bits of repetition info (0 if there is no repetition)\n  uint32 bits_rep = 1;\n  // The number of bits of definition info (0 if there is no definition)\n  uint32 bits_def = 2;\n  // The number of bits of value info\n  //\n  // Note: we use bits here (and not bytes) for consistency with other encodings.  However, in practice,\n  // there is never a reason to use a bits per value that is not a multiple of 8.  The complexity is not\n  // worth the small savings in space since this encoding is typically used with large values already.\n  oneof details {\n    // If this is a fixed width block then we need to have a fixed number of bits per value\n    uint32 bits_per_value = 3;\n    // If this is a variable width block then we need to have a fixed number of bits per offset\n    uint32 bits_per_offset = 4;\n  }\n  // The number of items in the page\n  uint32 num_items = 5;\n  // The number of visible items in the page\n  uint32 num_visible_items = 6;\n  // Description of the compression of values\n  CompressiveEncoding value_compression = 7;\n  // The meaning of each repdef layer, used to interpret repdef buffers correctly\n  repeated RepDefLayer layers = 8;\n\n}\n</code></pre> <p>The protobuf for the full zip layout describes the compression of the data buffer. It also tells us the size of the control words and how many bits we have per value (for fixed-width data) or how many bits we have per offset (for variable-width data).</p>"},{"location":"format/file/encoding/#all-null-page-layout","title":"All Null Page Layout","text":"<p>This layout is used when all the values are null. Surprisingly, this does not mean there is no data. If there are any levels of struct or list then we need to store the rep/def levels so that we can distinguish between null structs, null lists, empty lists, and null values.</p>"},{"location":"format/file/encoding/#repetition-and-definition-levels-buffers-0-and-1","title":"Repetition and Definition Levels (Buffers 0 and 1)","text":"<p>Note: We currently store rep levels in the first buffer with a flat layout of 16-bit values and def levels in the second buffer with a flat layout of 16-bit values. This will likely change in future versions.</p>"},{"location":"format/file/encoding/#protobuf_2","title":"Protobuf","text":"<pre><code>message AllNullLayout {\n  // The meaning of each repdef layer, used to interpret repdef buffers correctly\n  repeated RepDefLayer layers = 5;\n\n}\n</code></pre> <p>All we need to know is the meaning of each rep/def level.</p>"},{"location":"format/file/encoding/#blob-page-layout","title":"Blob Page Layout","text":"<p>The blob page layout is a layout for large binary values where we would only have a few values per disk page. The actual data is stored out-of-line in external buffers. The disk page stores a \"description\" which is a struct array of two fields: <code>position</code> and <code>size</code>. The <code>position</code> is the absolute file offset of the blob and the <code>size</code> is the size (in bytes) of the blob. The inner page layout describes how the descriptions are encoded.</p> <p>The validity information (definition levels) is smuggled into the descriptions. If the size and position are both zero then the value is empty. Otherwise, if the size is zero and the position is non-zero then the value is null and the position is the definition level.</p> <p>This layout is only recommended when you can justify a single IOP per value. For example, when values are 1MiB or larger.</p> <p>This layout has no buffers of its own and merely wraps an inner layout.</p>"},{"location":"format/file/encoding/#protobuf_3","title":"Protobuf","text":"<pre><code>message BlobLayout {\n  // The inner layout used to store the descriptions\n  PageLayout inner_layout = 1;\n  // The meaning of each repdef layer, used to interpret repdef buffers correctly\n  //\n  // The inner layout's repdef layers will always be 1 all valid item layer\n  repeated RepDefLayer layers = 2;\n\n}\n</code></pre> <p>Since we smuggle the validity into the descriptions we don't need to store it in the inner layout and so the rep/def meaning is stored in the blob layout and the rep/def meaning in the inner layout will be 1 all valid item layer.</p>"},{"location":"format/file/encoding/#semi-structural-transformations","title":"Semi-Structural Transformations","text":"<p>There are some data transformations that are applied to the data before (or during) the structural encoding process. These are described here.</p>"},{"location":"format/file/encoding/#dictionary-encoding","title":"Dictionary Encoding","text":"<p>Dictionary encoding is a technique that can be applied to any kind of array. It is useful when there are not very many unique values in the array. First, a \"dictionary\" of unique values is created. Then we create a second array of indices into the dictionary.</p> <p>Dictionary encoding is also known as \"categorical encoding\" in other contexts.</p> <p>Dictionary encoding could be treated as simply another compression technique but, when applied, it would be an opaque compression technique which would limit its usability (e.g. in a full zip context). As a result, we apply it before any structural encoding takes place. This allows us to place the dictionary in the search cache for random access.</p>"},{"location":"format/file/encoding/#struct-packing","title":"Struct Packing","text":"<p>Struct packing is an alternative representation to apply to struct values. Instead of storing that struct in a columnar fashion it will be stored in a row-major fashion. This will reduce the number of IOPS needed for random access but will prevent the ability to read a single field at a time. This is useful when all fields in the struct are always accessed together.</p> <p>Packed struct is always opt-in (see section on configuration below).</p> <p>Currently packed struct is limited to fixed-width data.</p>"},{"location":"format/file/encoding/#fixed-size-list","title":"Fixed Size List","text":"<p>Fixed size lists are an Arrow data type that needs specialized handling at the structural level. If the underlying data type is primitive then the fixed size list will be primitive (e.g. a tensor). If the underlying data type is structural (struct/list) then the fixed size list is structural and should be treated the same as a variable-size list.</p> <p>We don't want compression libraries to need to worry about the intricacies of fixed-size lists. As a result we flatten the list as part of structural encoding. This complicates random access as we must translate between rows (an entire fixed size list) and items (a single item in the list).</p> <p>If the items in a fixed size list are nullable then we do not treat that validity array as a repetition or definition level. Instead, we store the validity as a separate buffer. For example, when encoding nullable fixed size lists with mini-block encoding the validity buffer is another buffer in the mini-block. When encoding nullable fixed size lists with full-zip encoding the validity buffer is zipped together with the values.</p> <p>The good news is that fixed size lists are entirely a structural encoding concern. Compression techniques are free to pretend that the fixed-size list data type does not exist.</p>"},{"location":"format/file/encoding/#compression","title":"Compression","text":"<p>Once a structural encoding is chosen we must determine how to compress the data. There are various buffers that might be compressed (e.g. data, repetition, definition, dictionary, etc.). The available compression algorithms are also constrained by the structural encoding chosen. For example, when using the full zip layout we require transparent compression. As a result, each encoding technique may or may not be usable in a given scenario. In addition, the same technique may be applied in a different way depending on the encoding chosen.</p> <p>In implementation terms we have a trait for each compression constraint. The techniques then implement the traits that they can be applied to. To start with, here is a summary of compression techniques which are implemented in at least one scenario and a list of which traits the technique implements. A \u2753 is used to indicate that the technique should be usable in that context but we do not yet do so while a \u274c indicates that the technique is not usable because it is not transparent. Note, even though a technique is not transparent it can still be applied on a per-value basis. We use \u2611\ufe0f to mark a technique that is applied on a per-value basis:</p> Compression Used in Block Context Used in Full Zip Context Used in Mini-Block Context Flat \u2705 (2.1) \u2705 (2.1) \u2705 (2.1) Variable \u2705 (2.1) \u2705 (2.1) \u2705 (2.1) Constant \u2705 (2.1) \u2753 \u2753 Bitpacking \u2705 (2.1) \u2753 \u2705 (2.1) Fsst \u2753 \u2705 (2.1) \u2705 (2.1) Rle \u2753 \u274c \u2705 (2.1) ByteStreamSplit \u2753 \u274c \u2705 (2.1) General \u2753 \u2611\ufe0f (2.1) \u2705 (2.1) <p>In the following sections we will describe each technique in a bit more detail and explain how it is utilized in various contexts.</p>"},{"location":"format/file/encoding/#flat","title":"Flat","text":"<p>Flat compression is the uncompressed representation of fixed-width data. There is a single buffer of data with a fixed number of bits per value.</p> <p>When applied in a mini-block context we find the largest power of 2 number of values that will be less than 8,186 bytes and use that as the block size.</p>"},{"location":"format/file/encoding/#variable","title":"Variable","text":"<p>Variable compression is the uncompressed representation of variable-width data. There is a buffer of values and a buffer of offsets.</p> <p>When applied in a mini-block context each block may have a different number of values. We walk through the values until we find the point that would exceed 4,096 bytes and then use the most recent power of 2 number of values that we have passed.</p>"},{"location":"format/file/encoding/#constant","title":"Constant","text":"<p>Constant compression is currently only utilized in a few specialized scenarios such as all-null arrays.</p> <p>This will likely change in future versions.</p>"},{"location":"format/file/encoding/#bitpacking","title":"Bitpacking","text":"<p>Bitpacking is a compression technique that removes the unused bits from a set of values. For example, if we have a u32 array and the maximum value is 5000 then we only need 13 bits to store each value.</p> <p>When used in a mini-block context we always use 1024 values per block. In addition, we store the compressed bit width inline in the block itself.</p> <p>Bitpacking is, in theory, usable in a full zip context. However, values in this context are so large that shaving off a few bits is unlikely to have any meaningful impact. Also, the full-zip context keeps things byte-aligned and so we would have to remove at least 8 bits per value.</p>"},{"location":"format/file/encoding/#fsst","title":"Fsst","text":"<p>Fsst is a fast and transparent compression algorithm for variable-width data. It is the primary compression algorithm that we apply to variable-width data.</p> <p>Currently we use a single FSST symbol table per disk page and store that symbol table in the protobuf description. This is for historical reasons and is not ideal and will likely change in future versions.</p> <p>When FSST is applied in a mini-block context we simply compress the data and let the underlying compressor (always <code>Variable</code> at the moment) handle the chunking.</p>"},{"location":"format/file/encoding/#run-length-encoding-rle","title":"Run Length Encoding (RLE)","text":"<p>Run length encoding is a compression technique that compresses large runs of identical values into an array of values and an array of run lengths. This is currently used in the mini-block context. To determine if we should apply run-length encoding we look at the number of runs divided by the number of values. If the ratio is below a threshold (by default 0.5) then we apply run-length encoding.</p>"},{"location":"format/file/encoding/#byte-stream-split-bss","title":"Byte Stream Split (BSS)","text":"<p>Byte stream split is a compression technique that splits multi-byte values by byte position, creating separate streams for each byte position across all values. This is a rudimentary and simple form of translating floating point values into a more compressible format because it tends to cluster the mantissa bits together which are often consistent across a column of floating point values. It does not actually make the data smaller by itself. As a result, BSS is only applied if general compression is also applied on the column.</p> <p>We currently determine whether or not to apply BSS by looking at an entropy statistics. There is a configurable sensitivity parameter. A sensitivity of 0.0 means never apply BSS and a sensitivity of 1.0 means always apply BSS.</p>"},{"location":"format/file/encoding/#general","title":"General","text":"<p>General compression is a catch-all term for classical opaque compression techniques such as LZ4, ZStandard, Snappy, etc. These techniques are typically back-referencing compressors which replace values with a \"back reference\" to a spot where we already saw the value.</p> <p>When applied in a mini-block context we run general compression after all other compression and compress the entire mini-block.</p> <p>When applied in a full zip context we run general compression on each value.</p> <p>The only time general compression is automatically applied is in a full-zip context when we have values that are at least 32KiB large. This is because general compression can be CPU intensive.</p> <p>However, general compression is highly effective and we allow it to be opted into in other contexts via configuration.</p>"},{"location":"format/file/encoding/#compression-configuration","title":"Compression Configuration","text":"<p>The following section lists the available configuration options. These can be set programmatically through writer options. However, they can also be set in the field metadata in the schema.</p> Key Values Default Description <code>lance-encoding:compression</code> <code>lz4</code>, <code>zstd</code>, <code>none</code>, ... <code>none</code> Opt-in to general compression. The value indicates the scheme. <code>lance-encoding:compression-level</code> Integers (range is scheme dependent) Varies by scheme Higher indicates more work should be done to compress the data. <code>lance-encoding:rle-threshold</code> <code>0.0-1.0</code> <code>0.5</code> See below <code>lance-encoding:bss</code> <code>off</code>, <code>on</code>, <code>auto</code> <code>auto</code> See below <code>lance-encoding:dict-divisor</code> Integers greater than 1 <code>2</code> See below <code>lance-encoding:general</code> <code>off</code>, <code>on</code> <code>off</code> Whether to apply general compression. <code>lance-encoding:packed</code> Any string Not set Whether to apply packed struct encoding (see above). <code>lance-encoding:structural-encoding</code> <code>miniblock</code>, <code>fullzip</code> Not set Force a particular structural encoding to be applied (only useful for testing purposes)"},{"location":"format/file/encoding/#configuration-details","title":"Configuration Details","text":""},{"location":"format/file/encoding/#compression-scheme","title":"Compression Scheme","text":"<p>The <code>lance-encoding:compression</code> setting enables general-purpose compression algorithms to be applied. Available schemes:</p> <ul> <li><code>lz4</code>: Fast compression with good compression ratios. Default compression level is fast mode.</li> <li><code>zstd</code>: High compression ratios with configurable levels (0-22). Better compression than LZ4 but slower.</li> <li><code>none</code>: No general compression applied (default).</li> <li><code>fsst</code>: Fast Static Symbol Table compression for string data.</li> </ul> <p>General compression is applied on top of other encoding techniques (RLE, BSS, bitpacking, etc.) to further reduce data size. For mini-block layouts, compression is applied to entire mini-blocks. For full-zip layouts with large values (\u226532KiB), compression is automatically applied per-value.</p>"},{"location":"format/file/encoding/#compression-level","title":"Compression Level","text":"<p>The compression level is scheme dependent. Currently the following schemes support the following levels:</p> Scheme Crate Used Levels Default <code>zstd</code> <code>zstd</code> <code>0-22</code> <code>crate dependent</code> (3 as of this writing) <code>lz4</code> <code>lz4</code> N/A The LZ4 crate has two modes (fast and high compression) and currently this is not exposed to configuration. The LZ4 crate wraps a C library and the default is dependent on the C library. The default as of this writing is fast <p>Higher compression levels generally provide better compression at the cost of slower encoding speed. Decoding speed is typically less affected by the compression level.</p>"},{"location":"format/file/encoding/#run-length-encoding-rle-threshold","title":"Run Length Encoding (RLE) Threshold","text":"<p>The RLE threshold is used to determine whether or not to apply run-length encoding. The threshold is a ratio calculated by dividing the number of runs by the number of values. If the ratio is less than the threshold then we apply run-length encoding. The default is 0.5 which means we apply run-length encoding if the number of runs is less than half the number of values.</p> <p>Key points: - RLE is automatically selected when data has sufficient repetition (run_count / num_values &lt; threshold) - Supported types: All fixed-width primitives (u8, i8, u16, i16, u32, i32, f32, u64, i64, f64) - Maximum chunk size: 2048 values per mini-block - Setting threshold to <code>0.0</code> effectively disables RLE - Setting threshold to <code>1.0</code> makes RLE very aggressive (used whenever any runs exist)</p> <p>RLE is particularly effective for: - Sorted or partially sorted data - Columns with many repeated values (status codes, categories, etc.) - Low-cardinality columns</p>"},{"location":"format/file/encoding/#byte-stream-split-bss_1","title":"Byte Stream Split (BSS)","text":"<p>The configuration variable for BSS is a simple enum. A value of <code>off</code> means to never apply BSS, a value of <code>on</code> means to always apply BSS, and a value of <code>auto</code> means to apply BSS based on an entropy calculation (see code for details).</p> <p>Important: BSS is only applied when the <code>lance-encoding:compression</code> variable is also set (to a non-<code>none</code> value). BSS is a data transformation that makes floating-point data more compressible; it does not reduce size on its own.</p> <p>Key points: - Supported types: Only 32-bit and 64-bit data (f32, f64, timestamps) - Maximum chunk sizes: 1024 values (f32), 512 values (f64) - <code>auto</code> mode: Uses entropy analysis with 0.5 sensitivity threshold - <code>on</code> mode: Always applies BSS for supported types - <code>off</code> mode: Never applies BSS</p> <p>BSS works by splitting multi-byte values by byte position, creating separate byte streams. This clusters similar bits together (especially mantissa bits in floating-point numbers), which general compression algorithms can then compress more effectively.</p> <p>BSS is particularly effective for: - Floating-point measurements with similar ranges - Time-series data with consistent precision - Scientific data with correlated mantissa patterns</p>"},{"location":"format/file/encoding/#dictionary-divisor","title":"Dictionary Divisor","text":"<p>Currently this is used to determine whether or not we apply dictionary encoding. First, we use HLL to estimate the number of unique values in the column. Then we divide the number of total values by the divisor to get a threshold. If the number of unique values is less than the threshold then we apply dictionary encoding. The configuration variable defines the divisor that we apply and it defaults to 2 which means we apply dictionary encoding if we estimate that less than half the values are unique.</p> <p>Dictionary encoding is effective for columns with low cardinality where the same values repeat many times. The dictionary is stored once per page and indices are stored in place of the actual values.</p> <p>This is likely to change in future versions.</p>"},{"location":"format/file/encoding/#packed-struct-encoding","title":"Packed Struct Encoding","text":"<p>Packed struct encoding is a semi-structural transformation described above. When enabled, struct values are stored in row-major format rather than the default columnar format. This reduces the number of I/O operations needed for random access but prevents reading individual fields independently.</p> <p>This is always opt-in and should only be used when all struct fields are typically accessed together.</p>"},{"location":"format/file/versioning/","title":"Versioning","text":"<p>The Lance file format has a single version number for both the overall file format and the encoding strategy. The major number is changed when the file format itself is modified while the minor number is changed when only the encoding strategy is modified. Newer versions will typically have better performance and compression but may not be readable by older versions of Lance.</p> <p>In addition, the latest version of the file format (next) is unstable and should not be used for production use cases. Breaking changes could be made to unstable encodings and that would mean that files written with these encodings are no longer readable by any newer versions of Lance. The <code>next</code> version should only be used for experimentation and benchmarking upcoming features.</p> <p>The following values are supported:</p> Version Minimal Lance Version Maximum Lance Version Description 0.1 Any 0.34 (write) This is the initial Lance format. It is no longer writable. 2.0 0.16.0 Any Rework of the Lance file format that removed row groups and introduced null support for lists, fixed size lists, and primitives 2.1 (unstable) None Any Enhances integer and string compression, adds support for nulls in struct fields, and improves random access performance with nested fields. legacy N/A N/A Alias for 0.1 stable N/A N/A Alias for the latest stable version (currently 2.0) next N/A N/A Alias for the latest unstable version (currently 2.1)"},{"location":"format/namespace/","title":"Lance Namespace Spec","text":"<p>Lance Namespace is an open specification for describing access and operations against a collection of tables in a multimodal lakehouse. The spec provides a unified model for table-related objects, their relationships within a hierarchy, and the operations available on these objects, enabling integration with metadata services and compute engines alike.</p>"},{"location":"format/namespace/#what-the-lance-namespace-spec-contains","title":"What the Lance Namespace Spec Contains","text":"<p>The Lance Namespace spec consists of four main parts:</p> <ol> <li> <p>Client Spec: A consistent abstraction that adapts to various catalog specs,    allowing users to access and operate on a collection of tables in a multimodal lakehouse.     This is the core reason why we call it \"Namespace\" rather than \"Catalog\" -     namespace can mean catalog, schema, metastore, database, metalake, etc.,     and the spec provides a unified interface across all of them.</p> </li> <li> <p>Native Catalog Specs: Natively maintained catalog specs that are compliant with the Lance Namespace client spec:</p> <ul> <li>Directory Namespace Spec: A storage-only catalog spec that requires no external metadata service dependencies \u2014   tables are organized directly on storage (local filesystem, S3, GCS, Azure, etc.)</li> <li>REST Namespace Spec: A REST-based catalog spec ideal for data infrastructure teams that want to develop   their own custom handling in their specific enterprise environments.</li> </ul> </li> <li> <p>Implementation Specs: Defines how a given catalog spec integrates with the client spec.    It details how an object in a Lance Namespace maps to an object in the specific catalog spec,    and how each operation in Lance Namespace is fulfilled by the catalog spec.    The implementation specs for Directory and REST namespaces are part of the native Lance Namespace spec.    Implementation specs for other catalog specs    (e.g. Apache Polaris, Unity Catalog, Apache Hive Metastore, Apache Iceberg REST Catalog)    are considered integrations - anyone can provide additional implementation specs outside Lance Namespace,    and they can be owned by external parties without needing to go through the Lance community voting process to be adopted.</p> </li> <li> <p>Partitioning Spec: Defines a storage format for partitioned namespaces built on the Directory Namespace.    It enables organizing data into physically separated units (partitions) that share a common schema,    with support for partition evolution, pruning, and multi-partition transactions.</p> </li> </ol>"},{"location":"format/namespace/#how-the-spec-translates-to-code","title":"How the Spec Translates to Code","text":"<p>For each programming language, a Lance Namespace Client SDK provides a unified interface that compute engines can integrate against. For example, the Java SDK <code>org.lance:lance-namespace-core</code> enables engines like Apache Spark, Apache Flink, Apache Kafka, Trino, Presto, etc. to build their Lance connectors.</p> <p>Each catalog spec has corresponding implementations in supported languages that fulfill the client SDK interface/trait. In this example, the Java implementations for Directory and REST namespaces are in <code>org.lance:lance-core</code>, while integrations are provided by dedicated libraries like <code>org.lance:lance-namespace-polaris</code> and <code>org.lance:lance-namespace-unity</code>.</p> <p>All implementations must follow their respective implementation specs as the source of truth. This separation of spec and implementation enables both developers to have consistent implementations across different language SDKs, as well as AI code agents to easily generate high-quality, language-specific implementations.</p>"},{"location":"format/namespace/partitioning-spec/","title":"Lance Partitioning Spec","text":"<p>Partitioning is a common data organization strategy that divides data into physically separated units. Lance tables do not natively support partitioning, instead promoting clustering to achieve similar performance benefits.</p> <p>However, there are use cases where true partitioning makes sense. For example, an organization might want to store one table per business unit,  where each table is fully isolated yet shares a common schema and data management lifecycle. Most of the time, queries like vector search are only against a specific partition, but sometimes  it would be convenient to query across all business units as a unified dataset.</p> <p>A Partitioned Namespace is designed for these use cases. It is a Directory Namespace containing a collection of tables that share a common schema. These tables are physically separated and independent, but logically related through partition fields definition.</p> <p>This document defines the storage format for Partitioned Namespace. Similar to Lance being a storage-only format, the storage-only Directory Namespace spec serves as the foundation for this Partitioned Namespace format.</p> <p>The following example illustrates the logical layout of a partitioned namespace:</p> <pre><code>Root Namespace (__manifest Lance table)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Table metadata (root namespace properties):      \u2502\n\u2502     - schema = &lt;shared Schema&gt;                   \u2502\n\u2502     - partition_spec_v1 = [event_date]           \u2502\n\u2502     - partition_spec_v2 = [event_year, country]  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n                Spec Version Level\n                        \u2502\n        \u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                               \u2502\n       v1                              v2\n    (Namespace)                     (Namespace)\n        \u2502                               \u2502\n        \u2502\u2500\u2500 &lt;id1&gt;                       \u2502\u2500\u2500 &lt;id3&gt;\n        \u2502   (Namespace)                 \u2502   (Namespace)\n        \u2502   event_date=2025-12-10       \u2502   event_year=2025\n        \u2502     \u2514\u2500\u2500 dataset (Table)       \u2502     \u2502\n        \u2502                               \u2502     \u2514\u2500\u2500 &lt;id4&gt;\n        \u2502\u2500\u2500 &lt;id2&gt;                       \u2502         (Namespace)\n        \u2502   (Namespace)                 \u2502         country=US\n        \u2502   event_date=2025-12-11       \u2502           \u2514\u2500\u2500 dataset (Table)\n        \u2502     \u2514\u2500\u2500 dataset (Table)       \u2502\n        \u2514\u2500\u2500 ...                         \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"format/namespace/partitioning-spec/#metadata-definition","title":"Metadata Definition","text":"<p>A directory namespace is identified as a partitioned namespace if the <code>__manifest</code> table's metadata contains at least one partition spec version key.</p> <p>The following properties are stored in the <code>__manifest</code> table's metadata map:</p> <ul> <li><code>partition_spec_v&lt;N&gt;</code> (String): A JSON string representing a partition spec object for version N. The object contains the spec ID and an array of partition field definitions. See Partition Spec for details.</li> <li><code>schema</code> (String): A json string describing the Schema of the entire partitioned namespace, based on the JsonArrowSchema schema in client spec. See Namespace Schema for more details.</li> </ul> <p>See Appendix A: Metadata Example for a complete example.</p>"},{"location":"format/namespace/partitioning-spec/#schema","title":"Schema","text":"<p>The Namespace Schema defines the schema for all partition tables in the partitioned namespace. Implementations must enforce that all partition table schemas must be consistent with each other, as well as with the namespace schema. Most importantly, each field in the schema has a unique field ID stored in metadata under the key <code>lance:field_id</code>. Field IDs are never reused and must remain consistent across partition tables. This ensures partition specs using <code>source_ids</code> remain valid even if columns are renamed.</p>"},{"location":"format/namespace/partitioning-spec/#partition-spec","title":"Partition Spec","text":"<p>The Namespace Partition Spec defines how to derive partition values from a record in a partitioned namespace. The partitioning information is stored in <code>partition_spec_v&lt;N&gt;</code> (e.g., <code>partition_spec_v1</code>), which is a JSON object containing a spec ID and an array of partition field definitions.</p>"},{"location":"format/namespace/partitioning-spec/#partition-spec-schema","title":"Partition Spec Schema","text":"<p>A partition spec is a JSON object with the following fields:</p> Field JSON representation Example Description <code>id</code> <code>JSON int</code> <code>1</code> The spec version ID, matching the <code>N</code> in the key name <code>fields</code> <code>JSON array of objects</code> <code>[...]</code> Array of partition field definitions (see Partition Field Schema)"},{"location":"format/namespace/partitioning-spec/#partition-field-schema","title":"Partition Field Schema","text":"<p>Each element in the <code>fields</code> array is a partition field object with the following fields:</p> Field JSON representation Example Description <code>field_id</code> <code>JSON string</code> <code>\"event_year\"</code> Unique identifier for this partition field (must not be renamed) <code>source_ids</code> <code>JSON int array</code> <code>[1]</code> Field IDs of the source columns in the schema <code>transform</code> <code>JSON object</code> <code>{ \"type\": \"year\" }</code> Well-known partition transform (see Partition Transform). Exactly one of <code>transform</code> or <code>expression</code> must be specified. <code>expression</code> <code>JSON string</code> <code>\"date_part('year', col0)\"</code> DataFusion SQL expression using <code>col0</code>, <code>col1</code>, ... as column references. Exactly one of <code>transform</code> or <code>expression</code> must be specified. <code>result_type</code> <code>JSON object</code> <code>{ \"type\": \"int32\" }</code> The output type of the partition value (JsonArrowDataType format) <p>Transform vs Expression: Exactly one of <code>transform</code> or <code>expression</code> must be specified. When <code>transform</code> is specified, the expression is derived from the transform type. Custom partition logic that doesn't fit a well-known transform must use <code>expression</code> directly.</p> <p>Partition Field ID: The <code>field_id</code> is a string that uniquely identifies each partition field across all spec versions. It is used as the column name suffix in <code>__manifest</code> (e.g., <code>partition_field_event_year</code>). Once assigned, a <code>field_id</code> must never be renamed or reused. This ensures stable column names in the manifest table.</p> <p>Field ID Reuse: When evolving partition specs, if a new partition field has the same <code>source_ids</code> and <code>transform</code> (or <code>expression</code>) as an existing field, the same <code>field_id</code> must be reused. Otherwise, a new unique <code>field_id</code> must be assigned.</p> <p>Source Field IDs: The <code>source_ids</code> array references field IDs stored in the schema's field metadata under the key <code>lance:field_id</code>. Using field IDs instead of column names ensures that partition specs remain valid even when source columns are renamed. In the partition expression, source columns are referenced as <code>col0</code>, <code>col1</code>, <code>col2</code>, etc., corresponding to the order of field IDs in the <code>source_ids</code> array.</p>"},{"location":"format/namespace/partitioning-spec/#partition-expression","title":"Partition Expression","text":"<p>The <code>expression</code> field contains a DataFusion SQL expression that transforms source column values into a partition value. The placeholders <code>col0</code>, <code>col1</code>, <code>col2</code>, etc. represent the source columns in order corresponding to the <code>source_ids</code> array. For single-column partitions, only <code>col0</code> is used. The expression result type is declared by the <code>result_type</code> field.</p> <p>All partition expressions must satisfy the following requirements:</p> <ol> <li>Deterministic: The same input value must always produce the same output value.</li> <li>Stateless: The expression must not depend on external state (e.g., current time, random values, session variables).</li> <li>Type-promotion resistant: The expression must produce the same result for equivalent values regardless of their numeric type (e.g., <code>int32(5)</code> and <code>int64(5)</code> must yield the same partition value).</li> <li>Column removal resistant: If a source field ID is not found in the schema, the column should be interpreted as NULL.</li> <li>NULL safe: The partition expression should properly handle NULL case and have defined behavior (e.g. return NULL if NULL for single-column expression, ignore the NULL column for multi-column expression)</li> <li>Consistent with result type: The <code>result_type</code> field declares the output type of the partition expression as an Arrow data type.   This enables type checking without expression evaluation and ensures consistency across implementations.   The partition expression's return type must be consistent with the result type in non-NULL case.</li> </ol>"},{"location":"format/namespace/partitioning-spec/#partition-transform","title":"Partition Transform","text":"<p>Partition transforms are well-known partition expressions with structured metadata that enables query optimization such as Storage Partitioned Join.  When a partition field uses a well-known transform, the <code>transform</code> field should be specified instead of the <code>expression</code> field.</p>"},{"location":"format/namespace/partitioning-spec/#transform-schema","title":"Transform Schema","text":"<p>The <code>transform</code> field is a JSON object with the following structure:</p> Field JSON representation Required Description <code>type</code> <code>JSON string</code> Yes The transform type (see table below) <code>num_buckets</code> <code>JSON int</code> For bucket transforms Number of buckets N <code>width</code> <code>JSON int</code> For truncate transforms Truncation width W"},{"location":"format/namespace/partitioning-spec/#supported-transforms","title":"Supported Transforms","text":"Transform Type Parameters Derived Expression Result Type Description <code>identity</code> (none) <code>col0</code> same as source Source value, unmodified <code>year</code> (none) <code>date_part('year', col0)</code> <code>int32</code> Extract year from date/timestamp <code>month</code> (none) <code>date_part('month', col0)</code> <code>int32</code> Extract month (1-12) from date/timestamp <code>day</code> (none) <code>date_part('day', col0)</code> <code>int32</code> Extract day of month from date/timestamp <code>hour</code> (none) <code>date_part('hour', col0)</code> <code>int32</code> Extract hour (0-23) from timestamp <code>bucket</code> <code>num_buckets</code> <code>abs(murmur3(col0)) % N</code> <code>int32</code> Hash single column into N buckets <code>multi_bucket</code> <code>num_buckets</code> <code>abs(murmur3_multi(col0, col1, ...)) % N</code> <code>int32</code> Hash multiple columns into N buckets <code>truncate</code> <code>width</code> <code>left(col0, W)</code> (string) or <code>col0 - (col0 % W)</code> (numeric) same as source Truncate to width W"},{"location":"format/namespace/partitioning-spec/#hash-functions","title":"Hash Functions","text":"<p>The <code>bucket</code> and <code>multi_bucket</code> transforms use Murmur3 hash functions provided as Lance extensions to DataFusion:</p> <ul> <li><code>murmur3(col)</code>: Computes the 32-bit Murmur3 hash (x86 variant, seed 0) of a single column. Returns a signed 32-bit integer. Returns NULL if input is NULL.</li> <li><code>murmur3_multi(col0, col1, ...)</code>: Computes the Murmur3 hash across multiple columns. Returns a signed 32-bit integer. NULL fields are ignored during hashing; returns NULL only if all inputs are NULL.</li> </ul> <p>The hash result is wrapped with <code>abs()</code> and modulo <code>N</code> to produce a non-negative bucket number in the range <code>[0, N)</code>. For implementations that do not use DataFusion, the same behavior for hashing should be preserved.</p>"},{"location":"format/namespace/partitioning-spec/#physical-layout-and-naming","title":"Physical Layout and Naming","text":"<p>A partitioned namespace supports multi-level partitioning with the following physical hierarchy:</p> <ul> <li>Root Namespace: The root namespace is implicit and represented by the <code>__manifest</code> table itself. Its properties (partition specs, schema) are stored in the <code>__manifest</code> table's metadata.</li> <li>Spec Version Namespace: The first-level child namespace, named <code>v1</code>, <code>v2</code>, etc. This identifies which partition spec version the data underneath was written with. When retrieving properties via API, these namespaces dynamically include a <code>partition_spec</code> property containing the partition spec for that version (copied from the root's <code>partition_spec_v&lt;N&gt;</code>).</li> <li>Partition Namespace: Each subsequent level of child namespaces represents a partition field. The order of partition namespace levels corresponds to the order of partition fields in the partition spec. Namespace names are randomly generated identifiers (see Namespace Naming).</li> <li>Partition Table: At the end of the partition hierarchy, a <code>Table</code> object with the fixed name <code>dataset</code> contains the actual data. This is a standard, independently accessible Lance <code>Dataset</code> containing a subset of the partitioned namespace's data.</li> </ul> <p>See Appendix B: Physical Layout Example for a complete directory structure example.</p>"},{"location":"format/namespace/partitioning-spec/#partition-namespace-naming","title":"Partition Namespace Naming","text":"<p>Partition namespaces use random identifier naming to avoid issues with special characters in partition values.</p> <p>Partition namespace names are randomly generated 16-character base36 strings (using characters <code>a-z0-9</code>). This provides ~83 bits of entropy, ensuring virtually zero collision probability for any practical number of partitions. This approach ensures:</p> <ul> <li>No conflicts with reserved characters (e.g., <code>$</code>, <code>/</code>, <code>=</code>) that may appear in partition column values</li> <li>Consistent namespace names across different client implementations</li> <li>Fixed-length, predictable namespace identifiers</li> </ul> <p>Since namespace names are random identifiers, the actual partition values are stored in the <code>__manifest</code> table's partition columns (see Manifest Table Schema).</p>"},{"location":"format/namespace/partitioning-spec/#runtime-namespace-properties","title":"Runtime Namespace Properties","text":"<p>Since namespace names are random identifiers, the actual partition values are stored in the <code>__manifest</code> table's partition columns (see Manifest Table Schema).</p> <p>Implementations may dynamically populate properties when retrieving namespace information via API:</p> <ul> <li>For partition namespaces: <code>partition.&lt;field_id&gt; = &lt;value&gt;</code> entries</li> <li>For spec version namespaces (v1, v2, etc.): <code>partition_spec</code> containing the partition spec for that version</li> </ul> <p>These runtime properties are optional. Implementations may choose not to expose them for security or other reasons. See Appendix E: Runtime Namespace Properties Example for examples.</p>"},{"location":"format/namespace/partitioning-spec/#query-optimization","title":"Query Optimization","text":"<p>This section describes query optimization techniques that leverage partitioned namespace metadata.</p>"},{"location":"format/namespace/partitioning-spec/#manifest-table-schema","title":"Manifest Table Schema","text":"<p>The <code>__manifest</code> table schema is extended to include partition columns for efficient query optimization use cases.  Instead of parsing namespace names to filter partitions, query engines can directly push down predicates to the manifest table.</p> <p>Extended Schema: For each partition field defined in any partition spec version,  the <code>__manifest</code> table includes an additional nullable column.  The column name is <code>partition_field_{i}</code> where <code>{i}</code> is the partition field's <code>field_id</code>, and the type is the partition field's <code>result_type</code>.  This naming convention avoids potential conflicts with user-defined column names.  When a new partition spec version is defined, the <code>__manifest</code> table schema is updated accordingly to include any new partition columns.</p> Column Type Description <code>object_id</code> <code>string</code> Full namespace path with <code>$</code> separator (existing) <code>object_type</code> <code>string</code> <code>\"namespace\"</code> or <code>\"table\"</code> (existing) <code>metadata</code> <code>string</code> JSON-encoded metadata/properties (existing) <code>read_version</code> <code>uint64</code> Table version for reads (optional, see Transaction) <code>read_branch</code> <code>string</code> Table branch for reads (optional, see Transaction) <code>read_tag</code> <code>string</code> Table tag for reads (optional, see Transaction) <code>partition_field_{field_id}</code> <code>&lt;type&gt;</code> Partition value for the field (nullable, inherited from parent namespaces) ... ... Additional partition field columns as needed <p>Partition values are inherited from parent namespaces - each row has all partition values from its ancestors.  See Appendix C: Manifest Table Example for a complete example.</p>"},{"location":"format/namespace/partitioning-spec/#partition-pruning","title":"Partition Pruning","text":"<p>Partition pruning is performed via the <code>__manifest</code> table, which contains partition column values for efficient filtering.</p> <p>Here is the end-to-end workflow:</p> <ol> <li>Query engine analyzes the query predicate to identify filters on partition columns</li> <li>For each partition expression, the engine evaluates the expression with the query values to compute the expected partition value(s)</li> <li>Engine queries <code>__manifest</code> with filters on the partition columns</li> <li>Engine retrieves the paths of matching <code>dataset</code> tables</li> <li>Engine scans only the relevant partition tables</li> </ol>"},{"location":"format/namespace/partitioning-spec/#storage-partitioned-join","title":"Storage Partitioned Join","text":"<p>Storage Partitioned Join (SPJ) is an optimization that eliminates or reduces shuffle operations when  joining two partitioned datasets on their partition columns.  When both sides of a join are partitioned by the same or compatible transforms on the join keys,  the query engine can join partitions directly without redistributing data.</p> <p>SPJ can be applied when:</p> <ol> <li>Both datasets are partitioned by the same column(s) used in the join predicate</li> <li>The partition transforms are compatible (see Transform Compatibility)</li> <li>The query engine supports reporting partition information</li> </ol> <p>For SPJ to work, the partition transforms must be compatible:</p> <ul> <li>Same transform type: Both sides use the same transform (e.g., both use <code>year</code> on a date column)</li> <li>Bucket divisibility: For bucket transforms, one bucket count must evenly divide the other. The side with fewer buckets becomes the \"coarser\" partition that may match multiple finer partitions.</li> <li>Time hierarchy: Coarser time transforms can match finer ones (e.g., <code>day</code> partitions can be grouped to match <code>month</code> partitions)</li> </ul> <p>Here is the end-to-end workflow:</p> <ol> <li>Query engine analyzes the join predicate to identify join keys</li> <li>For each partitioned namespace, the engine reads the partition spec to determine the transform on join keys</li> <li>If transforms are compatible, the engine computes which partitions can be joined without shuffle:<ul> <li>For identical transforms: Partitions with equal partition values are joined directly</li> <li>For compatible bucket transforms: Partitions from the coarser side match multiple partitions from the finer side based on <code>finer_bucket % coarser_bucket_count</code></li> <li>For compatible time transforms: Partitions from the finer side are grouped to match coarser partitions</li> </ul> </li> <li>Engine executes the join partition-by-partition, avoiding full data shuffle</li> </ol> <p>See Appendix F: Storage Partitioned Join Example for a complete example.</p>"},{"location":"format/namespace/partitioning-spec/#partition-evolution","title":"Partition Evolution","text":"<p>The partition spec supports versioning to allow partition strategies to evolve over time.  Each partition spec version defines its own set of partition columns and expressions.  Data written to the partitioned namespace records which spec version it was created under via the version namespace (<code>v1/</code>, <code>v2/</code>, etc.).</p>"},{"location":"format/namespace/partitioning-spec/#evolution-scenarios","title":"Evolution Scenarios","text":"<ul> <li>Adding partition columns: Create a new spec version with additional partition columns. New data is written under the new version while existing partitions remain accessible.</li> <li>Changing partition expressions: Create a new spec version with different expressions (e.g., changing from daily to yearly partitioning). Both versions coexist.</li> <li>Removing partition columns: Create a new spec version without certain columns. Legacy data under old versions remains queryable.</li> </ul>"},{"location":"format/namespace/partitioning-spec/#compatibility-with-partition-pruning","title":"Compatibility with Partition Pruning","text":"<p>When querying across multiple spec versions, the query engine must handle each version according to its partition spec.  For example, if <code>v1</code> partitions by <code>event_date</code> and <code>v2</code> partitions by <code>year(event_date)</code>, a query filtering on <code>event_date = '2025-12-10'</code> will:</p> <ol> <li>Match exact partitions in <code>v1</code></li> <li>Compute <code>year('2025-12-10') = 2025</code> and scan all matching year partitions in <code>v2</code></li> </ol> <p>This design ensures backward compatibility while enabling partition strategy evolution without data migration.</p>"},{"location":"format/namespace/partitioning-spec/#transaction","title":"Transaction","text":""},{"location":"format/namespace/partitioning-spec/#single-partition-transaction","title":"Single-Partition Transaction","text":"<p>Operations within a single partition table are ACID-compliant according to the Lance table specification. Each partition is an independent Lance table, so reads and writes to a single partition follow standard Lance transaction semantics.</p>"},{"location":"format/namespace/partitioning-spec/#multi-partition-transaction","title":"Multi-Partition Transaction","text":"<p>By default, operations across multiple partitions have weaker guarantees:</p> <ul> <li>Writes across partitions are not atomic or consistent: A write that affects multiple partitions may partially succeed, leaving some partitions updated while others are not.</li> <li>Reads across partitions are not isolated: A read spanning multiple partitions may observe different versions of each partition, leading to inconsistent views.</li> </ul> <p>To enable stronger transactional guarantees across partitions, the <code>__manifest</code> table can optionally include <code>read_version</code>, <code>read_branch</code>, and <code>read_tag</code> columns for a table. These columns record which version of each partition table to read.</p>"},{"location":"format/namespace/partitioning-spec/#read-behavior","title":"Read Behavior","text":"<p>Users should specify one of the following combinations:</p> <ol> <li><code>read_version</code> only: Read the specified version from the main branch.</li> <li><code>read_branch</code> + <code>read_version</code>: Read the specified version from the specified branch.</li> <li><code>read_tag</code> only: Read the version referenced by the specified tag.</li> </ol> <p>When all columns are NULL or not present, readers should read the latest version from the main branch.</p>"},{"location":"format/namespace/partitioning-spec/#commit-behavior","title":"Commit Behavior","text":"<p>Multi-partition transactions are guarded by commits against the <code>__manifest</code> table. A typical multi-partition write follows this pattern:</p> <ol> <li>Write data to each affected partition table independently</li> <li>Atomically update the <code>read_version</code> (and optionally <code>read_branch</code> or <code>read_tag</code>) of all affected partitions in a single <code>__manifest</code> commit</li> </ol> <p>This ensures all-or-nothing visibility of changes across partitions.</p>"},{"location":"format/namespace/partitioning-spec/#conflict-resolution","title":"Conflict Resolution","text":"<p>If concurrent commits have been committed to <code>__manifest</code> since the transaction began, the implementation must either:</p> <ol> <li>Rebase the current commit onto the latest <code>__manifest</code> version and retry the commit, or</li> <li>Fail the current commit and return an error to the caller</li> </ol> <p>Implementations are responsible for ensuring the appropriate conflict detection and resolution strategy to guarantee ACID semantics during multi-partition transactions.</p>"},{"location":"format/namespace/partitioning-spec/#appendices","title":"Appendices","text":""},{"location":"format/namespace/partitioning-spec/#appendix-a-metadata-example","title":"Appendix A: Metadata Example","text":"<p>A complete example of partitioned namespace metadata properties with two spec versions:</p> <pre><code>{\n  \"partition_spec_v1\": {\n    \"id\": 1,\n    \"fields\": [\n      {\n        \"field_id\": \"event_date\",\n        \"source_ids\": [1],\n        \"transform\": { \"type\": \"identity\" },\n        \"result_type\": { \"type\": \"date32\" }\n      }\n    ]\n  },\n  \"partition_spec_v2\": {\n    \"id\": 2,\n    \"fields\": [\n      {\n        \"field_id\": \"event_year\",\n        \"source_ids\": [1],\n        \"transform\": { \"type\": \"year\" },\n        \"result_type\": { \"type\": \"int32\" }\n      },\n      {\n        \"field_id\": \"country\",\n        \"source_ids\": [2],\n        \"transform\": { \"type\": \"identity\" },\n        \"result_type\": { \"type\": \"utf8\" }\n      }\n    ]\n  },\n  \"schema\": {\n    \"fields\": [\n      {\n        \"name\": \"id\",\n        \"nullable\": false,\n        \"type\": { \"type\": \"int64\" },\n        \"metadata\": { \"lance:field_id\": \"0\" }\n      },\n      {\n        \"name\": \"event_date\",\n        \"nullable\": true,\n        \"type\": { \"type\": \"date32\" },\n        \"metadata\": { \"lance:field_id\": \"1\" }\n      },\n      {\n        \"name\": \"country\",\n        \"nullable\": true,\n        \"type\": { \"type\": \"utf8\" },\n        \"metadata\": { \"lance:field_id\": \"2\" }\n      }\n    ]\n  }\n}\n</code></pre> <p>In this example: - <code>v1</code> partitions by <code>event_date</code> using the identity transform with <code>result_type: date32</code> - <code>v2</code> partitions first by year of <code>event_date</code> using the year transform with <code>result_type: int32</code>, then by <code>country</code> using the identity transform with <code>result_type: utf8</code> - The <code>__manifest</code> table will have three partition columns: <code>partition_field_event_date</code> (date32), <code>partition_field_event_year</code> (int32), <code>partition_field_country</code> (utf8) - The schema follows JsonArrowSchema format</p>"},{"location":"format/namespace/partitioning-spec/#appendix-b-physical-layout-example","title":"Appendix B: Physical Layout Example","text":"<p>A partitioned namespace with two spec versions (<code>v1</code> partitioned by <code>event_date</code>, <code>v2</code> partitioned by <code>event_year</code> and <code>country</code>) in V2 Manifest:</p> <p>Namespaces exist only as entries in the <code>__manifest</code> table - they do not have physical directories. Only tables (the leaf <code>dataset</code> objects) have directories, following the V2 format <code>&lt;hash&gt;_&lt;object_id&gt;</code>.</p> <pre><code>.\n\u2514\u2500\u2500 /my/dir1/\n    \u251c\u2500\u2500 __manifest/                                                 # The manifest table\n    \u2502   \u251c\u2500\u2500 data/\n    \u2502   \u2502   \u2514\u2500\u2500 ...\n    \u2502   \u2514\u2500\u2500 _versions/\n    \u2502       \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 b4a3c2d1_v1$k7m2n9p4q8r5s3t6$dataset/                       # Table: event_date=2025-12-10\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 55667788_v1$w1x2y3z4a5b6c7d8$dataset/                       # Table: event_date=2025-12-11\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 aabbccdd_v2$e9f0g1h2i3j4k5l6$m7n8o9p0q1r2s3t4$dataset/      # Table: event_year=2025, country=US\n    \u2502   \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 ...\n</code></pre> <p>The namespaces (<code>v1</code>, <code>v1$k7m2n9p4q8r5s3t6</code>, etc.) are tracked in the <code>__manifest</code> table but have no corresponding directories.</p>"},{"location":"format/namespace/partitioning-spec/#appendix-c-manifest-table-example","title":"Appendix C: Manifest Table Example","text":"<p>The <code>__manifest</code> table for a partitioned namespace with partition fields <code>event_date</code> (v1), <code>event_year</code> (v2) and <code>country</code> (v2), showing entries from both spec versions:</p> object_id object_type metadata read_version read_branch read_tag partition_field_event_date partition_field_event_year partition_field_country v1 namespace {} NULL NULL NULL NULL NULL NULL v1$k7m2n9p4q8r5s3t6 namespace {} NULL NULL NULL 2025-12-10 NULL NULL v1$k7m2n9p4q8r5s3t6$dataset table {} 5 NULL NULL 2025-12-10 NULL NULL v2 namespace {} NULL NULL NULL NULL NULL NULL v2$e9f0g1h2i3j4k5l6 namespace {} NULL NULL NULL NULL 2025 NULL v2$e9f0g1h2i3j4k5l6$m7n8o9p0q1r2s3t4 namespace {} NULL NULL NULL NULL 2025 US v2$e9f0g1h2i3j4k5l6$m7n8o9p0q1r2s3t4$dataset table {} 3 NULL NULL NULL 2025 US <p>Note: The root namespace properties (<code>partition_spec_v1</code>, <code>partition_spec_v2</code>, <code>schema</code>) are stored in the <code>__manifest</code> table's metadata, not as a row. The <code>object_id</code> uses <code>$</code> as the namespace path separator. Partition columns use the naming convention <code>partition_field_{field_id}</code> where <code>{field_id}</code> is the partition field's string identifier. Partition values are inherited from parent namespaces. When retrieving properties via API, partition values are converted to <code>partition.&lt;field_id&gt; = &lt;value&gt;</code> entries.</p> <p>See Appendix D: Partition Pruning Example for an example of how partition pruning queries work.</p>"},{"location":"format/namespace/partitioning-spec/#appendix-d-partition-pruning-example","title":"Appendix D: Partition Pruning Example","text":"<p>This example demonstrates how a query engine translates a user query into a partition pruning query against the <code>__manifest</code> table.</p> <p>Given a user query:</p> <pre><code>SELECT * FROM partitioned_namespace\nWHERE event_date = '2025-12-10' AND country = 'US'\n</code></pre> <p>The engine translates this to the following <code>__manifest</code> DataFusion query plan to examine related partition tables.</p> <p><pre><code>SELECT object_id, location, read_version, read_branch, read_tag\nFROM __manifest\nWHERE object_type = 'table'\n  AND (\n    (object_id LIKE 'v1$%'\n      AND partition_field_event_date = DATE '2025-12-10')\n    OR\n    (object_id LIKE 'v2$%'\n      AND partition_field_event_year = date_part('year', DATE '2025-12-10')\n      AND partition_field_country = 'US')\n  )\n</code></pre> Notice here that the query plan can leverage the partition expression, in this case <code>date_part('year', col0)</code>. One example way to perform such substitution is:</p> <ol> <li>Parsing the expression string (e.g., <code>date_part('year', col0)</code>) into an expression AST using DataFusion's SQL parser</li> <li>Traversing the AST and replacing all <code>col0</code>, <code>col1</code>, etc. column references with the corresponding literal query values (e.g., <code>DATE '2025-12-10'</code>)</li> <li>Evaluating the modified expression to produce the partition filter value (e.g., <code>2025</code>)</li> </ol> <p>This query returns:</p> object_id location read_version read_branch read_tag v1$k7m2n9p4q8r5s3t6$dataset b4a3c2d1_v1$k7m2n9p4q8r5s3t6$dataset 5 NULL NULL v2$e9f0g1h2i3j4k5l6$m7n8o9p0q1r2s3t4$dataset aabbccdd_v2$e9f0g1h2i3j4k5l6$m7n8o9p0q1r2s3t4$dataset 3 NULL NULL <ul> <li>For partition spec v1, the <code>country = 'US'</code> filter cannot be pushed to partition pruning (v1 has no <code>country</code> partition), so it must be applied during the table scan</li> <li>For partition spec v2, both filters are pushed down: <code>partition_field_event_year = 2025</code> (computed from <code>year(event_date)</code>) and <code>partition_field_country = 'US'</code></li> <li>The engine reads each table at the version specified by <code>read_version</code>, <code>read_branch</code>, or <code>read_tag</code> for consistent snapshot reads</li> </ul>"},{"location":"format/namespace/partitioning-spec/#appendix-e-runtime-namespace-properties-example","title":"Appendix E: Runtime Namespace Properties Example","text":"<p>This appendix shows examples of runtime properties that implementations MAY return when describing namespaces. These are optional behaviors - implementations may choose not to expose them for security or other reasons.</p> <p>Spec Version Namespace</p> <p><code>DescribeNamespace([\"v1\"])</code> returns:</p> <pre><code>{\n  \"properties\": {\n    \"partition_spec\": \"{\\\"id\\\":1,\\\"fields\\\":[{\\\"field_id\\\":\\\"event_date\\\",\\\"source_ids\\\":[1],\\\"transform\\\":{\\\"type\\\":\\\"identity\\\"},\\\"result_type\\\":{\\\"type\\\":\\\"date32\\\"}}]}\"\n  }\n}\n</code></pre> <p>Partition Namespace (v1)</p> <p><code>DescribeNamespace([\"v1\", \"k7m2n9p4q8r5s3t6\"])</code> returns:</p> <pre><code>{\n  \"properties\": {\n    \"partition.event_date\": \"2025-12-10\"\n  }\n}\n</code></pre> <p>Partition Namespace (v2, first level)</p> <p><code>DescribeNamespace([\"v2\", \"e9f0g1h2i3j4k5l6\"])</code> returns:</p> <pre><code>{\n  \"properties\": {\n    \"partition.event_year\": \"2025\"\n  }\n}\n</code></pre> <p>Partition Namespace (v2, second level)</p> <p><code>DescribeNamespace([\"v2\", \"e9f0g1h2i3j4k5l6\", \"m7n8o9p0q1r2s3t4\"])</code> returns:</p> <pre><code>{\n  \"properties\": {\n    \"partition.country\": \"US\"\n  }\n}\n</code></pre> <p>Note: Each namespace only returns the partition value for its own level. To get all partition values in a path, the client must query each ancestor namespace.</p>"},{"location":"format/namespace/partitioning-spec/#appendix-f-storage-partitioned-join-example","title":"Appendix F: Storage Partitioned Join Example","text":"<p>This example demonstrates how a query engine performs a Storage Partitioned Join (SPJ) between two partitioned namespaces.</p> <p>Setup: Two partitioned namespaces with compatible bucket transforms:</p> <ul> <li><code>orders</code> namespace: partitioned by <code>bucket(customer_id, 16)</code> with partition field <code>customer_bucket</code></li> <li><code>customers</code> namespace: partitioned by <code>bucket(id, 8)</code> with partition field <code>id_bucket</code></li> </ul> <p>User Query:</p> <pre><code>SELECT o.*, c.name\nFROM orders o\nJOIN customers c ON o.customer_id = c.id\n</code></pre> <p>SPJ Analysis:</p> <ol> <li>The engine reads partition specs from both namespaces' <code>__manifest</code> tables</li> <li>Both join keys use bucket transforms: <code>orders.customer_id</code> \u2192 <code>bucket(16)</code>, <code>customers.id</code> \u2192 <code>bucket(8)</code></li> <li>Since 8 divides 16 evenly, the transforms are compatible</li> </ol> <p>Partition Matching:</p> <p>For each <code>customers</code> partition with bucket value <code>i</code>,  the matching <code>orders</code> partitions have bucket values where <code>bucket % 8 == i</code>:</p> customers bucket orders buckets 0 0, 8 1 1, 9 2 2, 10 3 3, 11 4 4, 12 5 5, 13 6 6, 14 7 7, 15 <p>Execution Plan:</p> <p>The engine queries both <code>__manifest</code> tables to get partition locations:</p> <pre><code>-- Get orders partitions\nSELECT partition_field_customer_bucket, location, read_version\nFROM orders.__manifest\nWHERE object_type = 'table'\n\n-- Get customers partitions\nSELECT partition_field_id_bucket, location, read_version\nFROM customers.__manifest\nWHERE object_type = 'table'\n</code></pre> <p>For each customers partition <code>i</code>, the engine:</p> <ol> <li>Reads the customers partition where <code>partition_field_id_bucket = i</code></li> <li>Reads the orders partitions where <code>partition_field_customer_bucket % 8 = i</code></li> <li>Performs a local join without shuffle</li> </ol> <p>Result: The join completes with 8 parallel partition-wise joins instead of a full shuffle of both datasets.</p>"},{"location":"format/namespace/client/","title":"Lance Namespace Client Spec","text":"<p>The Lance Namespace Client Spec defines a standardized framework for accessing and operating on a collection of tables in a multimodal lakehouse. It provides a consistent abstraction that adapts to various catalog specs, allowing users to choose any catalog to store and use Lance tables.</p>"},{"location":"format/namespace/client/#why-namespace-instead-of-catalog","title":"Why \"Namespace\" Instead of \"Catalog\"?","text":"<p>We use the term Namespace rather than Catalog because we want a generic term that fits into any hierarchical structure. Different systems use different names for their organizational units. The Lance Namespace spec provides a unified framework across all of these systems. A \"namespace\" in Lance can represent a catalog, schema, metastore, database, metalake, or any other hierarchical container \u2014 the spec abstracts away these differences.</p> <p>The following examples show how different catalog systems map to Lance Namespace.</p>"},{"location":"format/namespace/client/#directory-1-level","title":"Directory (1-level)","text":"<p>The simplest case: tables directly in a storage directory, a common use case for ML/AI scientists:</p> Directory Lance Namespace /data/ Root Namespace \u2514\u2500 users.lance Table <code>[\"users\"]</code> \u2514\u2500 orders.lance Table <code>[\"orders\"]</code>"},{"location":"format/namespace/client/#unity-catalog-3-level","title":"Unity Catalog (3-level)","text":"<p>Unity Catalog uses a 3-level hierarchy under a metastore (one metastore per server):</p> Unity Catalog Lance Namespace Root Metastore Root Namespace \u2514\u2500 Catalog \"prod\" Namespace <code>[\"prod\"]</code> \u2003\u2003\u2514\u2500 Schema \"analytics\" Namespace <code>[\"prod\", \"analytics\"]</code> \u2003\u2003\u2003\u2003\u2514\u2500 Table \"users\" Table <code>[\"prod\", \"analytics\", \"users\"]</code>"},{"location":"format/namespace/client/#apache-polaris-flexible-levels","title":"Apache Polaris (flexible levels)","text":"<p>Apache Polaris supports arbitrary namespace nesting:</p> Polaris Lance Namespace Root Catalog Root Namespace \u2514\u2500 Namespace \"prod\" Namespace <code>[\"prod\"]</code> \u2003\u2003\u2514\u2500 Namespace \"team_a\" Namespace <code>[\"prod\", \"team_a\"]</code> \u2003\u2003\u2003\u2003\u2514\u2500 Namespace \"ml\" Namespace <code>[\"prod\", \"team_a\", \"ml\"]</code> \u2003\u2003\u2003\u2003\u2003\u2003\u2514\u2500 Table \"model\" Table <code>[\"prod\", \"team_a\", \"ml\", \"model\"]</code>"},{"location":"format/namespace/client/object-relationship/","title":"Objects &amp; Relationships","text":"<p>This page describes the objects in a namespace and their relationships with each other.</p>"},{"location":"format/namespace/client/object-relationship/#namespace-definition","title":"Namespace Definition","text":"<p>A namespace is a centralized repository for discovering, organizing, and managing tables. It can not only contain a collection of tables, but also a collection of namespaces recursively. It is designed to encapsulates concepts including namespace, metastore, database, schema, etc. that frequently appear in other similar data systems to allow easy integration with any system of any type of object hierarchy.</p> <p>Here is an example layout of a namespace:</p> <p></p>"},{"location":"format/namespace/client/object-relationship/#parent-child","title":"Parent &amp; Child","text":"<p>We use the term parent and child to describe relationship between 2 objects. If namespace A directly contains B, then A is the parent namespace of B, i.e. B is a child of A. For examples:</p> <ul> <li>Namespace <code>ns1</code> contains a child namespace <code>ns4</code>. i.e. <code>ns1</code> is the parent namespace of <code>ns4</code>.</li> <li>Namespace <code>ns2</code> contains a child table <code>t2</code>, i.e. <code>t2</code> belongs to parent namespace <code>ns2</code>.</li> </ul>"},{"location":"format/namespace/client/object-relationship/#root-namespace","title":"Root Namespace","text":"<p>A root namespace is a namespace that has no parent. The root namespace is assumed to always exist and is ready to be connected to by a tool to explore objects in the namespace. The lifecycle management (e.g. creation, deletion) of the root namespace is out of scope of this specification.</p>"},{"location":"format/namespace/client/object-relationship/#object-name","title":"Object Name","text":"<p>The name of an object is a string that uniquely identifies the object within the parent namespace it belongs to. The name of any object must be unique among all other objects that share the same parent namespace. For examples:</p> <ul> <li><code>cat2</code>, <code>cat3</code> and <code>cat4</code> are all unique names under the root namespace</li> <li><code>t3</code> and <code>t4</code> are both unique names under <code>cat4</code></li> </ul>"},{"location":"format/namespace/client/object-relationship/#object-identifier","title":"Object Identifier","text":"<p>The identifier of an object uniquely identifies the object within the root namespace it belongs to. The identifier of any object must be unique among all other objects that share the same root namespace.</p> <p>Based on the uniqueness property of an object name within its parent namespace, an object identifier is the list of object names starting from (not including) the root namespace to (including) the object itself. This is also called an list style identifier. For examples:</p> <ul> <li>the list style identifier of <code>cat5</code> is <code>[cat2, cat5]</code></li> <li>the list style identifier of <code>t1</code> is <code>[cat2, cat5, t1]</code></li> </ul> <p>The dollar (<code>$</code>) symbol is used as the default delimiter to join all the names to form an string style identifier, but other symbols could also be used if the dollar sign is used in the object name. For examples:</p> <ul> <li>the string style identifier of <code>cat5</code> is <code>cat2$cat5</code></li> <li>the string style identifier of <code>t1</code> is <code>cat2$cat5$t1</code></li> <li>the string style identifier of <code>t3</code> is <code>cat4#t3</code> when using delimiter <code>#</code></li> </ul>"},{"location":"format/namespace/client/object-relationship/#name-and-identifier-for-root-namespace","title":"Name and Identifier for Root Namespace","text":"<p>The root namespace itself has no name or identifier. When represented in code, its name and string style identifier is represented by an empty or null string, and its list style identifier is represented by an empty or null list.</p> <p>The actual name and identifier of the root namespace is typically assigned by users through some configuration when used in a tool. For example, a root namespace can be called <code>cat1</code> in Ray, but called <code>cat2</code> in Apache Spark, and they are both configured to connect to the same root namespace.</p>"},{"location":"format/namespace/client/object-relationship/#object-level","title":"Object Level","text":"<p>The root namespace is always at level 0. This means if an object has list style identifier with list size <code>N</code>, the object is at the <code>N</code>th level in the entire namespace hierarchy, and its corresponding object identifier has <code>N</code> levels. For examples, a namespace <code>[ns1, ns2]</code> is at level 2, and its identifier <code>ns1$ns2</code> has 2 levels. A table <code>[catalog1, database2, table3]</code> is at level 3, and its identifier <code>catalog1$database2$table3</code> has 3 levels.</p>"},{"location":"format/namespace/client/object-relationship/#leveled-namespace","title":"Leveled Namespace","text":"<p>If every table in the root namespace are at the same level <code>N</code>, the namespace is called leveled, and we say this namespace is a <code>N</code>-level namespace. For example, a directory namespace is a 1-level namespace, and a Hive 2.x namespace is a 2-level namespace.</p>"},{"location":"format/namespace/client/operations/","title":"Namespace Operations","text":"<p>The Lance Namespace Specification defines a list of operations that can be performed against any Lance namespace.</p>"},{"location":"format/namespace/client/operations/#openapi-standardization","title":"OpenAPI Standardization","text":"<p>The spec uses OpenAPI to define the request and response models for each operation. This standardization allows clients in any language to generate a client library from the OpenAPI specification and use it to invoke operations with the corresponding request model, receiving responses in the expected response model.</p> <p>The actual execution of an operation can be:</p> <ul> <li>Client-side: The operation is executed entirely within the client (e.g., directory namespace)</li> <li>Server-side: The operation is sent to a remote server for execution (e.g., REST namespace)</li> <li>Hybrid: A combination of both, depending on the integrated catalog spec and service</li> </ul> <p>This flexibility allows the same client interface to work across different namespace implementations while maintaining consistent request/response contracts.</p>"},{"location":"format/namespace/client/operations/#duality-with-rest-namespace-spec","title":"Duality with REST Namespace Spec","text":"<p>The request and response models defined here are designed to work seamlessly with the REST Namespace spec. The REST namespace uses these same schemas directly as HTTP request and response bodies, minimizing data conversion between client and server.</p> <p>This duality explains why certain fields like <code>id</code> are marked as optional in the request models:</p> <ul> <li>In REST Namespace Spec: The object identifier is already present in the REST route path   (e.g., <code>/v1/table/{id}/describe</code>), so the <code>id</code> field in the request body is optional and   can be omitted to avoid redundancy.</li> <li>In Client-Side Access Spec: When invoking operations directly through a client library   (e.g., for directory namespace), the <code>id</code> field must be specified in the request since   there is no REST route to carry this information.</li> </ul> <p>When both the route path and request body contain the <code>id</code>, the REST server must validate that they match and return a 400 Bad Request error if they differ.  See REST Routes for more details.</p>"},{"location":"format/namespace/client/operations/#operation-list","title":"Operation List","text":"Operation ID Current Version Namespace Table Index Metadata Data Transaction CreateNamespace 1 \u2713 \u2713 ListNamespaces 1 \u2713 \u2713 DescribeNamespace 1 \u2713 \u2713 DropNamespace 1 \u2713 \u2713 NamespaceExists 1 \u2713 \u2713 ListTables 1 \u2713 \u2713 \u2713 ListAllTables 1 \u2713 \u2713 RegisterTable 1 \u2713 \u2713 DescribeTable 1 \u2713 \u2713 TableExists 1 \u2713 \u2713 DropTable 1 \u2713 \u2713 DeregisterTable 1 \u2713 \u2713 InsertIntoTable 1 \u2713 \u2713 MergeInsertIntoTable 1 \u2713 \u2713 UpdateTable 1 \u2713 \u2713 DeleteFromTable 1 \u2713 \u2713 QueryTable 1 \u2713 \u2713 CountTableRows 1 \u2713 \u2713 CreateTable 1 \u2713 \u2713 DeclareTable 1 \u2713 \u2713 CreateEmptyTable 1 (deprecated) \u2713 \u2713 CreateTableIndex 1 \u2713 \u2713 \u2713 CreateTableScalarIndex 1 \u2713 \u2713 \u2713 ListTableIndices 1 \u2713 \u2713 \u2713 DescribeTableIndexStats 1 \u2713 \u2713 \u2713 RestoreTable 1 \u2713 \u2713 RenameTable 1 \u2713 \u2713 ListTableVersions 1 \u2713 \u2713 ExplainTableQueryPlan 1 \u2713 \u2713 AnalyzeTableQueryPlan 1 \u2713 \u2713 AlterTableAddColumns 1 \u2713 \u2713 AlterTableAlterColumns 1 \u2713 \u2713 AlterTableDropColumns 1 \u2713 \u2713 UpdateTableSchemaMetadata 1 \u2713 \u2713 GetTableStats 1 \u2713 \u2713 ListTableTags 1 \u2713 \u2713 GetTableTagVersion 1 \u2713 \u2713 CreateTableTag 1 \u2713 \u2713 DeleteTableTag 1 \u2713 \u2713 UpdateTableTag 1 \u2713 \u2713 DropTableIndex 1 \u2713 \u2713 \u2713 DescribeTransaction 1 \u2713 \u2713 AlterTransaction 1 \u2713 \u2713"},{"location":"format/namespace/client/operations/#recommended-basic-operations","title":"Recommended Basic Operations","text":"<p>To have a functional basic namespace implementation, the following metadata operations are recommended as a minimum:</p> <p>Namespace Metadata Operations:</p> <ul> <li>CreateNamespace - Create a new namespace</li> <li>ListNamespaces - List available namespaces</li> <li>DescribeNamespace - Get namespace details</li> <li>DropNamespace - Remove a namespace</li> </ul> <p>Table Metadata Operations:</p> <ul> <li>DeclareTable - Declare a table as exist</li> <li>ListTables - List tables in a namespace</li> <li>DescribeTable - Get table details</li> <li>DeregisterTable - Unregister a table while preserving its data</li> </ul> <p>These operations provide the foundational metadata management capabilities needed for namespace and table administration without requiring data or index operation support. With the namespace able to provide basic information about the table, the Lance SDK can be used to fulfill the other operations.</p>"},{"location":"format/namespace/client/operations/#restrictions-for-basic-operations","title":"Restrictions for Basic Operations","text":"<p>The following restrictions apply to the recommended basic operations to minimize implementation complexity:</p> <p>DropNamespace: Only the <code>Restrict</code> behavior mode is required. This means the namespace must be empty (no tables or child namespaces) before it can be dropped. The <code>Cascade</code> behavior mode, which recursively drops all contents, is not required for basic implementations.</p> <p>DescribeTable: Only <code>load_detailed_metadata=false</code> (the default) is required. This means the implementation only needs to return the table <code>location</code> without opening the dataset. Returning detailed metadata such as <code>version</code>, <code>schema</code>, and <code>stats</code> (which require opening the dataset) is not required for basic implementations.</p>"},{"location":"format/namespace/client/operations/#why-not-createtable-and-droptable","title":"Why Not <code>CreateTable</code> and <code>DropTable</code>?","text":"<p><code>CreateTable</code> and <code>DropTable</code> are common in most catalog systems, but are intentionally excluded from the recommended basic operations because they involve data operations that present challenges for catalog implementations:</p> <ul> <li> <p>Data Operation Complexity:   Both <code>CreateTable</code> and <code>DropTable</code> are data operations rather than pure metadata operations.   They can be long-running, especially when dealing with large datasets or remote storage systems.   This makes them difficult to implement reliably in catalog systems designed for fast metadata lookups.</p> </li> <li> <p>Atomicity Guarantees:   Data operations require careful handling of atomicity. A failed <code>CreateTable</code> or <code>DropTable</code> operation   can leave the system in an inconsistent state with partially created or deleted data files.   Catalog implementations would need to implement complex cleanup and recovery mechanisms.</p> </li> <li> <p>CreateTable Challenges: <code>CreateTable</code> is particularly difficult for catalogs to fully implement because features like   CREATE TABLE AS SELECT (CTAS) require either complicated staging mechanisms or multi-statement   transaction support.</p> </li> </ul> <p>While some catalog systems can handle these complex workflows, doing so typically requires deep, dedicated integration. Lance Namespace aims to enable as many catalogs as possible to adopt Lance format. By focusing on <code>DeclareTable</code> and <code>DeregisterTable</code> instead of <code>CreateTable</code> and <code>DropTable</code>, namespace implementations only need to handle metadata operations that are simple, fast and atomic across all catalog solutions. <code>CreateTable</code> and <code>DropTable</code> can then be fulfilled by combining these metadata operations with the Lance SDK.</p>"},{"location":"format/namespace/client/operations/#operation-versioning","title":"Operation Versioning","text":"<p>When a backwards incompatible change is introduced, a new operation version needs to be created, with a naming convention of <code>&lt;OperationId&gt;V&lt;version&gt;</code>, for example <code>ListNamespacesV2</code>, <code>DescribeTableV3</code>, etc.</p>"},{"location":"format/namespace/client/operations/#request-and-response-models","title":"Request and Response Models","text":"<p>Each operation has a corresponding request and response model defined in the Models section. The naming convention is <code>&lt;OperationId&gt;Request</code> and <code>&lt;OperationId&gt;Response</code>.</p> <p>For example:</p> <ul> <li><code>CreateNamespaceRequest</code> / <code>CreateNamespaceResponse</code></li> <li><code>ListTablesRequest</code> / <code>ListTablesResponse</code></li> <li><code>DescribeTableRequest</code> / <code>DescribeTableResponse</code></li> </ul>"},{"location":"format/namespace/client/operations/#error-handling","title":"Error Handling","text":"<p>All operations use a standardized error model with numeric error codes.  Each operation documents the specific errors it may return. See Error Handling for the complete list of error codes and per-operation error documentation.</p>"},{"location":"format/namespace/client/operations/errors/","title":"Error Handling","text":"<p>All Lance Namespace operations use a standardized error model for consistent error handling across different implementations and languages.</p>"},{"location":"format/namespace/client/operations/errors/#error-codes","title":"Error Codes","text":"<p>Error codes are globally unique integers that identify the specific error type. These codes are consistent across all Lance Namespace implementations (Python, Java, Rust, REST).</p> Code Name Description 0 Unsupported Operation not supported by this backend 1 NamespaceNotFound The specified namespace does not exist 2 NamespaceAlreadyExists A namespace with this name already exists 3 NamespaceNotEmpty Namespace contains tables or child namespaces 4 TableNotFound The specified table does not exist 5 TableAlreadyExists A table with this name already exists 6 TableIndexNotFound The specified table index does not exist 7 TableIndexAlreadyExists A table index with this name already exists 8 TableTagNotFound The specified table tag does not exist 9 TableTagAlreadyExists A table tag with this name already exists 10 TransactionNotFound The specified transaction does not exist 11 TableVersionNotFound The specified table version does not exist 12 TableColumnNotFound The specified table column does not exist 13 InvalidInput Malformed request or invalid parameters 14 ConcurrentModification Optimistic concurrency conflict 15 PermissionDenied User lacks permission for this operation 16 Unauthenticated Authentication credentials are missing or invalid 17 ServiceUnavailable Service is temporarily unavailable 18 Internal Unexpected server/implementation error 19 InvalidTableState Table is in an invalid state for the operation 20 TableSchemaValidationError Table schema validation failed"},{"location":"format/namespace/client/operations/errors/#per-operation-errors","title":"Per-Operation Errors","text":"<p>Each operation can return a specific set of errors. The following sections document which errors are expected for each operation category.</p>"},{"location":"format/namespace/client/operations/errors/#common-errors","title":"Common Errors","text":"<p>All operations may return the following errors:</p> <ul> <li>0 (Unsupported): The operation is not supported by this backend</li> <li>13 (InvalidInput): The request contains invalid parameters</li> <li>15 (PermissionDenied): The user lacks permission for this operation</li> <li>16 (Unauthenticated): Authentication credentials are missing or invalid</li> <li>17 (ServiceUnavailable): The service is temporarily unavailable</li> <li>18 (Internal): An unexpected internal error occurred</li> </ul>"},{"location":"format/namespace/client/operations/errors/#namespace-metadata-operations","title":"Namespace Metadata Operations","text":"Operation Additional Errors CreateNamespace 2 (NamespaceAlreadyExists) ListNamespaces 1 (NamespaceNotFound) DescribeNamespace 1 (NamespaceNotFound) DropNamespace 1 (NamespaceNotFound), 3 (NamespaceNotEmpty) NamespaceExists 1 (NamespaceNotFound) ListTables 1 (NamespaceNotFound)"},{"location":"format/namespace/client/operations/errors/#table-metadata-operations","title":"Table Metadata Operations","text":"Operation Additional Errors ListAllTables - RegisterTable 1 (NamespaceNotFound), 5 (TableAlreadyExists), 14 (ConcurrentModification) DescribeTable 1 (NamespaceNotFound), 4 (TableNotFound), 11 (TableVersionNotFound) TableExists 1 (NamespaceNotFound), 4 (TableNotFound) DropTable 1 (NamespaceNotFound), 4 (TableNotFound) DeregisterTable 1 (NamespaceNotFound), 4 (TableNotFound) CreateEmptyTable 1 (NamespaceNotFound), 5 (TableAlreadyExists), 14 (ConcurrentModification) RestoreTable 1 (NamespaceNotFound), 4 (TableNotFound), 11 (TableVersionNotFound), 14 (ConcurrentModification) RenameTable 1 (NamespaceNotFound), 4 (TableNotFound), 5 (TableAlreadyExists), 14 (ConcurrentModification) ListTableVersions 1 (NamespaceNotFound), 4 (TableNotFound) GetTableStats 1 (NamespaceNotFound), 4 (TableNotFound) AlterTableAlterColumns 1 (NamespaceNotFound), 4 (TableNotFound), 12 (TableColumnNotFound), 14 (ConcurrentModification), 20 (TableSchemaValidationError) AlterTableDropColumns 1 (NamespaceNotFound), 4 (TableNotFound), 12 (TableColumnNotFound), 14 (ConcurrentModification) UpdateTableSchemaMetadata 1 (NamespaceNotFound), 4 (TableNotFound), 14 (ConcurrentModification)"},{"location":"format/namespace/client/operations/errors/#table-data-operations","title":"Table Data Operations","text":"Operation Additional Errors InsertIntoTable 1 (NamespaceNotFound), 4 (TableNotFound), 14 (ConcurrentModification), 19 (InvalidTableState), 20 (TableSchemaValidationError) MergeInsertIntoTable 1 (NamespaceNotFound), 4 (TableNotFound), 12 (TableColumnNotFound), 14 (ConcurrentModification), 19 (InvalidTableState) UpdateTable 1 (NamespaceNotFound), 4 (TableNotFound), 12 (TableColumnNotFound), 14 (ConcurrentModification), 19 (InvalidTableState) DeleteFromTable 1 (NamespaceNotFound), 4 (TableNotFound), 14 (ConcurrentModification), 19 (InvalidTableState) QueryTable 1 (NamespaceNotFound), 4 (TableNotFound), 11 (TableVersionNotFound), 12 (TableColumnNotFound) CountTableRows 1 (NamespaceNotFound), 4 (TableNotFound), 11 (TableVersionNotFound) CreateTable 1 (NamespaceNotFound), 5 (TableAlreadyExists), 14 (ConcurrentModification), 20 (TableSchemaValidationError) ExplainTableQueryPlan 1 (NamespaceNotFound), 4 (TableNotFound) AnalyzeTableQueryPlan 1 (NamespaceNotFound), 4 (TableNotFound) AlterTableAddColumns 1 (NamespaceNotFound), 4 (TableNotFound), 14 (ConcurrentModification), 20 (TableSchemaValidationError)"},{"location":"format/namespace/client/operations/errors/#index-metadata-operations","title":"Index Metadata Operations","text":"Operation Additional Errors CreateTableIndex 1 (NamespaceNotFound), 4 (TableNotFound), 7 (TableIndexAlreadyExists), 12 (TableColumnNotFound), 14 (ConcurrentModification) CreateTableScalarIndex 1 (NamespaceNotFound), 4 (TableNotFound), 7 (TableIndexAlreadyExists), 12 (TableColumnNotFound), 14 (ConcurrentModification) ListTableIndices 1 (NamespaceNotFound), 4 (TableNotFound) DescribeTableIndexStats 1 (NamespaceNotFound), 4 (TableNotFound), 6 (TableIndexNotFound) DropTableIndex 1 (NamespaceNotFound), 4 (TableNotFound), 6 (TableIndexNotFound)"},{"location":"format/namespace/client/operations/errors/#tag-metadata-operations","title":"Tag Metadata Operations","text":"Operation Additional Errors ListTableTags 1 (NamespaceNotFound), 4 (TableNotFound) GetTableTagVersion 1 (NamespaceNotFound), 4 (TableNotFound), 8 (TableTagNotFound) CreateTableTag 1 (NamespaceNotFound), 4 (TableNotFound), 9 (TableTagAlreadyExists), 11 (TableVersionNotFound), 14 (ConcurrentModification) DeleteTableTag 1 (NamespaceNotFound), 4 (TableNotFound), 8 (TableTagNotFound) UpdateTableTag 1 (NamespaceNotFound), 4 (TableNotFound), 8 (TableTagNotFound), 11 (TableVersionNotFound), 14 (ConcurrentModification)"},{"location":"format/namespace/client/operations/errors/#transaction-metadata-operations","title":"Transaction Metadata Operations","text":"Operation Additional Errors DescribeTransaction 10 (TransactionNotFound) AlterTransaction 10 (TransactionNotFound), 14 (ConcurrentModification)"},{"location":"format/namespace/client/operations/models/AddVirtualColumnEntry/","title":"AddVirtualColumnEntry","text":""},{"location":"format/namespace/client/operations/models/AddVirtualColumnEntry/#properties","title":"Properties","text":"Name Type Description Notes inputColumns List&lt;String&gt; List of input column names for the virtual column dataType Object Data type of the virtual column using JSON representation image String Docker image to use for the UDF udf String Base64 encoded pickled UDF udfName String Name of the UDF udfVersion String Version of the UDF"},{"location":"format/namespace/client/operations/models/AlterColumnsEntry/","title":"AlterColumnsEntry","text":""},{"location":"format/namespace/client/operations/models/AlterColumnsEntry/#properties","title":"Properties","text":"Name Type Description Notes path String Column path to alter dataType Object New data type for the column using JSON representation (optional) rename String New name for the column (optional) [optional] nullable Boolean Whether the column should be nullable (optional) [optional] virtualColumn AlterVirtualColumnEntry Virtual column alterations (optional) [optional]"},{"location":"format/namespace/client/operations/models/AlterTableAddColumnsRequest/","title":"AlterTableAddColumnsRequest","text":""},{"location":"format/namespace/client/operations/models/AlterTableAddColumnsRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] newColumns List&lt;NewColumnTransform&gt; List of new columns to add"},{"location":"format/namespace/client/operations/models/AlterTableAddColumnsResponse/","title":"AlterTableAddColumnsResponse","text":""},{"location":"format/namespace/client/operations/models/AlterTableAddColumnsResponse/#properties","title":"Properties","text":"Name Type Description Notes transactionId String Optional transaction identifier [optional] version Long Version of the table after adding columns"},{"location":"format/namespace/client/operations/models/AlterTableAlterColumnsRequest/","title":"AlterTableAlterColumnsRequest","text":""},{"location":"format/namespace/client/operations/models/AlterTableAlterColumnsRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] alterations List&lt;AlterColumnsEntry&gt; List of column alterations to perform"},{"location":"format/namespace/client/operations/models/AlterTableAlterColumnsResponse/","title":"AlterTableAlterColumnsResponse","text":""},{"location":"format/namespace/client/operations/models/AlterTableAlterColumnsResponse/#properties","title":"Properties","text":"Name Type Description Notes transactionId String Optional transaction identifier [optional] version Long Version of the table after altering columns"},{"location":"format/namespace/client/operations/models/AlterTableDropColumnsRequest/","title":"AlterTableDropColumnsRequest","text":""},{"location":"format/namespace/client/operations/models/AlterTableDropColumnsRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] columns List&lt;String&gt; Names of columns to drop"},{"location":"format/namespace/client/operations/models/AlterTableDropColumnsResponse/","title":"AlterTableDropColumnsResponse","text":""},{"location":"format/namespace/client/operations/models/AlterTableDropColumnsResponse/#properties","title":"Properties","text":"Name Type Description Notes transactionId String Optional transaction identifier [optional] version Long Version of the table after dropping columns"},{"location":"format/namespace/client/operations/models/AlterTransactionAction/","title":"AlterTransactionAction","text":"<p>A single action that could be performed to alter a transaction. This action holds the model definition for all types of specific actions models, this is to minimize difference and compatibility issue across codegen in different languages. When used, only one of the actions should be non-null for each action. If you would like to perform multiple actions, set a list of actions in the AlterTransactionRequest. </p>"},{"location":"format/namespace/client/operations/models/AlterTransactionAction/#properties","title":"Properties","text":"Name Type Description Notes setStatusAction AlterTransactionSetStatus [optional] setPropertyAction AlterTransactionSetProperty [optional] unsetPropertyAction AlterTransactionUnsetProperty [optional]"},{"location":"format/namespace/client/operations/models/AlterTransactionRequest/","title":"AlterTransactionRequest","text":"<p>Alter a transaction with a list of actions. The server should either succeed and apply all actions, or fail and apply no action. </p>"},{"location":"format/namespace/client/operations/models/AlterTransactionRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] actions List&lt;AlterTransactionAction&gt;"},{"location":"format/namespace/client/operations/models/AlterTransactionResponse/","title":"AlterTransactionResponse","text":""},{"location":"format/namespace/client/operations/models/AlterTransactionResponse/#properties","title":"Properties","text":"Name Type Description Notes status String The status of a transaction. Case insensitive, supports both PascalCase and snake_case. Valid values are: - Queued: the transaction is queued and not yet started - Running: the transaction is currently running - Succeeded: the transaction has completed successfully - Failed: the transaction has failed - Canceled: the transaction was canceled properties Map&lt;String, String&gt; [optional]"},{"location":"format/namespace/client/operations/models/AlterTransactionSetProperty/","title":"AlterTransactionSetProperty","text":""},{"location":"format/namespace/client/operations/models/AlterTransactionSetProperty/#properties","title":"Properties","text":"Name Type Description Notes key String [optional] value String [optional] mode String The behavior if the property key already exists. Case insensitive, supports both PascalCase and snake_case. Valid values are: - Overwrite (default): overwrite the existing value with the provided value - Fail: fail the entire operation - Skip: keep the existing value and skip setting the provided value [optional]"},{"location":"format/namespace/client/operations/models/AlterTransactionSetStatus/","title":"AlterTransactionSetStatus","text":""},{"location":"format/namespace/client/operations/models/AlterTransactionSetStatus/#properties","title":"Properties","text":"Name Type Description Notes status String The status of a transaction. Case insensitive, supports both PascalCase and snake_case. Valid values are: - Queued: the transaction is queued and not yet started - Running: the transaction is currently running - Succeeded: the transaction has completed successfully - Failed: the transaction has failed - Canceled: the transaction was canceled [optional]"},{"location":"format/namespace/client/operations/models/AlterTransactionUnsetProperty/","title":"AlterTransactionUnsetProperty","text":""},{"location":"format/namespace/client/operations/models/AlterTransactionUnsetProperty/#properties","title":"Properties","text":"Name Type Description Notes key String [optional] mode String The behavior if the property key to unset does not exist. Case insensitive, supports both PascalCase and snake_case. Valid values are: - Skip (default): skip the property to unset - Fail: fail the entire operation [optional]"},{"location":"format/namespace/client/operations/models/AlterVirtualColumnEntry/","title":"AlterVirtualColumnEntry","text":""},{"location":"format/namespace/client/operations/models/AlterVirtualColumnEntry/#properties","title":"Properties","text":"Name Type Description Notes inputColumns List&lt;String&gt; List of input column names for the virtual column (optional) [optional] image String Docker image to use for the UDF (optional) [optional] udf String Base64 encoded pickled UDF (optional) [optional] udfName String Name of the UDF (optional) [optional] udfVersion String Version of the UDF (optional) [optional]"},{"location":"format/namespace/client/operations/models/AnalyzeTableQueryPlanRequest/","title":"AnalyzeTableQueryPlanRequest","text":""},{"location":"format/namespace/client/operations/models/AnalyzeTableQueryPlanRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] bypassVectorIndex Boolean Whether to bypass vector index [optional] columns QueryTableRequestColumns [optional] distanceType String Distance metric to use [optional] ef Integer Search effort parameter for HNSW index [optional] fastSearch Boolean Whether to use fast search [optional] filter String Optional SQL filter expression [optional] fullTextQuery QueryTableRequestFullTextQuery [optional] k Integer Number of results to return lowerBound Float Lower bound for search [optional] nprobes Integer Number of probes for IVF index [optional] offset Integer Number of results to skip [optional] prefilter Boolean Whether to apply filtering before vector search [optional] refineFactor Integer Refine factor for search [optional] upperBound Float Upper bound for search [optional] vector QueryTableRequestVector vectorColumn String Name of the vector column to search [optional] version Long Table version to query [optional] withRowId Boolean If true, return the row id as a column called `_rowid` [optional]"},{"location":"format/namespace/client/operations/models/AnalyzeTableQueryPlanResponse/","title":"AnalyzeTableQueryPlanResponse","text":""},{"location":"format/namespace/client/operations/models/AnalyzeTableQueryPlanResponse/#properties","title":"Properties","text":"Name Type Description Notes analysis String Detailed analysis of the query execution plan"},{"location":"format/namespace/client/operations/models/BooleanQuery/","title":"BooleanQuery","text":"<p>Boolean query with must, should, and must_not clauses</p>"},{"location":"format/namespace/client/operations/models/BooleanQuery/#properties","title":"Properties","text":"Name Type Description Notes must List&lt;FtsQuery&gt; Queries that must match (AND) mustNot List&lt;FtsQuery&gt; Queries that must not match (NOT) should List&lt;FtsQuery&gt; Queries that should match (OR)"},{"location":"format/namespace/client/operations/models/BoostQuery/","title":"BoostQuery","text":"<p>Boost query that scores documents matching positive query higher and negative query lower</p>"},{"location":"format/namespace/client/operations/models/BoostQuery/#properties","title":"Properties","text":"Name Type Description Notes positive FtsQuery negative FtsQuery negativeBoost Float Boost factor for negative query (default: 0.5) [optional]"},{"location":"format/namespace/client/operations/models/CountTableRowsRequest/","title":"CountTableRowsRequest","text":""},{"location":"format/namespace/client/operations/models/CountTableRowsRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] version Long Version of the table to describe. If not specified, server should resolve it to the latest version. [optional] predicate String Optional SQL predicate to filter rows for counting [optional]"},{"location":"format/namespace/client/operations/models/CreateEmptyTableRequest/","title":"CreateEmptyTableRequest","text":"<p>Request for creating an empty table.  Deprecated: Use <code>DeclareTableRequest</code> instead. </p>"},{"location":"format/namespace/client/operations/models/CreateEmptyTableRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] location String Optional storage location for the table. If not provided, the namespace implementation should determine the table location. [optional] vendCredentials Boolean Whether to include vended credentials in the response `storage_options`. When true, the implementation should provide vended credentials for accessing storage. When not set, the implementation can decide whether to return vended credentials. [optional]"},{"location":"format/namespace/client/operations/models/CreateEmptyTableResponse/","title":"CreateEmptyTableResponse","text":"<p>Response for creating an empty table.  Deprecated: Use <code>DeclareTableResponse</code> instead. </p>"},{"location":"format/namespace/client/operations/models/CreateEmptyTableResponse/#properties","title":"Properties","text":"Name Type Description Notes transactionId String Optional transaction identifier [optional] location String [optional] storageOptions Map&lt;String, String&gt; Configuration options to be used to access storage. The available options depend on the type of storage in use. These will be passed directly to Lance to initialize storage access. [optional]"},{"location":"format/namespace/client/operations/models/CreateNamespaceRequest/","title":"CreateNamespaceRequest","text":""},{"location":"format/namespace/client/operations/models/CreateNamespaceRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] mode String There are three modes when trying to create a namespace, to differentiate the behavior when a namespace of the same name already exists. Case insensitive, supports both PascalCase and snake_case. Valid values are:   * Create: the operation fails with 409.   * ExistOk: the operation succeeds and the existing namespace is kept.   * Overwrite: the existing namespace is dropped and a new empty namespace with this name is created. [optional] properties Map&lt;String, String&gt; [optional]"},{"location":"format/namespace/client/operations/models/CreateNamespaceResponse/","title":"CreateNamespaceResponse","text":""},{"location":"format/namespace/client/operations/models/CreateNamespaceResponse/#properties","title":"Properties","text":"Name Type Description Notes transactionId String Optional transaction identifier [optional] properties Map&lt;String, String&gt; Properties after the namespace is created.  If the server does not support namespace properties, it should return null for this field. If namespace properties are supported, but none are set, it should return an empty object. [optional]"},{"location":"format/namespace/client/operations/models/CreateTableIndexRequest/","title":"CreateTableIndexRequest","text":""},{"location":"format/namespace/client/operations/models/CreateTableIndexRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] column String Name of the column to create index on indexType String Type of index to create (e.g., BTREE, BITMAP, LABEL_LIST, IVF_FLAT, IVF_PQ, IVF_HNSW_SQ, FTS) name String Optional name for the index. If not provided, a name will be auto-generated. [optional] distanceType String Distance metric type for vector indexes (e.g., l2, cosine, dot) [optional] withPosition Boolean Optional FTS parameter for position tracking [optional] baseTokenizer String Optional FTS parameter for base tokenizer [optional] language String Optional FTS parameter for language [optional] maxTokenLength Integer Optional FTS parameter for maximum token length [optional] lowerCase Boolean Optional FTS parameter for lowercase conversion [optional] stem Boolean Optional FTS parameter for stemming [optional] removeStopWords Boolean Optional FTS parameter for stop word removal [optional] asciiFolding Boolean Optional FTS parameter for ASCII folding [optional]"},{"location":"format/namespace/client/operations/models/CreateTableIndexResponse/","title":"CreateTableIndexResponse","text":"<p>Response for create index operation</p>"},{"location":"format/namespace/client/operations/models/CreateTableIndexResponse/#properties","title":"Properties","text":"Name Type Description Notes transactionId String Optional transaction identifier [optional]"},{"location":"format/namespace/client/operations/models/CreateTableRequest/","title":"CreateTableRequest","text":"<p>Request for creating a table, excluding the Arrow IPC stream. </p>"},{"location":"format/namespace/client/operations/models/CreateTableRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] mode String There are three modes when trying to create a table, to differentiate the behavior when a table of the same name already exists. Case insensitive, supports both PascalCase and snake_case. Valid values are:   * Create: the operation fails with 409.   * ExistOk: the operation succeeds and the existing table is kept.   * Overwrite: the existing table is dropped and a new table with this name is created. [optional]"},{"location":"format/namespace/client/operations/models/CreateTableResponse/","title":"CreateTableResponse","text":""},{"location":"format/namespace/client/operations/models/CreateTableResponse/#properties","title":"Properties","text":"Name Type Description Notes transactionId String Optional transaction identifier [optional] location String [optional] version Long [optional] storageOptions Map&lt;String, String&gt; Configuration options to be used to access storage. The available options depend on the type of storage in use. These will be passed directly to Lance to initialize storage access. [optional]"},{"location":"format/namespace/client/operations/models/CreateTableScalarIndexResponse/","title":"CreateTableScalarIndexResponse","text":"<p>Response for create scalar index operation</p>"},{"location":"format/namespace/client/operations/models/CreateTableScalarIndexResponse/#properties","title":"Properties","text":"Name Type Description Notes transactionId String Optional transaction identifier [optional]"},{"location":"format/namespace/client/operations/models/CreateTableTagRequest/","title":"CreateTableTagRequest","text":""},{"location":"format/namespace/client/operations/models/CreateTableTagRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] tag String Name of the tag to create version Long Version number for the tag to point to"},{"location":"format/namespace/client/operations/models/CreateTableTagResponse/","title":"CreateTableTagResponse","text":"<p>Response for create tag operation</p>"},{"location":"format/namespace/client/operations/models/CreateTableTagResponse/#properties","title":"Properties","text":"Name Type Description Notes transactionId String Optional transaction identifier [optional]"},{"location":"format/namespace/client/operations/models/DeclareTableRequest/","title":"DeclareTableRequest","text":"<p>Request for declaring a table. </p>"},{"location":"format/namespace/client/operations/models/DeclareTableRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] location String Optional storage location for the table. If not provided, the namespace implementation should determine the table location. [optional] vendCredentials Boolean Whether to include vended credentials in the response `storage_options`. When true, the implementation should provide vended credentials for accessing storage. When not set, the implementation can decide whether to return vended credentials. [optional]"},{"location":"format/namespace/client/operations/models/DeclareTableResponse/","title":"DeclareTableResponse","text":"<p>Response for declaring a table. </p>"},{"location":"format/namespace/client/operations/models/DeclareTableResponse/#properties","title":"Properties","text":"Name Type Description Notes transactionId String Optional transaction identifier [optional] location String [optional] storageOptions Map&lt;String, String&gt; Configuration options to be used to access storage. The available options depend on the type of storage in use. These will be passed directly to Lance to initialize storage access. [optional]"},{"location":"format/namespace/client/operations/models/DeleteFromTableRequest/","title":"DeleteFromTableRequest","text":"<p>Delete data from table based on a SQL predicate. Returns the number of rows that were deleted. </p>"},{"location":"format/namespace/client/operations/models/DeleteFromTableRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; The namespace identifier [optional] predicate String SQL predicate to filter rows for deletion"},{"location":"format/namespace/client/operations/models/DeleteFromTableResponse/","title":"DeleteFromTableResponse","text":""},{"location":"format/namespace/client/operations/models/DeleteFromTableResponse/#properties","title":"Properties","text":"Name Type Description Notes transactionId String Optional transaction identifier [optional] version Long The commit version associated with the operation [optional]"},{"location":"format/namespace/client/operations/models/DeleteTableTagRequest/","title":"DeleteTableTagRequest","text":""},{"location":"format/namespace/client/operations/models/DeleteTableTagRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] tag String Name of the tag to delete"},{"location":"format/namespace/client/operations/models/DeleteTableTagResponse/","title":"DeleteTableTagResponse","text":"<p>Response for delete tag operation</p>"},{"location":"format/namespace/client/operations/models/DeleteTableTagResponse/#properties","title":"Properties","text":"Name Type Description Notes transactionId String Optional transaction identifier [optional]"},{"location":"format/namespace/client/operations/models/DeregisterTableRequest/","title":"DeregisterTableRequest","text":"<p>The table content remains available in the storage. </p>"},{"location":"format/namespace/client/operations/models/DeregisterTableRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional]"},{"location":"format/namespace/client/operations/models/DeregisterTableResponse/","title":"DeregisterTableResponse","text":""},{"location":"format/namespace/client/operations/models/DeregisterTableResponse/#properties","title":"Properties","text":"Name Type Description Notes transactionId String Optional transaction identifier [optional] id List&lt;String&gt; [optional] location String [optional] properties Map&lt;String, String&gt; [optional]"},{"location":"format/namespace/client/operations/models/DescribeNamespaceRequest/","title":"DescribeNamespaceRequest","text":""},{"location":"format/namespace/client/operations/models/DescribeNamespaceRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional]"},{"location":"format/namespace/client/operations/models/DescribeNamespaceResponse/","title":"DescribeNamespaceResponse","text":""},{"location":"format/namespace/client/operations/models/DescribeNamespaceResponse/#properties","title":"Properties","text":"Name Type Description Notes properties Map&lt;String, String&gt; Properties stored on the namespace, if supported by the server. If the server does not support namespace properties, it should return null for this field. If namespace properties are supported, but none are set, it should return an empty object. [optional]"},{"location":"format/namespace/client/operations/models/DescribeTableIndexStatsRequest/","title":"DescribeTableIndexStatsRequest","text":""},{"location":"format/namespace/client/operations/models/DescribeTableIndexStatsRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] version Long Optional table version to get stats for [optional] indexName String Name of the index [optional]"},{"location":"format/namespace/client/operations/models/DescribeTableIndexStatsResponse/","title":"DescribeTableIndexStatsResponse","text":""},{"location":"format/namespace/client/operations/models/DescribeTableIndexStatsResponse/#properties","title":"Properties","text":"Name Type Description Notes distanceType String Distance type for vector indexes [optional] indexType String Type of the index [optional] numIndexedRows Long Number of indexed rows [optional] numUnindexedRows Long Number of unindexed rows [optional] numIndices Integer Number of indices [optional]"},{"location":"format/namespace/client/operations/models/DescribeTableRequest/","title":"DescribeTableRequest","text":""},{"location":"format/namespace/client/operations/models/DescribeTableRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] version Long Version of the table to describe. If not specified, server should resolve it to the latest version. [optional] withTableUri Boolean Whether to include the table URI in the response. Default is false. [optional] loadDetailedMetadata Boolean Whether to load detailed metadata that requires opening the dataset. When true, the response must include all detailed metadata such as `version`, `schema`, and `stats` which require reading the dataset. When not set, the implementation can decide whether to return detailed metadata and which parts of detailed metadata to return. [optional] vendCredentials Boolean Whether to include vended credentials in the response `storage_options`. When true, the implementation should provide vended credentials for accessing storage. When not set, the implementation can decide whether to return vended credentials. [optional]"},{"location":"format/namespace/client/operations/models/DescribeTableResponse/","title":"DescribeTableResponse","text":""},{"location":"format/namespace/client/operations/models/DescribeTableResponse/#properties","title":"Properties","text":"Name Type Description Notes table String Table name. Only populated when `load_detailed_metadata` is true. [optional] namespace List&lt;String&gt; The namespace identifier as a list of parts. Only populated when `load_detailed_metadata` is true. [optional] version Long Table version number. Only populated when `load_detailed_metadata` is true. [optional] location String Table storage location (e.g., S3/GCS path). [optional] tableUri String Table URI. Unlike location, this field must be a complete and valid URI. Only returned when `with_table_uri` is true. [optional] schema JsonArrowSchema Table schema in JSON Arrow format. Only populated when `load_detailed_metadata` is true. [optional] storageOptions Map&lt;String, String&gt; Configuration options to be used to access storage. The available options depend on the type of storage in use. These will be passed directly to Lance to initialize storage access. When `vend_credentials` is true, this field may include vended credentials. If the vended credentials are temporary, the `expires_at_millis` key should be included to indicate the millisecond timestamp when the credentials expire. [optional] stats TableBasicStats Table statistics. Only populated when `load_detailed_metadata` is true. [optional] metadata Map&lt;String, String&gt; Optional table metadata as key-value pairs. [optional]"},{"location":"format/namespace/client/operations/models/DescribeTransactionRequest/","title":"DescribeTransactionRequest","text":""},{"location":"format/namespace/client/operations/models/DescribeTransactionRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional]"},{"location":"format/namespace/client/operations/models/DescribeTransactionResponse/","title":"DescribeTransactionResponse","text":""},{"location":"format/namespace/client/operations/models/DescribeTransactionResponse/#properties","title":"Properties","text":"Name Type Description Notes status String The status of a transaction. Case insensitive, supports both PascalCase and snake_case. Valid values are: - Queued: the transaction is queued and not yet started - Running: the transaction is currently running - Succeeded: the transaction has completed successfully - Failed: the transaction has failed - Canceled: the transaction was canceled properties Map&lt;String, String&gt; [optional]"},{"location":"format/namespace/client/operations/models/DropNamespaceRequest/","title":"DropNamespaceRequest","text":""},{"location":"format/namespace/client/operations/models/DropNamespaceRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] mode String The mode for dropping a namespace, deciding the server behavior when the namespace to drop is not found. Case insensitive, supports both PascalCase and snake_case. Valid values are: - Fail (default): the server must return 400 indicating the namespace to drop does not exist. - Skip: the server must return 204 indicating the drop operation has succeeded. [optional] behavior String The behavior for dropping a namespace. Case insensitive, supports both PascalCase and snake_case. Valid values are: - Restrict (default): the namespace should not contain any table or child namespace when drop is initiated.     If tables are found, the server should return error and not drop the namespace. - Cascade: all tables and child namespaces in the namespace are dropped before the namespace is dropped. [optional]"},{"location":"format/namespace/client/operations/models/DropNamespaceResponse/","title":"DropNamespaceResponse","text":""},{"location":"format/namespace/client/operations/models/DropNamespaceResponse/#properties","title":"Properties","text":"Name Type Description Notes properties Map&lt;String, String&gt; [optional] transactionId List&lt;String&gt; If present, indicating the operation is long running and should be tracked using DescribeTransaction [optional]"},{"location":"format/namespace/client/operations/models/DropTableIndexRequest/","title":"DropTableIndexRequest","text":""},{"location":"format/namespace/client/operations/models/DropTableIndexRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] indexName String Name of the index to drop [optional]"},{"location":"format/namespace/client/operations/models/DropTableIndexResponse/","title":"DropTableIndexResponse","text":"<p>Response for drop index operation</p>"},{"location":"format/namespace/client/operations/models/DropTableIndexResponse/#properties","title":"Properties","text":"Name Type Description Notes transactionId String Optional transaction identifier [optional]"},{"location":"format/namespace/client/operations/models/DropTableRequest/","title":"DropTableRequest","text":"<p>If the table and its data can be immediately deleted, return information of the deleted table. Otherwise, return a transaction ID that client can use to track deletion progress. </p>"},{"location":"format/namespace/client/operations/models/DropTableRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional]"},{"location":"format/namespace/client/operations/models/DropTableResponse/","title":"DropTableResponse","text":""},{"location":"format/namespace/client/operations/models/DropTableResponse/#properties","title":"Properties","text":"Name Type Description Notes transactionId String Optional transaction identifier [optional] id List&lt;String&gt; [optional] location String [optional] properties Map&lt;String, String&gt; [optional]"},{"location":"format/namespace/client/operations/models/ErrorResponse/","title":"ErrorResponse","text":"<p>Common JSON error response model</p>"},{"location":"format/namespace/client/operations/models/ErrorResponse/#properties","title":"Properties","text":"Name Type Description Notes error String A brief, human-readable message about the error. [optional] code Integer Lance Namespace error code identifying the error type.  Error codes:   0 - Unsupported: Operation not supported by this backend   1 - NamespaceNotFound: The specified namespace does not exist   2 - NamespaceAlreadyExists: A namespace with this name already exists   3 - NamespaceNotEmpty: Namespace contains tables or child namespaces   4 - TableNotFound: The specified table does not exist   5 - TableAlreadyExists: A table with this name already exists   6 - TableIndexNotFound: The specified table index does not exist   7 - TableIndexAlreadyExists: A table index with this name already exists   8 - TableTagNotFound: The specified table tag does not exist   9 - TableTagAlreadyExists: A table tag with this name already exists   10 - TransactionNotFound: The specified transaction does not exist   11 - TableVersionNotFound: The specified table version does not exist   12 - TableColumnNotFound: The specified table column does not exist   13 - InvalidInput: Malformed request or invalid parameters   14 - ConcurrentModification: Optimistic concurrency conflict   15 - PermissionDenied: User lacks permission for this operation   16 - Unauthenticated: Authentication credentials are missing or invalid   17 - ServiceUnavailable: Service is temporarily unavailable   18 - Internal: Unexpected server/implementation error   19 - InvalidTableState: Table is in an invalid state for the operation   20 - TableSchemaValidationError: Table schema validation failed detail String An optional human-readable explanation of the error. This can be used to record additional information such as stack trace. [optional] instance String A string that identifies the specific occurrence of the error. This can be a URI, a request or response ID, or anything that the implementation can recognize to trace specific occurrence of the error. [optional]"},{"location":"format/namespace/client/operations/models/ExplainTableQueryPlanRequest/","title":"ExplainTableQueryPlanRequest","text":""},{"location":"format/namespace/client/operations/models/ExplainTableQueryPlanRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] query QueryTableRequest verbose Boolean Whether to return verbose explanation [optional]"},{"location":"format/namespace/client/operations/models/ExplainTableQueryPlanResponse/","title":"ExplainTableQueryPlanResponse","text":""},{"location":"format/namespace/client/operations/models/ExplainTableQueryPlanResponse/#properties","title":"Properties","text":"Name Type Description Notes plan String Human-readable query execution plan"},{"location":"format/namespace/client/operations/models/FragmentStats/","title":"FragmentStats","text":""},{"location":"format/namespace/client/operations/models/FragmentStats/#properties","title":"Properties","text":"Name Type Description Notes numFragments Long The number of fragments in the table numSmallFragments Long The number of uncompacted fragments in the table lengths FragmentSummary Statistics on the number of rows in the table fragments"},{"location":"format/namespace/client/operations/models/FragmentSummary/","title":"FragmentSummary","text":""},{"location":"format/namespace/client/operations/models/FragmentSummary/#properties","title":"Properties","text":"Name Type Description Notes min Long max Long mean Long p25 Long p50 Long p75 Long p99 Long"},{"location":"format/namespace/client/operations/models/FtsQuery/","title":"FtsQuery","text":"<p>Full-text search query. Exactly one query type field must be provided. This structure follows the same pattern as AlterTransactionAction to minimize differences and compatibility issues across codegen in different languages. </p>"},{"location":"format/namespace/client/operations/models/FtsQuery/#properties","title":"Properties","text":"Name Type Description Notes match MatchQuery [optional] phrase PhraseQuery [optional] boost BoostQuery [optional] multiMatch MultiMatchQuery [optional] _boolean BooleanQuery [optional]"},{"location":"format/namespace/client/operations/models/GetTableStatsRequest/","title":"GetTableStatsRequest","text":""},{"location":"format/namespace/client/operations/models/GetTableStatsRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional]"},{"location":"format/namespace/client/operations/models/GetTableStatsResponse/","title":"GetTableStatsResponse","text":""},{"location":"format/namespace/client/operations/models/GetTableStatsResponse/#properties","title":"Properties","text":"Name Type Description Notes totalBytes Long The total number of bytes in the table numRows Long The number of rows in the table numIndices Long The number of indices in the table fragmentStats FragmentStats Statistics on table fragments"},{"location":"format/namespace/client/operations/models/GetTableTagVersionRequest/","title":"GetTableTagVersionRequest","text":""},{"location":"format/namespace/client/operations/models/GetTableTagVersionRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] tag String Name of the tag to get version for"},{"location":"format/namespace/client/operations/models/GetTableTagVersionResponse/","title":"GetTableTagVersionResponse","text":""},{"location":"format/namespace/client/operations/models/GetTableTagVersionResponse/#properties","title":"Properties","text":"Name Type Description Notes version Long version number that the tag points to"},{"location":"format/namespace/client/operations/models/Identity/","title":"Identity","text":"<p>Identity information of a request. </p>"},{"location":"format/namespace/client/operations/models/Identity/#properties","title":"Properties","text":"Name Type Description Notes apiKey String API key for authentication.  REST NAMESPACE ONLY This is passed via the `x-api-key` header. [optional] authToken String Bearer token for authentication.  REST NAMESPACE ONLY This is passed via the `Authorization` header with the Bearer scheme (e.g., `Bearer &lt;token&gt;`). [optional]"},{"location":"format/namespace/client/operations/models/IndexContent/","title":"IndexContent","text":""},{"location":"format/namespace/client/operations/models/IndexContent/#properties","title":"Properties","text":"Name Type Description Notes indexName String Name of the index indexUuid String Unique identifier for the index columns List&lt;String&gt; Columns covered by this index status String Current status of the index"},{"location":"format/namespace/client/operations/models/InsertIntoTableRequest/","title":"InsertIntoTableRequest","text":"<p>Request for inserting records into a table, excluding the Arrow IPC stream. </p>"},{"location":"format/namespace/client/operations/models/InsertIntoTableRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] mode String How the insert should behave. Case insensitive, supports both PascalCase and snake_case. Valid values are: - Append (default): insert data to the existing table - Overwrite: remove all data in the table and then insert data to it [optional]"},{"location":"format/namespace/client/operations/models/InsertIntoTableResponse/","title":"InsertIntoTableResponse","text":"<p>Response from inserting records into a table</p>"},{"location":"format/namespace/client/operations/models/InsertIntoTableResponse/#properties","title":"Properties","text":"Name Type Description Notes transactionId String Optional transaction identifier [optional]"},{"location":"format/namespace/client/operations/models/JsonArrowDataType/","title":"JsonArrowDataType","text":"<p>JSON representation of an Apache Arrow DataType</p>"},{"location":"format/namespace/client/operations/models/JsonArrowDataType/#properties","title":"Properties","text":"Name Type Description Notes fields List&lt;JsonArrowField&gt; Fields for complex types like Struct, Union, etc. [optional] length Long Length for fixed-size types [optional] type String The data type name"},{"location":"format/namespace/client/operations/models/JsonArrowField/","title":"JsonArrowField","text":"<p>JSON representation of an Apache Arrow field. </p>"},{"location":"format/namespace/client/operations/models/JsonArrowField/#properties","title":"Properties","text":"Name Type Description Notes metadata Map&lt;String, String&gt; [optional] name String nullable Boolean type JsonArrowDataType"},{"location":"format/namespace/client/operations/models/JsonArrowSchema/","title":"JsonArrowSchema","text":"<p>JSON representation of a Apache Arrow schema. </p>"},{"location":"format/namespace/client/operations/models/JsonArrowSchema/#properties","title":"Properties","text":"Name Type Description Notes fields List&lt;JsonArrowField&gt; metadata Map&lt;String, String&gt; [optional]"},{"location":"format/namespace/client/operations/models/ListNamespacesRequest/","title":"ListNamespacesRequest","text":""},{"location":"format/namespace/client/operations/models/ListNamespacesRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] pageToken String An opaque token that allows pagination for list operations (e.g. ListNamespaces).  For an initial request of a list operation,  if the implementation cannot return all items in one response, or if there are more items than the page limit specified in the request, the implementation must return a page token in the response, indicating there are more results available.  After the initial request,  the value of the page token from each response must be used as the page token value for the next request.  Caller must interpret either `null`,  missing value or empty string value of the page token from the implementation's response as the end of the listing results. [optional] limit Integer An inclusive upper bound of the  number of results that a caller will receive. [optional]"},{"location":"format/namespace/client/operations/models/ListNamespacesResponse/","title":"ListNamespacesResponse","text":""},{"location":"format/namespace/client/operations/models/ListNamespacesResponse/#properties","title":"Properties","text":"Name Type Description Notes namespaces Set&lt;String&gt; The list of names of the child namespaces relative to the parent namespace `id` in the request. pageToken String An opaque token that allows pagination for list operations (e.g. ListNamespaces).  For an initial request of a list operation,  if the implementation cannot return all items in one response, or if there are more items than the page limit specified in the request, the implementation must return a page token in the response, indicating there are more results available.  After the initial request,  the value of the page token from each response must be used as the page token value for the next request.  Caller must interpret either `null`,  missing value or empty string value of the page token from the implementation's response as the end of the listing results. [optional]"},{"location":"format/namespace/client/operations/models/ListTableIndicesRequest/","title":"ListTableIndicesRequest","text":""},{"location":"format/namespace/client/operations/models/ListTableIndicesRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; The namespace identifier [optional] version Long Optional table version to list indexes from [optional] pageToken String An opaque token that allows pagination for list operations (e.g. ListNamespaces).  For an initial request of a list operation,  if the implementation cannot return all items in one response, or if there are more items than the page limit specified in the request, the implementation must return a page token in the response, indicating there are more results available.  After the initial request,  the value of the page token from each response must be used as the page token value for the next request.  Caller must interpret either `null`,  missing value or empty string value of the page token from the implementation's response as the end of the listing results. [optional] limit Integer An inclusive upper bound of the  number of results that a caller will receive. [optional]"},{"location":"format/namespace/client/operations/models/ListTableIndicesResponse/","title":"ListTableIndicesResponse","text":""},{"location":"format/namespace/client/operations/models/ListTableIndicesResponse/#properties","title":"Properties","text":"Name Type Description Notes indexes List&lt;IndexContent&gt; List of indexes on the table pageToken String An opaque token that allows pagination for list operations (e.g. ListNamespaces).  For an initial request of a list operation,  if the implementation cannot return all items in one response, or if there are more items than the page limit specified in the request, the implementation must return a page token in the response, indicating there are more results available.  After the initial request,  the value of the page token from each response must be used as the page token value for the next request.  Caller must interpret either `null`,  missing value or empty string value of the page token from the implementation's response as the end of the listing results. [optional]"},{"location":"format/namespace/client/operations/models/ListTableTagsRequest/","title":"ListTableTagsRequest","text":""},{"location":"format/namespace/client/operations/models/ListTableTagsRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; The table identifier [optional] pageToken String An opaque token that allows pagination for list operations (e.g. ListNamespaces).  For an initial request of a list operation,  if the implementation cannot return all items in one response, or if there are more items than the page limit specified in the request, the implementation must return a page token in the response, indicating there are more results available.  After the initial request,  the value of the page token from each response must be used as the page token value for the next request.  Caller must interpret either `null`,  missing value or empty string value of the page token from the implementation's response as the end of the listing results. [optional] limit Integer An inclusive upper bound of the  number of results that a caller will receive. [optional]"},{"location":"format/namespace/client/operations/models/ListTableTagsResponse/","title":"ListTableTagsResponse","text":"<p>Response containing table tags</p>"},{"location":"format/namespace/client/operations/models/ListTableTagsResponse/#properties","title":"Properties","text":"Name Type Description Notes tags Map&lt;String, TagContents&gt; Map of tag names to their contents pageToken String An opaque token that allows pagination for list operations (e.g. ListNamespaces).  For an initial request of a list operation,  if the implementation cannot return all items in one response, or if there are more items than the page limit specified in the request, the implementation must return a page token in the response, indicating there are more results available.  After the initial request,  the value of the page token from each response must be used as the page token value for the next request.  Caller must interpret either `null`,  missing value or empty string value of the page token from the implementation's response as the end of the listing results. [optional]"},{"location":"format/namespace/client/operations/models/ListTableVersionsRequest/","title":"ListTableVersionsRequest","text":""},{"location":"format/namespace/client/operations/models/ListTableVersionsRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] pageToken String An opaque token that allows pagination for list operations (e.g. ListNamespaces).  For an initial request of a list operation,  if the implementation cannot return all items in one response, or if there are more items than the page limit specified in the request, the implementation must return a page token in the response, indicating there are more results available.  After the initial request,  the value of the page token from each response must be used as the page token value for the next request.  Caller must interpret either `null`,  missing value or empty string value of the page token from the implementation's response as the end of the listing results. [optional] limit Integer An inclusive upper bound of the  number of results that a caller will receive. [optional]"},{"location":"format/namespace/client/operations/models/ListTableVersionsResponse/","title":"ListTableVersionsResponse","text":""},{"location":"format/namespace/client/operations/models/ListTableVersionsResponse/#properties","title":"Properties","text":"Name Type Description Notes versions List&lt;TableVersion&gt; List of table versions pageToken String An opaque token that allows pagination for list operations (e.g. ListNamespaces).  For an initial request of a list operation,  if the implementation cannot return all items in one response, or if there are more items than the page limit specified in the request, the implementation must return a page token in the response, indicating there are more results available.  After the initial request,  the value of the page token from each response must be used as the page token value for the next request.  Caller must interpret either `null`,  missing value or empty string value of the page token from the implementation's response as the end of the listing results. [optional]"},{"location":"format/namespace/client/operations/models/ListTablesRequest/","title":"ListTablesRequest","text":""},{"location":"format/namespace/client/operations/models/ListTablesRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] pageToken String An opaque token that allows pagination for list operations (e.g. ListNamespaces).  For an initial request of a list operation,  if the implementation cannot return all items in one response, or if there are more items than the page limit specified in the request, the implementation must return a page token in the response, indicating there are more results available.  After the initial request,  the value of the page token from each response must be used as the page token value for the next request.  Caller must interpret either `null`,  missing value or empty string value of the page token from the implementation's response as the end of the listing results. [optional] limit Integer An inclusive upper bound of the  number of results that a caller will receive. [optional]"},{"location":"format/namespace/client/operations/models/ListTablesResponse/","title":"ListTablesResponse","text":""},{"location":"format/namespace/client/operations/models/ListTablesResponse/#properties","title":"Properties","text":"Name Type Description Notes tables Set&lt;String&gt; The list of names of all the tables under the connected namespace implementation. This should recursively list all the tables in all child namespaces. Each string in the list is the full identifier in string form. pageToken String An opaque token that allows pagination for list operations (e.g. ListNamespaces).  For an initial request of a list operation,  if the implementation cannot return all items in one response, or if there are more items than the page limit specified in the request, the implementation must return a page token in the response, indicating there are more results available.  After the initial request,  the value of the page token from each response must be used as the page token value for the next request.  Caller must interpret either `null`,  missing value or empty string value of the page token from the implementation's response as the end of the listing results. [optional]"},{"location":"format/namespace/client/operations/models/MatchQuery/","title":"MatchQuery","text":""},{"location":"format/namespace/client/operations/models/MatchQuery/#properties","title":"Properties","text":"Name Type Description Notes boost Float [optional] column String [optional] fuzziness Integer [optional] maxExpansions Integer The maximum number of terms to expand for fuzzy matching. Default to 50. [optional] operator String The operator to use for combining terms. Case insensitive, supports both PascalCase and snake_case. Valid values are: - And: All terms must match. - Or: At least one term must match. [optional] prefixLength Integer The number of beginning characters being unchanged for fuzzy matching. Default to 0. [optional] terms String"},{"location":"format/namespace/client/operations/models/MergeInsertIntoTableRequest/","title":"MergeInsertIntoTableRequest","text":"<p>Request for merging or inserting records into a table, excluding the Arrow IPC stream. </p>"},{"location":"format/namespace/client/operations/models/MergeInsertIntoTableRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] on String Column name to use for matching rows (required) [optional] whenMatchedUpdateAll Boolean Update all columns when rows match [optional] whenMatchedUpdateAllFilt String The row is updated (similar to UpdateAll) only for rows where the SQL expression evaluates to true [optional] whenNotMatchedInsertAll Boolean Insert all columns when rows don't match [optional] whenNotMatchedBySourceDelete Boolean Delete all rows from target table that don't match a row in the source table [optional] whenNotMatchedBySourceDeleteFilt String Delete rows from the target table if there is no match AND the SQL expression evaluates to true [optional] timeout String Timeout for the operation (e.g., \\\"30s\\\", \\\"5m\\\") [optional] useIndex Boolean Whether to use index for matching rows [optional]"},{"location":"format/namespace/client/operations/models/MergeInsertIntoTableResponse/","title":"MergeInsertIntoTableResponse","text":"<p>Response from merge insert operation</p>"},{"location":"format/namespace/client/operations/models/MergeInsertIntoTableResponse/#properties","title":"Properties","text":"Name Type Description Notes transactionId String Optional transaction identifier [optional] numUpdatedRows Long Number of rows updated [optional] numInsertedRows Long Number of rows inserted [optional] numDeletedRows Long Number of rows deleted (typically 0 for merge insert) [optional] version Long The commit version associated with the operation [optional]"},{"location":"format/namespace/client/operations/models/MultiMatchQuery/","title":"MultiMatchQuery","text":""},{"location":"format/namespace/client/operations/models/MultiMatchQuery/#properties","title":"Properties","text":"Name Type Description Notes matchQueries List&lt;MatchQuery&gt;"},{"location":"format/namespace/client/operations/models/NamespaceExistsRequest/","title":"NamespaceExistsRequest","text":""},{"location":"format/namespace/client/operations/models/NamespaceExistsRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional]"},{"location":"format/namespace/client/operations/models/NewColumnTransform/","title":"NewColumnTransform","text":""},{"location":"format/namespace/client/operations/models/NewColumnTransform/#properties","title":"Properties","text":"Name Type Description Notes name String Name of the new column expression String SQL expression to compute the column value (optional if virtual_column is specified) [optional] virtualColumn AddVirtualColumnEntry Virtual column definition (optional if expression is specified) [optional]"},{"location":"format/namespace/client/operations/models/PhraseQuery/","title":"PhraseQuery","text":""},{"location":"format/namespace/client/operations/models/PhraseQuery/#properties","title":"Properties","text":"Name Type Description Notes column String [optional] slop Integer [optional] terms String"},{"location":"format/namespace/client/operations/models/QueryTableRequest/","title":"QueryTableRequest","text":""},{"location":"format/namespace/client/operations/models/QueryTableRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] bypassVectorIndex Boolean Whether to bypass vector index [optional] columns QueryTableRequestColumns [optional] distanceType String Distance metric to use [optional] ef Integer Search effort parameter for HNSW index [optional] fastSearch Boolean Whether to use fast search [optional] filter String Optional SQL filter expression [optional] fullTextQuery QueryTableRequestFullTextQuery [optional] k Integer Number of results to return lowerBound Float Lower bound for search [optional] nprobes Integer Number of probes for IVF index [optional] offset Integer Number of results to skip [optional] prefilter Boolean Whether to apply filtering before vector search [optional] refineFactor Integer Refine factor for search [optional] upperBound Float Upper bound for search [optional] vector QueryTableRequestVector vectorColumn String Name of the vector column to search [optional] version Long Table version to query [optional] withRowId Boolean If true, return the row id as a column called `_rowid` [optional]"},{"location":"format/namespace/client/operations/models/QueryTableRequestColumns/","title":"QueryTableRequestColumns","text":"<p>Optional columns to return. Provide either column_names or column_aliases, not both. </p>"},{"location":"format/namespace/client/operations/models/QueryTableRequestColumns/#properties","title":"Properties","text":"Name Type Description Notes columnNames List&lt;String&gt; List of column names to return [optional] columnAliases Map&lt;String, String&gt; Object mapping output aliases to source column names [optional]"},{"location":"format/namespace/client/operations/models/QueryTableRequestFullTextQuery/","title":"QueryTableRequestFullTextQuery","text":"<p>Optional full-text search query. Provide either string_query or structured_query, not both.</p>"},{"location":"format/namespace/client/operations/models/QueryTableRequestFullTextQuery/#properties","title":"Properties","text":"Name Type Description Notes stringQuery StringFtsQuery [optional] structuredQuery StructuredFtsQuery [optional]"},{"location":"format/namespace/client/operations/models/QueryTableRequestVector/","title":"QueryTableRequestVector","text":"<p>Query vector(s) for similarity search. Provide either single_vector or multi_vector, not both.</p>"},{"location":"format/namespace/client/operations/models/QueryTableRequestVector/#properties","title":"Properties","text":"Name Type Description Notes singleVector List&lt;Float&gt; Single query vector [optional] multiVector List&lt;List&lt;Float&gt;&gt; Multiple query vectors for batch search [optional]"},{"location":"format/namespace/client/operations/models/RegisterTableRequest/","title":"RegisterTableRequest","text":""},{"location":"format/namespace/client/operations/models/RegisterTableRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] location String mode String There are two modes when trying to register a table, to differentiate the behavior when a table of the same name already exists. Case insensitive, supports both PascalCase and snake_case. Valid values are:   * Create (default): the operation fails with 409.   * Overwrite: the existing table registration is replaced with the new registration. [optional] properties Map&lt;String, String&gt; [optional]"},{"location":"format/namespace/client/operations/models/RegisterTableResponse/","title":"RegisterTableResponse","text":""},{"location":"format/namespace/client/operations/models/RegisterTableResponse/#properties","title":"Properties","text":"Name Type Description Notes transactionId String Optional transaction identifier [optional] location String [optional] properties Map&lt;String, String&gt; [optional]"},{"location":"format/namespace/client/operations/models/RenameTableRequest/","title":"RenameTableRequest","text":""},{"location":"format/namespace/client/operations/models/RenameTableRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; The table identifier [optional] newTableName String New name for the table newNamespaceId List&lt;String&gt; New namespace identifier to move the table to (optional, if not specified the table stays in the same namespace) [optional]"},{"location":"format/namespace/client/operations/models/RenameTableResponse/","title":"RenameTableResponse","text":""},{"location":"format/namespace/client/operations/models/RenameTableResponse/#properties","title":"Properties","text":"Name Type Description Notes transactionId String Optional transaction identifier [optional]"},{"location":"format/namespace/client/operations/models/RestoreTableRequest/","title":"RestoreTableRequest","text":""},{"location":"format/namespace/client/operations/models/RestoreTableRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] version Long Version to restore to"},{"location":"format/namespace/client/operations/models/RestoreTableResponse/","title":"RestoreTableResponse","text":"<p>Response for restore table operation</p>"},{"location":"format/namespace/client/operations/models/RestoreTableResponse/#properties","title":"Properties","text":"Name Type Description Notes transactionId String Optional transaction identifier [optional]"},{"location":"format/namespace/client/operations/models/StringFtsQuery/","title":"StringFtsQuery","text":""},{"location":"format/namespace/client/operations/models/StringFtsQuery/#properties","title":"Properties","text":"Name Type Description Notes columns List&lt;String&gt; [optional] query String"},{"location":"format/namespace/client/operations/models/StructuredFtsQuery/","title":"StructuredFtsQuery","text":""},{"location":"format/namespace/client/operations/models/StructuredFtsQuery/#properties","title":"Properties","text":"Name Type Description Notes query FtsQuery"},{"location":"format/namespace/client/operations/models/TableBasicStats/","title":"TableBasicStats","text":""},{"location":"format/namespace/client/operations/models/TableBasicStats/#properties","title":"Properties","text":"Name Type Description Notes numDeletedRows Integer Number of deleted rows in the table numFragments Integer Number of fragments in the table"},{"location":"format/namespace/client/operations/models/TableExistsRequest/","title":"TableExistsRequest","text":""},{"location":"format/namespace/client/operations/models/TableExistsRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] version Long Version of the table to check existence. If not specified, server should resolve it to the latest version. [optional]"},{"location":"format/namespace/client/operations/models/TableVersion/","title":"TableVersion","text":""},{"location":"format/namespace/client/operations/models/TableVersion/#properties","title":"Properties","text":"Name Type Description Notes version Long Version number timestamp OffsetDateTime Timestamp when the version was created metadata Map&lt;String, String&gt; Key-value pairs of metadata"},{"location":"format/namespace/client/operations/models/TagContents/","title":"TagContents","text":""},{"location":"format/namespace/client/operations/models/TagContents/#properties","title":"Properties","text":"Name Type Description Notes branch String Branch name that the tag was created on (if any) [optional] version Long Version number that the tag points to manifestSize Long Size of the manifest file in bytes"},{"location":"format/namespace/client/operations/models/UpdateTableRequest/","title":"UpdateTableRequest","text":"<p>Each update consists of a column name and an SQL expression that will be evaluated against the current row's value. Optionally, a predicate can be provided to filter which rows to update. </p>"},{"location":"format/namespace/client/operations/models/UpdateTableRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] predicate String Optional SQL predicate to filter rows for update [optional] updates List&lt;List&lt;String&gt;&gt; List of column updates as [column_name, expression] pairs"},{"location":"format/namespace/client/operations/models/UpdateTableResponse/","title":"UpdateTableResponse","text":""},{"location":"format/namespace/client/operations/models/UpdateTableResponse/#properties","title":"Properties","text":"Name Type Description Notes transactionId String Optional transaction identifier [optional] updatedRows Long Number of rows updated version Long The commit version associated with the operation"},{"location":"format/namespace/client/operations/models/UpdateTableSchemaMetadataRequest/","title":"UpdateTableSchemaMetadataRequest","text":""},{"location":"format/namespace/client/operations/models/UpdateTableSchemaMetadataRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; The table identifier [optional] metadata Map&lt;String, String&gt; Schema metadata key-value pairs to set [optional]"},{"location":"format/namespace/client/operations/models/UpdateTableSchemaMetadataResponse/","title":"UpdateTableSchemaMetadataResponse","text":""},{"location":"format/namespace/client/operations/models/UpdateTableSchemaMetadataResponse/#properties","title":"Properties","text":"Name Type Description Notes metadata Map&lt;String, String&gt; The updated schema metadata [optional] transactionId String Optional transaction identifier [optional]"},{"location":"format/namespace/client/operations/models/UpdateTableTagRequest/","title":"UpdateTableTagRequest","text":""},{"location":"format/namespace/client/operations/models/UpdateTableTagRequest/#properties","title":"Properties","text":"Name Type Description Notes identity Identity [optional] context Map&lt;String, String&gt; Arbitrary context for a request as key-value pairs. How to use the context is custom to the specific implementation.  REST NAMESPACE ONLY Context entries are passed via HTTP headers using the naming convention `x-lance-ctx-&lt;key&gt;: &lt;value&gt;`. For example, a context entry `{\\\"trace_id\\\": \\\"abc123\\\"}` would be sent as the header `x-lance-ctx-trace_id: abc123`. [optional] id List&lt;String&gt; [optional] tag String Name of the tag to update version Long New version number for the tag to point to"},{"location":"format/namespace/client/operations/models/UpdateTableTagResponse/","title":"UpdateTableTagResponse","text":"<p>Response for update tag operation</p>"},{"location":"format/namespace/client/operations/models/UpdateTableTagResponse/#properties","title":"Properties","text":"Name Type Description Notes transactionId String Optional transaction identifier [optional]"},{"location":"format/namespace/dir/catalog-spec/","title":"Lance Directory Namespace Catalog Spec","text":"<p>Lance directory namespace is a catalog that stores tables in a directory structure on any local or remote storage system. It has gone through 2 major spec versions so far:</p> <ul> <li>V1 (Directory Listing): A lightweight, simple 1-level namespace that discovers tables by scanning the directory.</li> <li>V2 (Manifest): A more advanced implementation backed by a manifest table (a Lance table) that supports nested namespaces and better performance at scale.</li> </ul>"},{"location":"format/namespace/dir/catalog-spec/#v1-directory-listing","title":"V1: Directory Listing","text":"<p>V1 is a simple 1-level namespace where each table corresponds to a subdirectory with the format <code>&lt;table_name&gt;.lance</code>. This mode is ideal for getting started quickly with Lance tables.</p>"},{"location":"format/namespace/dir/catalog-spec/#directory-layout","title":"Directory Layout","text":"<p>A directory namespace maps to a directory on storage, called the namespace directory. A Lance table corresponds to a subdirectory in the namespace directory that has the format <code>&lt;table_name&gt;.lance</code>, called a table directory.</p> <p>Consider the following example namespace directory layout:</p> <pre><code>.\n\u2514\u2500\u2500 /my/dir1/\n    \u251c\u2500\u2500 table1.lance/\n    \u2502   \u251c\u2500\u2500 data/\n    \u2502   \u2502   \u251c\u2500\u2500 0aa36d91-8293-406b-958c-faf9e7547938.lance\n    \u2502   \u2502   \u2514\u2500\u2500 ed7af55d-b064-4442-bcb5-47b524e98d0e.lance\n    \u2502   \u251c\u2500\u2500 _versions/\n    \u2502   \u2502   \u2514\u2500\u2500 9223372036854775707.manifest\n    \u2502   \u2514\u2500\u2500 _indices/\n    \u2502       \u2514\u2500\u2500 85814508-ed9a-41f2-b939-2050bb7a0ed5-fts/\n    \u2502           \u2514\u2500\u2500 index.idx\n    \u251c\u2500\u2500 table2.lance/\n    \u251c\u2500\u2500 table3.lance/\n    \u2502   \u2514\u2500\u2500 .lance-deregistered      # Marker: table3 is deregistered\n    \u2514\u2500\u2500 table4.lance/\n        \u2514\u2500\u2500 .lance-reserved          # Marker: table4 is reserved but not created\n</code></pre> <p>This describes a Lance directory namespace with the namespace directory at <code>/my/dir1/</code>. It contains active tables <code>table1</code> and <code>table2</code> at table directories <code>/my/dir1/table1.lance</code> and <code>/my/dir1/table2.lance</code>. Table <code>table3</code> exists on storage but is deregistered (excluded from table listings). Table <code>table4</code> is reserved but not yet created with data.</p>"},{"location":"format/namespace/dir/catalog-spec/#table-existence","title":"Table Existence","text":"<p>In V1, a table exists in a Lance directory namespace if a table directory of the specific name exists and the table is not marked as deregistered. In object store terms, this means the prefix <code>&lt;table_name&gt;.lance/</code> has at least one file in it and the file <code>&lt;table_name&gt;.lance/.lance-deregistered</code> does not exist.</p>"},{"location":"format/namespace/dir/catalog-spec/#marker-files","title":"Marker Files","text":"<p>V1 uses marker files within table directories to track table state:</p> Marker File Purpose <code>.lance-reserved</code> Indicates a table name/location is reserved but not yet created <code>.lance-deregistered</code> Indicates a table has been deregistered but data is preserved <p>When a table is deregistered via the <code>DeregisterTable</code> operation, the <code>.lance-deregistered</code> marker file is created inside the table directory. This causes the table to be excluded from <code>ListTables</code> results and to return \"not found\" for <code>DescribeTable</code> and <code>TableExists</code> operations, while preserving the table data for potential re-registration.</p>"},{"location":"format/namespace/dir/catalog-spec/#v2-manifest","title":"V2: Manifest","text":"<p>V2 uses a special <code>__manifest</code> table (a Lance table) stored in the namespace directory to track all tables and namespaces. This provides several advantages over V1:</p> <ul> <li>Nested namespaces: Support for hierarchical namespace organization</li> <li>Better performance: Table discovery queries the manifest table instead of scanning the directory and leverages Lance's random access capability.</li> <li>Metadata support: All operations can be supported, e.g. namespaces can have associated properties/metadata, tables can be renamed.</li> <li>Optimized directory path: Hash-based directory naming prevents conflicts and maximizes throughput in object storage.</li> </ul>"},{"location":"format/namespace/dir/catalog-spec/#directory-layout_1","title":"Directory Layout","text":"<pre><code>.\n\u2514\u2500\u2500 /my/dir1/\n    \u251c\u2500\u2500 __manifest/                    # The manifest table\n    \u2502   \u251c\u2500\u2500 data/\n    \u2502   \u2502   \u2514\u2500\u2500 ...\n    \u2502   \u2514\u2500\u2500 _versions/\n    \u2502       \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 table1.lance/                  # Root namespace table (compatibility mode)\n    \u2502   \u2514\u2500\u2500 ...\n    \u251c\u2500\u2500 a1b2c3d4_table2/               # Root namespace table (V2)\n    \u2502   \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 e5f6g7h8_ns1$table3/           # Table in child namespace\n        \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"format/namespace/dir/catalog-spec/#manifest-table-schema","title":"Manifest Table Schema","text":"<p>The <code>__manifest</code> table has the following schema:</p> Column Type Description <code>object_id</code> String Unique identifier for the object. For root-level objects, this is the name. For nested objects, this is the namespace path joined by <code>$</code> delimiter (e.g., <code>ns1$ns2$table_name</code>) <code>object_type</code> String Either <code>\"namespace\"</code> or <code>\"table\"</code> <code>location</code> String (nullable) Relative path to the table directory within the root (only for tables) <code>metadata</code> String (nullable) JSON-encoded metadata/properties (only for namespaces) <code>base_objects</code> List (nullable) Reserved for future use (e.g., view dependencies) <p>Schema Extensibility: The <code>__manifest</code> table schema may include additional columns beyond those listed above. Extensions like partitioned namespaces add columns for efficient filtering. Implementations should preserve unrecognized columns during updates.</p>"},{"location":"format/namespace/dir/catalog-spec/#root-namespace-properties","title":"Root Namespace Properties","text":"<p>In V2, the root namespace is implicit and does not have a row in the <code>__manifest</code> table. Instead, root namespace properties are stored in the <code>__manifest</code> Lance table's metadata map. Properties are stored as key-value pairs where the key is the property name and the value is a UTF-8 encoded byte array.</p> <p>For example, a partitioned namespace stores its <code>partition_spec_v1</code>, <code>partition_spec_v2</code>, and <code>schema</code> properties in the <code>__manifest</code> table's metadata.</p>"},{"location":"format/namespace/dir/catalog-spec/#manifest-table-indexes","title":"Manifest Table Indexes","text":"<p>The following indexes are created on the manifest table for query performance:</p> <ul> <li>BTREE index on <code>object_id</code> for fast lookups</li> <li>Bitmap index on <code>object_type</code> for efficient type filtering</li> <li>LabelList index on <code>base_objects</code> for view dependency queries</li> </ul>"},{"location":"format/namespace/dir/catalog-spec/#manifest-table-commits","title":"Manifest Table Commits","text":"<p>When adding a new entry in the manifest table, it must atomically check if the table already exists such entry, as well as if any concurrent operation writes the same entry, and fail the operation accordingly if such conflict exists.</p>"},{"location":"format/namespace/dir/catalog-spec/#manifest-table-directory","title":"Manifest Table Directory","text":"<p>In V2, table data is stored in directories with hash-based names in the format <code>&lt;hash&gt;_&lt;object_id&gt;</code>. For example, a table <code>my_table</code> in namespace <code>ns1</code> would be stored in a directory like <code>a1b2c3d4_ns1$my_table</code>.</p> <p>The hash prefix serves two purposes:</p> <ol> <li>Object store throughput: Many object stores (e.g., S3) partition data by key prefix. Random hash prefixes distribute tables across partitions for better parallelism.</li> <li>Conflict prevention: High entropy prevents issues when a table is created, deleted, and recreated with the same name in quick succession.</li> </ol> <p>The <code>object_id</code> suffix ensures uniqueness and aids debugging.</p> <p>In compatibility mode, root namespace tables use <code>&lt;table_name&gt;.lance</code> naming to remain compatible with V1.</p>"},{"location":"format/namespace/dir/catalog-spec/#compatibility-mode","title":"Compatibility Mode","text":"<p>By default, the directory namespace operates in compatibility mode, supporting both V1 and V2 tables simultaneously. This allows gradual migration from V1 to V2 without disrupting existing workflows.</p> <p>In compatibility mode:</p> <ol> <li>When checking if a table exists in the root namespace, the implementation first checks the manifest table, then falls back to checking if a <code>&lt;table_name&gt;.lance</code> directory exists.</li> <li>When listing tables in the root namespace, results from both the manifest table and directory listing are merged, with manifest entries taking precedence when duplicates exist.</li> <li>When creating tables in the root namespace, the table is registered in the manifest and uses the V1 <code>&lt;table_name&gt;.lance</code> naming convention for backward compatibility.</li> <li>If a table in the root namespace is renamed, it transitions to the V2 hash-based path naming.</li> <li>For operations in child namespaces, only V2 behavior is used since V1 does not support nested namespaces.</li> </ol>"},{"location":"format/namespace/dir/catalog-spec/#migration-from-v1-to-v2","title":"Migration from V1 to V2","text":"<p>To fully migrate from V1 to V2, add all existing V1 table directory paths to the manifest table. Once all tables are registered in the manifest, compatibility mode can be disabled to use only V2 behavior.</p>"},{"location":"format/namespace/dir/impl-spec/","title":"Lance Directory Namespace Implementation Spec","text":"<p>This document describes how the Lance Directory Namespace implements the Lance Namespace client spec.</p>"},{"location":"format/namespace/dir/impl-spec/#background","title":"Background","text":"<p>The Lance Directory Namespace is a catalog that stores tables in a directory structure on any local or remote storage system. For details on the catalog design including V1 (directory listing), V2 (manifest), and compatibility mode, see the Directory Namespace Catalog Spec.</p>"},{"location":"format/namespace/dir/impl-spec/#namespace-implementation-configuration-properties","title":"Namespace Implementation Configuration Properties","text":"<p>The Lance directory namespace implementation accepts the following configuration properties:</p> <p>The root property is required and specifies the root directory of the namespace where tables are stored. This can be a local path like <code>/my/dir</code> or a cloud storage URI like <code>s3://bucket/prefix</code>.</p> <p>The manifest_enabled property controls whether the manifest table is used for tracking tables and namespaces (V2). Defaults to <code>true</code>.</p> <p>The dir_listing_enabled property controls whether directory scanning is used for table discovery (V1). Defaults to <code>true</code>.</p> <p>By default, both properties are enabled, which means the implementation operates in Compatibility Mode.</p> <p>Properties with the storage. prefix are passed directly to the underlying Lance ObjectStore after removing the prefix. For example, <code>storage.region</code> becomes <code>region</code> when passed to the storage layer.</p>"},{"location":"format/namespace/dir/impl-spec/#object-mapping","title":"Object Mapping","text":""},{"location":"format/namespace/dir/impl-spec/#namespace","title":"Namespace","text":"<p>The root namespace is the root directory specified by the <code>root</code> configuration property. This is the base path where all tables are stored.</p> <p>A child namespace is a logical container tracked in the manifest table. Child namespaces are only supported in V2; V1 treats the root directory as a flat namespace containing only tables. Child namespaces do not correspond to physical subdirectories.</p> <p>The namespace identifier is a list of strings representing the namespace path. For example, a namespace <code>[\"prod\", \"analytics\"]</code> is serialized to <code>prod$analytics</code> when stored in the manifest table's <code>object_id</code> column.</p> <p>Namespace properties are stored as JSON in the <code>metadata</code> column of the manifest table. This is only available in V2.</p>"},{"location":"format/namespace/dir/impl-spec/#table","title":"Table","text":"<p>A table is a subdirectory containing Lance table data. The directory must contain valid Lance format files including the <code>_versions/</code> directory with version manifests.</p> <p>The table identifier is a list of strings representing the namespace path followed by the table name. For example, a table <code>[\"prod\", \"analytics\", \"users\"]</code> represents a table named <code>users</code> in namespace <code>[\"prod\", \"analytics\"]</code>. This is serialized to <code>prod$analytics$users</code> when stored in the manifest table's <code>object_id</code> column.</p> <p>The table location depends on the mode and namespace level:</p> <ul> <li>In V1 (root namespace only), tables are stored as <code>&lt;table_name&gt;.lance</code> directories</li> <li>In V2 with <code>dir_listing_enabled=true</code> and an empty namespace (root level), tables use the <code>&lt;table_name&gt;.lance</code> naming convention for backward compatibility</li> <li>In V2 for child namespaces, or when <code>dir_listing_enabled=false</code>, tables are stored as <code>&lt;hash&gt;_&lt;object_id&gt;</code> directories where hash provides entropy for object store throughput</li> </ul> <p>Table properties are stored in Lance table metadata and can be accessed via the Lance SDK.</p>"},{"location":"format/namespace/dir/impl-spec/#lance-table-identification","title":"Lance Table Identification","text":"<p>In a Directory Namespace, a Lance table is identified differently depending on the mode:</p> <p>In V1, a Lance table is any directory with the <code>.lance</code> suffix (e.g., <code>users.lance/</code>). The directory must contain valid Lance table data to be usable. Only single-level table identifiers (e.g., <code>[\"users\"]</code>) are supported in this mode.</p> <p>In V2, a Lance table is identified by a row in the manifest table with <code>object_type=\"table\"</code>. The row's <code>location</code> field points to the Lance table directory. Multi-level table identifiers (e.g., <code>[\"prod\", \"analytics\", \"users\"]</code>) are supported.</p> <p>A valid Lance table directory must be non-empty.</p>"},{"location":"format/namespace/dir/impl-spec/#basic-operations","title":"Basic Operations","text":""},{"location":"format/namespace/dir/impl-spec/#createnamespace","title":"CreateNamespace","text":"<p>This operation is only supported in V2. V1 does not support explicit namespace creation since it uses a flat directory structure.</p> <p>The implementation creates a new namespace by inserting a row into the manifest table:</p> <ol> <li>Validate the parent namespace exists (if not creating at root level)</li> <li>Check that no namespace with the same identifier already exists</li> <li>Insert a new row into the manifest table with:<ul> <li><code>object_id</code> set to the namespace identifier (e.g., <code>prod$analytics</code>)</li> <li><code>object_type</code> set to <code>\"namespace\"</code></li> <li><code>metadata</code> containing the namespace properties as JSON</li> <li><code>created_at</code> set to the current timestamp</li> </ul> </li> </ol> <p>Error Handling:</p> <p>If a namespace with the same identifier already exists, return error code <code>2</code> (NamespaceAlreadyExists).</p> <p>If the parent namespace does not exist (for nested namespaces), return error code <code>1</code> (NamespaceNotFound).</p> <p>If the identifier format is invalid, return error code <code>13</code> (InvalidInput).</p>"},{"location":"format/namespace/dir/impl-spec/#listnamespaces","title":"ListNamespaces","text":"<p>This operation lists child namespaces within a parent namespace.</p> <p>In V1, this operation returns an empty list since namespaces are not supported.</p> <p>In V2, the implementation queries the manifest table:</p> <ol> <li>Query for rows where <code>object_type = \"namespace\"</code></li> <li>Filter to rows where <code>object_id</code> starts with the parent namespace prefix</li> <li>Further filter to rows where <code>object_id</code> has exactly one more level than the parent</li> <li>Return the list of namespace names (the last component of each identifier)</li> </ol> <p>Error Handling:</p> <p>If the parent namespace does not exist (V2 only), return error code <code>1</code> (NamespaceNotFound).</p>"},{"location":"format/namespace/dir/impl-spec/#describenamespace","title":"DescribeNamespace","text":"<p>This operation is only supported in V2 and returns namespace metadata.</p> <p>The implementation:</p> <ol> <li>Query the manifest table for the row with the matching <code>object_id</code></li> <li>Parse the <code>metadata</code> column as JSON</li> <li>Return the namespace name and properties</li> </ol> <p>Error Handling:</p> <p>If the namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p>"},{"location":"format/namespace/dir/impl-spec/#dropnamespace","title":"DropNamespace","text":"<p>This operation is only supported in V2 and removes a namespace.</p> <p>The implementation:</p> <ol> <li>Check that the namespace exists in the manifest table</li> <li>Query for any child namespaces or tables with identifiers starting with this namespace's prefix</li> <li>If any children exist, the operation fails</li> <li>Delete the namespace row from the manifest table</li> </ol> <p>Error Handling:</p> <p>If the namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the namespace contains tables or child namespaces, return error code <code>3</code> (NamespaceNotEmpty).</p>"},{"location":"format/namespace/dir/impl-spec/#declaretable","title":"DeclareTable","text":"<p>This operation declares a new Lance table, reserving the table name and location without creating actual data files.</p> <p>The implementation:</p> <ol> <li>Validate the parent namespace exists (in V2)</li> <li>Check that no table with the same identifier already exists</li> <li>Determine the table location:<ul> <li>In V1: <code>&lt;root&gt;/&lt;table_name&gt;.lance</code></li> <li>In V2 with <code>dir_listing_enabled=true</code> at root level: <code>&lt;root&gt;/&lt;table_name&gt;.lance</code></li> <li>In V2 for child namespaces or with <code>dir_listing_enabled=false</code>: <code>&lt;root&gt;/&lt;hash&gt;_&lt;object_id&gt;/</code></li> </ul> </li> <li>Create a <code>.lance-reserved</code> file at the location to mark the table's existence</li> <li>In V2, insert a row into the manifest table with:<ul> <li><code>object_id</code> set to the table identifier</li> <li><code>object_type</code> set to <code>\"table\"</code></li> <li><code>location</code> set to the table directory path</li> </ul> </li> </ol> <p>Error Handling:</p> <p>If the parent namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If a table with the same identifier already exists, return error code <code>5</code> (TableAlreadyExists).</p> <p>If there is a concurrent creation attempt, return error code <code>14</code> (ConcurrentModification).</p>"},{"location":"format/namespace/dir/impl-spec/#listtables","title":"ListTables","text":"<p>This operation lists tables within a namespace.</p> <p>In V1:</p> <ol> <li>List all entries in the root directory</li> <li>Filter to directories matching the <code>*.lance</code> pattern</li> <li>Return the table names (directory names without the <code>.lance</code> suffix)</li> </ol> <p>In V2:</p> <ol> <li>Query the manifest table for rows where <code>object_type = \"table\"</code></li> <li>Filter to rows where <code>object_id</code> starts with the namespace prefix</li> <li>Further filter to rows where <code>object_id</code> has exactly one more level than the namespace</li> <li>Return the list of table names</li> </ol> <p>When both V1 and V2 are enabled (the default Compatibility Mode),  the implementation performs both queries and merges results, with manifest entries taking precedence when duplicates exist.</p> <p>Error Handling:</p> <p>If the namespace does not exist (V2 only), return error code <code>1</code> (NamespaceNotFound).</p>"},{"location":"format/namespace/dir/impl-spec/#describetable","title":"DescribeTable","text":"<p>This operation returns table metadata including schema, version, and properties.</p> <p>The implementation:</p> <ol> <li>Locate the table:<ul> <li>In V1, check for the <code>&lt;table_name&gt;.lance</code> directory</li> <li>In V2, query the manifest table for the table location</li> <li>When both V1 and V2 are enabled (the default Compatibility Mode),     first check the manifest table, then fall back to checking the <code>.lance</code> directory</li> </ul> </li> <li>Open the Lance table using the Lance SDK</li> <li>Read the table metadata and return:<ul> <li><code>name</code>: The table name</li> <li><code>schema</code>: The Arrow schema of the table</li> <li><code>version</code>: The current version number</li> <li><code>location</code>: The table directory path</li> </ul> </li> </ol> <p>Error Handling:</p> <p>If the parent namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the table does not exist, return error code <code>4</code> (TableNotFound).</p> <p>If a specific version is requested and does not exist, return error code <code>11</code> (TableVersionNotFound).</p>"},{"location":"format/namespace/dir/impl-spec/#droptable","title":"DropTable","text":"<p>This operation removes a table and its data.</p> <p>In V1:</p> <ol> <li>Locate the table by checking for the <code>&lt;table_name&gt;.lance</code> directory</li> <li>Delete the table directory and all its contents from storage</li> <li>If deletion fails midway (directory is still non-empty), the drop has failed and should be retried</li> </ol> <p>In V2:</p> <ol> <li>Locate the table by querying the manifest table for the table location</li> <li>Remove the table row from the manifest table first</li> <li>Delete the table directory and all its contents from storage     (failure here does not affect the success of the drop since the table is no longer reachable)</li> </ol> <p>When both V1 and V2 are enabled (the default Compatibility Mode),  first check the manifest table, then fall back to checking the <code>.lance</code> directory.  If found in manifest, follow V2 behavior; otherwise follow V1 behavior.</p> <p>Error Handling:</p> <p>If the parent namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the table does not exist, return error code <code>4</code> (TableNotFound).</p> <p>If there is a file system permission error, return error code <code>15</code> (PermissionDenied).</p> <p>If there is an unexpected I/O error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/dir/impl-spec/#deregistertable","title":"DeregisterTable","text":"<p>This operation deregisters a table from the namespace while preserving its data on storage. The table files remain at their storage location and can be re-registered later using RegisterTable.</p> <p>In V1:</p> <ol> <li>Locate the table by checking for the <code>&lt;table_name&gt;.lance</code> directory</li> <li>Verify the table exists and is not already deregistered</li> <li>Create a <code>.lance-deregistered</code> marker file inside the table directory</li> <li>Return the table location for reference</li> </ol> <p>The marker file approach ensures that: - Table data remains intact at its original location - The table is excluded from <code>ListTables</code> results - The table returns <code>TableNotFound</code> for <code>DescribeTable</code> and <code>TableExists</code> operations - The table can be re-registered by removing the marker file and calling <code>RegisterTable</code> - <code>DropTable</code> still works on deregistered tables (removes both data and marker file)</p> <p>In V2:</p> <ol> <li>Locate the table by querying the manifest table for the table location</li> <li>Remove the table row from the manifest table</li> <li>Keep the table files at the storage location</li> <li>Return the table location and properties for reference</li> </ol> <p>When both V1 and V2 are enabled (the default Compatibility Mode), first check the manifest table, then fall back to checking the <code>.lance</code> directory. If found in manifest, follow V2 behavior; otherwise follow V1 behavior.</p> <p>Error Handling:</p> <p>If the parent namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the table does not exist or is already deregistered, return error code <code>4</code> (TableNotFound).</p>"},{"location":"format/namespace/integrations/","title":"Lance Namespace Catalog Integrations","text":"<p>This section covers the available open source and commercial catalog integrations for Lance Namespace.</p>"},{"location":"format/namespace/integrations/biglake/","title":"Google BigLake Metastore","text":"<p>Google BigLake Metastore  is a fully managed, unified metastore service for data lakes on Google Cloud.</p> <p>To use Google BigLake Metastore with Lance, you can leverage BigLake's Iceberg REST Catalog, which exposes an Apache Iceberg REST Catalog-compatible interface.</p> <p>See Lance Namespace integration with Iceberg REST Catalog for more details.</p>"},{"location":"format/namespace/integrations/dataproc/","title":"Google Dataproc Metastore","text":"<p>Google Dataproc Metastore is a fully managed, highly available, autohealing, serverless metastore that runs on Google Cloud.</p> <p>To use Google Dataproc Metastore with Lance, you can leverage Dataproc's Hive metastore, which exposes a Apache Hive MetaStore-compatible interface.</p> <p>See Lance Namespace integration with Hive metastore (V2 or V3) for more details.</p>"},{"location":"format/namespace/integrations/glue/","title":"AWS Glue Data Catalog Lance Namespace Implementation Spec","text":"<p>This document describes how the AWS Glue Data Catalog implements the Lance Namespace client spec.</p>"},{"location":"format/namespace/integrations/glue/#background","title":"Background","text":"<p>AWS Glue Data Catalog is a fully managed metadata repository that stores structural and operational metadata for data assets.  It is based on the Apache Hive Metastore API, but uses JSON RPC instead of Apache Thrift for request response. It can be used as a central metadata repository for data lakes.  For details on AWS Glue, see the AWS Glue Data Catalog Documentation.</p>"},{"location":"format/namespace/integrations/glue/#namespace-implementation-configuration-properties","title":"Namespace Implementation Configuration Properties","text":"<p>The Lance Glue namespace implementation accepts the following configuration properties:</p> <p>The catalog_id property is optional and specifies the Catalog ID of the Glue catalog to use as the starting point. When not specified, it is resolved to the caller's AWS account ID.</p> <p>The endpoint property is optional and specifies a custom Glue service endpoint for API compatible metastores.</p> <p>The region property is optional and specifies the AWS region for all Glue operations. When not specified, it is resolved to the default AWS region in the caller's environment.</p> <p>The access_key_id property is optional and specifies the AWS access key ID for static credentials.</p> <p>The secret_access_key property is optional and specifies the AWS secret access key for static credentials.</p> <p>The session_token property is optional and specifies the AWS session token for temporary credentials.</p> <p>The assume_role_arn property is optional and specifies the ARN of the IAM role to assume for Glue operations.</p> <p>The assume_role_region property is optional and specifies the AWS region for the STS client when assuming a role.</p> <p>The assume_role_external_id property is optional and specifies the external ID for cross-account role assumption. For more details, see AWS external ID documentation.</p> <p>The assume_role_session_name property is optional and specifies the session name for the assumed role session. For more details, see AWS role session name documentation.</p> <p>The assume_role_timeout_sec property is optional and specifies the duration in seconds for which the assumed role session is valid (default: 3600). At the end of the timeout, a new set of role session credentials will be fetched through the STS client.</p>"},{"location":"format/namespace/integrations/glue/#authentication","title":"Authentication","text":"<p>The Glue namespace supports multiple authentication methods:</p> <ol> <li>Default AWS credential provider chain: When no explicit credentials are provided, the client uses the default AWS credential provider chain</li> <li>Static credentials: Set <code>access_key_id</code> and <code>secret_access_key</code> for basic AWS credentials</li> <li>Session credentials: Additionally provide <code>session_token</code> for temporary AWS credentials</li> <li>Assume role credentials: Set <code>assume_role_arn</code> to assume an IAM role. Optionally configure <code>assume_role_region</code>, <code>assume_role_external_id</code>, <code>assume_role_session_name</code>, and <code>assume_role_timeout_sec</code> to customize the role assumption behavior</li> </ol>"},{"location":"format/namespace/integrations/glue/#object-mapping","title":"Object Mapping","text":""},{"location":"format/namespace/integrations/glue/#namespace","title":"Namespace","text":"<p>AWS Glue Data Catalog supports a recursive catalog structure through the GetCatalog and GetCatalogs APIs.  This allows for multi-level namespace hierarchies.</p> <p>The root namespace is represented by the default AWS Glue Data Catalog. When the <code>catalog_id</code> property is not specified or set to <code>None</code>, it is resolved to the caller's AWS account ID. Users can specify a different <code>catalog_id</code> to use another AWS account's Glue catalog as the starting point.</p> <p>A child catalog within the root catalog forms a child namespace. The GetCatalogs API supports <code>ParentCatalogId</code> parameter to traverse the catalog hierarchy.</p> <p>A database within a catalog represents the leaf namespace level. Databases are created within a specific catalog using the <code>CatalogId</code> parameter in the CreateDatabase API.</p> <p>The namespace identifier follows a hierarchical pattern: - For catalogs: the catalog name (e.g., <code>my_catalog</code>) - For databases: the catalog chain joined with database name using the <code>$</code> delimiter (e.g., <code>catalog$database</code> or <code>parent_catalog$child_catalog$database</code>)</p> <p>Namespace properties are stored in: - Catalog's <code>Parameters</code> map for catalog-level namespaces - Database's <code>Parameters</code> map for database-level namespaces</p>"},{"location":"format/namespace/integrations/glue/#table","title":"Table","text":"<p>A table is represented as a Table object in AWS Glue with <code>TableType</code> set to <code>EXTERNAL_TABLE</code>.</p> <p>The table identifier is constructed by joining the full namespace path and table name with the <code>$</code> delimiter (e.g., <code>catalog$database$table</code>).</p> <p>The table location is stored in the <code>StorageDescriptor.Location</code> field, pointing to the root location of the Lance table.</p> <p>Table properties are stored in the table's <code>Parameters</code> map.</p>"},{"location":"format/namespace/integrations/glue/#lance-table-identification","title":"Lance Table Identification","text":"<p>A table in AWS Glue is identified as a Lance table when it meets the following criteria: the <code>TableType</code> is <code>EXTERNAL_TABLE</code>, and the <code>Parameters</code> map contains a key <code>table_type</code> with value <code>lance</code> (case insensitive). The <code>StorageDescriptor.Location</code> must point to a valid Lance table root directory.</p>"},{"location":"format/namespace/integrations/glue/#basic-operations","title":"Basic Operations","text":""},{"location":"format/namespace/integrations/glue/#createnamespace","title":"CreateNamespace","text":"<p>Creates a new catalog or database in AWS Glue.</p> <p>The implementation:</p> <ol> <li>Parse the namespace identifier to determine if it is a catalog or database level</li> <li>For catalog-level namespace:<ul> <li>Construct a CreateCatalog request with name and properties</li> <li>Set the <code>Parameters</code> map with the provided namespace properties</li> </ul> </li> <li>For database-level namespace:<ul> <li>Verify the parent catalog exists</li> <li>Construct a CreateDatabase request with database name and <code>CatalogId</code></li> <li>Set the <code>Parameters</code> map with the provided namespace properties</li> </ul> </li> <li>Handle creation mode (CREATE, EXIST_OK, OVERWRITE) appropriately</li> </ol> <p>Error Handling:</p> <p>If the namespace already exists and mode is CREATE, return error code <code>2</code> (NamespaceAlreadyExists).</p> <p>If the parent catalog does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If access is denied, return error code <code>16</code> (Forbidden).</p> <p>If the Glue service is unavailable, return error code <code>17</code> (ServiceUnavailable).</p>"},{"location":"format/namespace/integrations/glue/#listnamespaces","title":"ListNamespaces","text":"<p>Lists catalogs or databases in AWS Glue.</p> <p>The implementation:</p> <ol> <li>Parse the parent namespace identifier</li> <li>For root namespace (no parent):<ul> <li>Use GetCatalogs with <code>IncludeRoot=true</code> to list all catalogs</li> <li>Use <code>ParentCatalogId</code> set to account ID and <code>Recursive=false</code> for direct children</li> </ul> </li> <li>For catalog-level namespace:<ul> <li>Use GetDatabases with the catalog's <code>CatalogId</code></li> <li>Additionally use GetCatalogs with <code>ParentCatalogId</code> to list child catalogs</li> </ul> </li> <li>Sort the results and apply pagination using <code>NextToken</code></li> </ol> <p>Error Handling:</p> <p>If the parent namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If access is denied, return error code <code>16</code> (Forbidden).</p> <p>If the Glue service is unavailable, return error code <code>17</code> (ServiceUnavailable).</p>"},{"location":"format/namespace/integrations/glue/#describenamespace","title":"DescribeNamespace","text":"<p>Retrieves properties and metadata for a catalog or database.</p> <p>The implementation:</p> <ol> <li>Parse the namespace identifier to determine the level</li> <li>For catalog-level namespace:<ul> <li>Use GetCatalog with the catalog ID</li> <li>Extract properties from the <code>Parameters</code> map</li> </ul> </li> <li>For database-level namespace:<ul> <li>Use GetDatabase with the database name and <code>CatalogId</code></li> <li>Extract properties from the Database's <code>Parameters</code> map</li> </ul> </li> </ol> <p>Error Handling:</p> <p>If the namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If access is denied, return error code <code>16</code> (Forbidden).</p> <p>If the Glue service is unavailable, return error code <code>17</code> (ServiceUnavailable).</p>"},{"location":"format/namespace/integrations/glue/#dropnamespace","title":"DropNamespace","text":"<p>Removes a catalog or database from AWS Glue. Only RESTRICT mode is supported; CASCADE mode is not implemented.</p> <p>The implementation:</p> <ol> <li>Parse the namespace identifier to determine the level</li> <li>Check if the namespace exists (handle SKIP mode if not)</li> <li>For catalog-level namespace:<ul> <li>Verify the catalog has no child catalogs or databases</li> <li>Use DeleteCatalog with the catalog ID</li> </ul> </li> <li>For database-level namespace:<ul> <li>Verify the database is empty (no tables)</li> <li>Use DeleteDatabase with the database name and <code>CatalogId</code></li> </ul> </li> </ol> <p>Error Handling:</p> <p>If the namespace does not exist and mode is FAIL, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the namespace is not empty, return error code <code>3</code> (NamespaceNotEmpty).</p> <p>If access is denied, return error code <code>16</code> (Forbidden).</p> <p>If the Glue service is unavailable, return error code <code>17</code> (ServiceUnavailable).</p>"},{"location":"format/namespace/integrations/glue/#declaretable","title":"DeclareTable","text":"<p>Declares a new Lance table in AWS Glue without creating the underlying data.</p> <p>The implementation:</p> <ol> <li>Parse the table identifier to extract catalog, database, and table name</li> <li>Verify the parent namespace (database) exists using GetDatabase</li> <li>Construct a CreateTable request with:<ul> <li><code>CatalogId</code>: the catalog ID from the namespace</li> <li><code>DatabaseName</code>: the database name</li> <li><code>TableInput.Name</code>: the table name</li> <li><code>TableInput.TableType</code>: <code>EXTERNAL_TABLE</code></li> <li><code>TableInput.Parameters</code>: include <code>table_type=lance</code> and other properties</li> <li><code>TableInput.StorageDescriptor.Location</code>: the specified table location</li> </ul> </li> <li>POST the CreateTable request to Glue</li> </ol> <p>Error Handling:</p> <p>If the parent namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the table already exists, return error code <code>5</code> (TableAlreadyExists).</p> <p>If access is denied, return error code <code>16</code> (Forbidden).</p> <p>If the Glue service is unavailable, return error code <code>17</code> (ServiceUnavailable).</p>"},{"location":"format/namespace/integrations/glue/#listtables","title":"ListTables","text":"<p>Lists all Lance tables in a database.</p> <p>The implementation:</p> <ol> <li>Parse the namespace identifier to extract catalog and database</li> <li>Verify the namespace exists using GetDatabase</li> <li>Use GetTables with <code>CatalogId</code> and <code>DatabaseName</code></li> <li>Filter tables where <code>Parameters.table_type=lance</code> (case insensitive)</li> <li>Sort the results and apply pagination using <code>NextToken</code></li> </ol> <p>Error Handling:</p> <p>If the namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If access is denied, return error code <code>16</code> (Forbidden).</p> <p>If the Glue service is unavailable, return error code <code>17</code> (ServiceUnavailable).</p>"},{"location":"format/namespace/integrations/glue/#describetable","title":"DescribeTable","text":"<p>Retrieves metadata for a Lance table. Only <code>load_detailed_metadata=false</code> is supported. When <code>load_detailed_metadata=false</code>, only the table location and storage_options are returned; other fields (version, table_uri, schema, stats) are null.</p> <p>The implementation:</p> <ol> <li>Parse the table identifier to extract catalog, database, and table name</li> <li>Use GetTable with <code>CatalogId</code>, <code>DatabaseName</code>, and <code>Name</code></li> <li>Validate that the table is a Lance table (check <code>Parameters.table_type=lance</code>)</li> <li>Return the table location from <code>StorageDescriptor.Location</code> and storage_options from <code>Parameters</code></li> </ol> <p>Error Handling:</p> <p>If the table does not exist, return error code <code>4</code> (TableNotFound).</p> <p>If the table is not a Lance table, return error code <code>13</code> (InvalidInput).</p> <p>If access is denied, return error code <code>16</code> (Forbidden).</p> <p>If the Glue service is unavailable, return error code <code>17</code> (ServiceUnavailable).</p>"},{"location":"format/namespace/integrations/glue/#deregistertable","title":"DeregisterTable","text":"<p>Removes a Lance table registration from AWS Glue without deleting the underlying data.</p> <p>The implementation:</p> <ol> <li>Parse the table identifier to extract catalog, database, and table name</li> <li>Use GetTable to retrieve and validate the table is a Lance table</li> <li>Use DeleteTable with <code>CatalogId</code>, <code>DatabaseName</code>, and <code>Name</code></li> <li>The underlying Lance table data at <code>StorageDescriptor.Location</code> is not deleted</li> </ol> <p>Error Handling:</p> <p>If the table does not exist, return error code <code>4</code> (TableNotFound).</p> <p>If the table is not a Lance table, return error code <code>13</code> (InvalidInput).</p> <p>If access is denied, return error code <code>16</code> (Forbidden).</p> <p>If the Glue service is unavailable, return error code <code>17</code> (ServiceUnavailable).</p>"},{"location":"format/namespace/integrations/gravitino/","title":"Apache Gravitino","text":"<p>Apache Gravitino is a high-performance, geo-distributed, and federated metadata lake. It manages metadata directly in different sources, types, and regions, providing unified metadata access for data and AI assets.</p>"},{"location":"format/namespace/integrations/gravitino/#using-gravitino-with-lance","title":"Using Gravitino with Lance","text":"<p>Gravitino provides multiple ways to integrate with Lance, allowing you to choose the best option based on your use case.</p>"},{"location":"format/namespace/integrations/gravitino/#option-1-native-lance-rest-support","title":"Option 1: Native Lance REST Support","text":"<p>Starting from version 1.1.0, Apache Gravitino provides native integration with the Lance REST Namespace. This allows you to use Gravitino as a Lance namespace server directly, with full support for Lance tables.</p> <p>For details on configuring Gravitino as a Lance REST namespace server, see the Gravitino Lance REST Service Documentation.</p>"},{"location":"format/namespace/integrations/gravitino/#option-2-hive-metastore","title":"Option 2: Hive MetaStore","text":"<p>For users who primarily use Gravitino for its Hive MetaStore-related capabilities, Gravitino provides a Hive MetaStore-compatible endpoint through its Apache Hive Catalog.</p> <p>Configure your Lance Hive namespace to connect to Gravitino's Hive MetaStore endpoint. All the features and configurations of the Lance Hive Namespace (V2 or V3) apply when using Gravitino's Hive catalog.</p>"},{"location":"format/namespace/integrations/hive2/","title":"Apache Hive 2.X MetaStore Lance Namespace Implementation Spec","text":"<p>This document describes how the Hive 2.x MetaStore implements the Lance Namespace client spec.</p>"},{"location":"format/namespace/integrations/hive2/#background","title":"Background","text":"<p>Apache Hive MetaStore (HMS) is a centralized metadata repository for Apache Hive that stores schema and partition information for Hive tables. Hive 2.x uses a 2-level namespace hierarchy (database.table). For details on HMS 2.x, see the HMS AdminManual 2.x.</p>"},{"location":"format/namespace/integrations/hive2/#namespace-implementation-configuration-properties","title":"Namespace Implementation Configuration Properties","text":"<p>The Lance Hive 2.x namespace implementation accepts the following configuration properties:</p> <p>The client.pool-size property is optional and specifies the size of the HMS client connection pool. Default value is <code>3</code>.</p> <p>The root property is optional and specifies the storage root location of the lakehouse on Hive catalog. Default value is the current working directory.</p>"},{"location":"format/namespace/integrations/hive2/#object-mapping","title":"Object Mapping","text":""},{"location":"format/namespace/integrations/hive2/#namespace","title":"Namespace","text":"<p>The root namespace is represented by the HMS server itself.</p> <p>A child namespace is a database in HMS, forming a 2-level namespace hierarchy.</p> <p>The namespace identifier is the database name.</p> <p>Namespace properties are stored in the HMS Database object's parameters map, with special handling for <code>database.description</code>, <code>database.location-uri</code>, <code>database.owner</code>, and <code>database.owner-type</code>.</p>"},{"location":"format/namespace/integrations/hive2/#table","title":"Table","text":"<p>A table is represented as a Table object in HMS with <code>tableType</code> set to <code>EXTERNAL_TABLE</code>.</p> <p>The table identifier is constructed by joining database and table name with the <code>$</code> delimiter (e.g., <code>database$table</code>).</p> <p>The table location is stored in the <code>location</code> field of the table's <code>storageDescriptor</code>, pointing to the root location of the Lance table.</p> <p>Table properties are stored in the table's <code>parameters</code> map.</p>"},{"location":"format/namespace/integrations/hive2/#lance-table-identification","title":"Lance Table Identification","text":"<p>A table in HMS is identified as a Lance table when it meets the following criteria: the <code>tableType</code> is <code>EXTERNAL_TABLE</code>, and the <code>parameters</code> map contains a key <code>table_type</code> with value <code>lance</code> (case insensitive). The <code>location</code> in <code>storageDescriptor</code> must point to a valid Lance table root directory.</p>"},{"location":"format/namespace/integrations/hive2/#basic-operations","title":"Basic Operations","text":""},{"location":"format/namespace/integrations/hive2/#createnamespace","title":"CreateNamespace","text":"<p>Creates a new database in HMS.</p> <p>The implementation:</p> <ol> <li>Parse the namespace identifier (database name)</li> <li>Create a new Database object with the specified name, location, and properties</li> <li>Handle creation mode (CREATE, EXIST_OK, OVERWRITE) appropriately</li> </ol> <p>Error Handling:</p> <p>If the namespace already exists and mode is CREATE, return error code <code>2</code> (NamespaceAlreadyExists).</p> <p>If the HMS connection fails, return error code <code>17</code> (ServiceUnavailable).</p>"},{"location":"format/namespace/integrations/hive2/#listnamespaces","title":"ListNamespaces","text":"<p>Lists all databases in HMS.</p> <p>The implementation:</p> <ol> <li>Parse the parent namespace identifier</li> <li>For root namespace: list all databases</li> <li>Sort the results and apply pagination</li> </ol> <p>Error Handling:</p> <p>If the HMS connection fails, return error code <code>17</code> (ServiceUnavailable).</p>"},{"location":"format/namespace/integrations/hive2/#describenamespace","title":"DescribeNamespace","text":"<p>Retrieves properties and metadata for a database.</p> <p>The implementation:</p> <ol> <li>Parse the namespace identifier (database name)</li> <li>Retrieve the Database object and extract description, location, owner, and custom properties</li> </ol> <p>Error Handling:</p> <p>If the namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the HMS connection fails, return error code <code>17</code> (ServiceUnavailable).</p>"},{"location":"format/namespace/integrations/hive2/#dropnamespace","title":"DropNamespace","text":"<p>Removes a database from HMS. Only RESTRICT mode is supported; CASCADE mode is not implemented.</p> <p>The implementation:</p> <ol> <li>Parse the namespace identifier (database name)</li> <li>Check if the namespace exists (handle SKIP mode if not)</li> <li>Verify the namespace is empty (no tables)</li> <li>Drop the database from HMS</li> </ol> <p>Error Handling:</p> <p>If the namespace does not exist and mode is FAIL, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the namespace is not empty, return error code <code>3</code> (NamespaceNotEmpty).</p> <p>If the HMS connection fails, return error code <code>17</code> (ServiceUnavailable).</p>"},{"location":"format/namespace/integrations/hive2/#declaretable","title":"DeclareTable","text":"<p>Declares a new Lance table in HMS without creating the underlying data.</p> <p>The implementation:</p> <ol> <li>Parse the table identifier to extract database and table name</li> <li>Verify the parent namespace exists</li> <li>Create an HMS Table object with <code>tableType=EXTERNAL_TABLE</code></li> <li>Set the storage descriptor with the specified or default location. When location is not specified, it defaults to <code>{root}/{database}.db/{table}</code></li> <li>Add <code>table_type=lance</code> to the table parameters</li> <li>Register the table in HMS</li> </ol> <p>Error Handling:</p> <p>If the parent namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the table already exists, return error code <code>5</code> (TableAlreadyExists).</p> <p>If the HMS connection fails, return error code <code>17</code> (ServiceUnavailable).</p>"},{"location":"format/namespace/integrations/hive2/#listtables","title":"ListTables","text":"<p>Lists all Lance tables in a database.</p> <p>The implementation:</p> <ol> <li>Parse the namespace identifier (database name)</li> <li>Verify the namespace exists</li> <li>Retrieve all tables in the database</li> <li>Filter tables where <code>parameters.table_type=lance</code></li> <li>Sort the results and apply pagination</li> </ol> <p>Error Handling:</p> <p>If the namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the HMS connection fails, return error code <code>17</code> (ServiceUnavailable).</p>"},{"location":"format/namespace/integrations/hive2/#describetable","title":"DescribeTable","text":"<p>Retrieves metadata for a Lance table. Only <code>load_detailed_metadata=false</code> is supported. When <code>load_detailed_metadata=false</code>, only the table location is returned; other fields (version, table_uri, schema, stats) are null.</p> <p>The implementation:</p> <ol> <li>Parse the table identifier</li> <li>Retrieve the Table object from HMS</li> <li>Validate that it is a Lance table (check <code>table_type=lance</code>)</li> <li>Return the table location from <code>storageDescriptor.location</code></li> </ol> <p>Error Handling:</p> <p>If the table does not exist, return error code <code>4</code> (TableNotFound).</p> <p>If the table is not a Lance table, return error code <code>13</code> (InvalidInput).</p> <p>If the HMS connection fails, return error code <code>17</code> (ServiceUnavailable).</p>"},{"location":"format/namespace/integrations/hive2/#deregistertable","title":"DeregisterTable","text":"<p>Removes a Lance table registration from HMS without deleting the underlying data.</p> <p>The implementation:</p> <ol> <li>Parse the table identifier</li> <li>Retrieve the Table object and validate it is a Lance table</li> <li>Drop the table from HMS with <code>deleteData=false</code></li> </ol> <p>Error Handling:</p> <p>If the table does not exist, return error code <code>4</code> (TableNotFound).</p> <p>If the table is not a Lance table, return error code <code>13</code> (InvalidInput).</p> <p>If the HMS connection fails, return error code <code>17</code> (ServiceUnavailable).</p>"},{"location":"format/namespace/integrations/hive3/","title":"Apache Hive 3+.X MetaStore Lance Namespace Implementation Spec","text":"<p>This document describes how the Hive 3.x MetaStore implements the Lance Namespace client spec.</p>"},{"location":"format/namespace/integrations/hive3/#background","title":"Background","text":"<p>Apache Hive MetaStore (HMS) is a centralized metadata repository for Apache Hive that stores schema and partition information for Hive tables. Hive 3.x introduces a 3-level namespace hierarchy (catalog.database.table) with an additional catalog level. For details on HMS 3.x, see the HMS AdminManual 3.x.</p>"},{"location":"format/namespace/integrations/hive3/#namespace-implementation-configuration-properties","title":"Namespace Implementation Configuration Properties","text":"<p>The Lance Hive 3+.x namespace implementation accepts the following configuration properties:</p> <p>The client.pool-size property is optional and specifies the size of the HMS client connection pool. Default value is <code>3</code>.</p> <p>The root property is optional and specifies the storage root location of the lakehouse on Hive catalog. Default value is the current working directory.</p>"},{"location":"format/namespace/integrations/hive3/#object-mapping","title":"Object Mapping","text":""},{"location":"format/namespace/integrations/hive3/#namespace","title":"Namespace","text":"<p>The root namespace is represented by the HMS server itself.</p> <p>The first-level child namespace is a catalog in HMS, and the second-level child namespace is a database within that catalog, forming a 3-level namespace hierarchy.</p> <p>The namespace identifier is constructed by joining namespace levels with the <code>$</code> delimiter (e.g., <code>catalog$database</code>).</p> <p>Namespace properties are stored in the HMS Database object's parameters map, with special handling for <code>database.description</code>, <code>database.location-uri</code>, <code>database.owner</code>, and <code>database.owner-type</code>. Catalog properties are stored in the Catalog object.</p>"},{"location":"format/namespace/integrations/hive3/#table","title":"Table","text":"<p>A table is represented as a Table object in HMS with <code>tableType</code> set to <code>EXTERNAL_TABLE</code>.</p> <p>The table identifier is constructed by joining catalog, database, and table name with the <code>$</code> delimiter (e.g., <code>catalog$database$table</code>).</p> <p>The table location is stored in the <code>location</code> field of the table's <code>storageDescriptor</code>, pointing to the root location of the Lance table.</p> <p>Table properties are stored in the table's <code>parameters</code> map.</p>"},{"location":"format/namespace/integrations/hive3/#lance-table-identification","title":"Lance Table Identification","text":"<p>A table in HMS is identified as a Lance table when it meets the following criteria: the <code>tableType</code> is <code>EXTERNAL_TABLE</code>, and the <code>parameters</code> map contains a key <code>table_type</code> with value <code>lance</code> (case insensitive). The <code>location</code> in <code>storageDescriptor</code> must point to a valid Lance table root directory.</p>"},{"location":"format/namespace/integrations/hive3/#basic-operations","title":"Basic Operations","text":""},{"location":"format/namespace/integrations/hive3/#createnamespace","title":"CreateNamespace","text":"<p>Creates a new namespace (catalog or database) in HMS.</p> <p>The implementation:</p> <ol> <li>Parse the namespace identifier to determine the level (catalog or database)</li> <li>For catalog creation: create a new Catalog object with the specified name and location</li> <li>For database creation: verify the parent catalog exists, then create a new Database object with the specified name, location, and properties</li> <li>Handle creation mode (CREATE, EXIST_OK, OVERWRITE) appropriately</li> </ol> <p>Error Handling:</p> <p>If the namespace already exists and mode is CREATE, return error code <code>2</code> (NamespaceAlreadyExists).</p> <p>If the parent catalog does not exist when creating a database, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the HMS connection fails, return error code <code>17</code> (ServiceUnavailable).</p>"},{"location":"format/namespace/integrations/hive3/#listnamespaces","title":"ListNamespaces","text":"<p>Lists child namespaces under a given parent namespace.</p> <p>The implementation:</p> <ol> <li>Parse the parent namespace identifier</li> <li>For root namespace: list all catalogs</li> <li>For catalog namespace: list all databases in that catalog</li> <li>Sort the results and apply pagination</li> </ol> <p>Error Handling:</p> <p>If the parent namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the HMS connection fails, return error code <code>17</code> (ServiceUnavailable).</p>"},{"location":"format/namespace/integrations/hive3/#describenamespace","title":"DescribeNamespace","text":"<p>Retrieves properties and metadata for a namespace.</p> <p>The implementation:</p> <ol> <li>Parse the namespace identifier</li> <li>For catalog: retrieve the Catalog object and extract description and location</li> <li>For database: retrieve the Database object and extract description, location, owner, and custom properties</li> </ol> <p>Error Handling:</p> <p>If the namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the HMS connection fails, return error code <code>17</code> (ServiceUnavailable).</p>"},{"location":"format/namespace/integrations/hive3/#dropnamespace","title":"DropNamespace","text":"<p>Removes a namespace from HMS. Only RESTRICT mode is supported; CASCADE mode is not implemented.</p> <p>The implementation:</p> <ol> <li>Parse the namespace identifier</li> <li>Check if the namespace exists (handle SKIP mode if not)</li> <li>Verify the namespace is empty (no child namespaces or tables)</li> <li>Drop the catalog or database from HMS</li> </ol> <p>Error Handling:</p> <p>If the namespace does not exist and mode is FAIL, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the namespace is not empty, return error code <code>3</code> (NamespaceNotEmpty).</p> <p>If the HMS connection fails, return error code <code>17</code> (ServiceUnavailable).</p>"},{"location":"format/namespace/integrations/hive3/#declaretable","title":"DeclareTable","text":"<p>Declares a new Lance table in HMS without creating the underlying data.</p> <p>The implementation:</p> <ol> <li>Parse the table identifier to extract catalog, database, and table name</li> <li>Verify the parent namespace exists</li> <li>Create an HMS Table object with <code>tableType=EXTERNAL_TABLE</code></li> <li>Set the storage descriptor with the specified or default location. When location is not specified, it defaults to <code>{root}/{database}.db/{table}</code> for the default <code>hive</code> catalog (hive2-compatible), or <code>{root}/{catalog}/{database}.db/{table}</code> for other catalogs</li> <li>Add <code>table_type=lance</code> to the table parameters</li> <li>Register the table in HMS</li> </ol> <p>Error Handling:</p> <p>If the parent namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the table already exists, return error code <code>5</code> (TableAlreadyExists).</p> <p>If the HMS connection fails, return error code <code>17</code> (ServiceUnavailable).</p>"},{"location":"format/namespace/integrations/hive3/#listtables","title":"ListTables","text":"<p>Lists all Lance tables in a namespace.</p> <p>The implementation:</p> <ol> <li>Parse the namespace identifier (catalog$database)</li> <li>Verify the namespace exists</li> <li>Retrieve all tables in the database</li> <li>Filter tables where <code>parameters.table_type=lance</code></li> <li>Sort the results and apply pagination</li> </ol> <p>Error Handling:</p> <p>If the namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the HMS connection fails, return error code <code>17</code> (ServiceUnavailable).</p>"},{"location":"format/namespace/integrations/hive3/#describetable","title":"DescribeTable","text":"<p>Retrieves metadata for a Lance table. Only <code>load_detailed_metadata=false</code> is supported. When <code>load_detailed_metadata=false</code>, only the table location is returned; other fields (version, table_uri, schema, stats) are null.</p> <p>The implementation:</p> <ol> <li>Parse the table identifier</li> <li>Retrieve the Table object from HMS</li> <li>Validate that it is a Lance table (check <code>table_type=lance</code>)</li> <li>Return the table location from <code>storageDescriptor.location</code></li> </ol> <p>Error Handling:</p> <p>If the table does not exist, return error code <code>4</code> (TableNotFound).</p> <p>If the table is not a Lance table, return error code <code>13</code> (InvalidInput).</p> <p>If the HMS connection fails, return error code <code>17</code> (ServiceUnavailable).</p>"},{"location":"format/namespace/integrations/hive3/#deregistertable","title":"DeregisterTable","text":"<p>Removes a Lance table registration from HMS without deleting the underlying data.</p> <p>The implementation:</p> <ol> <li>Parse the table identifier</li> <li>Retrieve the Table object and validate it is a Lance table</li> <li>Drop the table from HMS with <code>deleteData=false</code></li> </ol> <p>Error Handling:</p> <p>If the table does not exist, return error code <code>4</code> (TableNotFound).</p> <p>If the table is not a Lance table, return error code <code>13</code> (InvalidInput).</p> <p>If the HMS connection fails, return error code <code>17</code> (ServiceUnavailable).</p>"},{"location":"format/namespace/integrations/iceberg/","title":"Apache Iceberg REST Catalog Lance Namespace Implementation Spec","text":"<p>This document describes how the Apache Iceberg REST Catalog implements the Lance Namespace client spec.</p>"},{"location":"format/namespace/integrations/iceberg/#background","title":"Background","text":"<p>Apache Iceberg REST Catalog is a standardized REST API for interacting with Iceberg catalogs. It provides a vendor-neutral interface for managing tables and namespaces across different catalog backends. When registering a Lance table, the implementation creates a companion Iceberg table with a dummy schema at the same location, using table properties to identify it as a Lance table. For details on the Iceberg REST Catalog, see the Iceberg REST Catalog Specification.</p>"},{"location":"format/namespace/integrations/iceberg/#namespace-implementation-configuration-properties","title":"Namespace Implementation Configuration Properties","text":"<p>The Lance Iceberg REST Catalog namespace implementation accepts the following configuration properties:</p> <p>The endpoint property is required and specifies the Iceberg REST Catalog server endpoint URL (e.g., <code>http://localhost:8181</code>). Must start with <code>http://</code> or <code>https://</code>.</p> <p>The auth_token property is optional and specifies the bearer token for authentication.</p> <p>The credential property is optional and specifies the OAuth2 client credential in <code>client_id:client_secret</code> format for client credentials authentication flow.</p> <p>The connect_timeout property is optional and specifies the connection timeout in milliseconds (default: 10000).</p> <p>The read_timeout property is optional and specifies the read timeout in milliseconds (default: 30000).</p> <p>The max_retries property is optional and specifies the maximum number of retries for failed requests (default: 3).</p> <p>The root property is optional and specifies the default storage root location for tables. When not specified, it defaults to the current working directory.</p>"},{"location":"format/namespace/integrations/iceberg/#object-mapping","title":"Object Mapping","text":""},{"location":"format/namespace/integrations/iceberg/#namespace","title":"Namespace","text":"<p>The root namespace (empty identifier) represents the Iceberg REST Catalog server itself.</p> <p>The warehouse is the first level of the namespace hierarchy. The implementation caches the warehouse to config mapping by calling the <code>/v1/config?warehouse={warehouse}</code> endpoint. If the config response contains a <code>prefix</code> in the defaults, that prefix is used for API paths; otherwise, the warehouse name itself is used as the prefix. A single-element identifier (e.g., <code>[\"my-warehouse\"]</code>) lists all top-level namespaces under that warehouse.</p> <p>A child namespace is a nested namespace in Iceberg. Iceberg supports arbitrary nesting depth. The namespace identifier format is <code>[warehouse, namespace1, namespace2, ...]</code> (e.g., <code>[\"my-warehouse\", \"level1\", \"level2\"]</code>).</p> <p>For API calls, namespace levels (excluding the warehouse) are joined with the <code>\\x1F</code> (unit separator) character. In user-facing contexts, a <code>.</code> delimiter is used.</p> <p>Namespace properties are stored in the namespace's properties map, returned by the Iceberg namespace API.</p>"},{"location":"format/namespace/integrations/iceberg/#table","title":"Table","text":"<p>A table is represented as a regular Iceberg table with a dummy schema. The dummy schema contains a single nullable string column named <code>dummy</code>. This approach ensures compatibility with the Iceberg REST Catalog API while storing the actual Lance table at the same location.</p> <p>The table identifier format is <code>[warehouse, namespace1, namespace2, ..., table_name]</code> (e.g., <code>[\"my-warehouse\", \"db\", \"my_table\"]</code>).</p> <p>The table location is stored in the <code>location</code> field of the Iceberg table metadata, pointing to the root location of the Lance table.</p> <p>Table properties are stored in the Iceberg table's <code>properties</code> map.</p>"},{"location":"format/namespace/integrations/iceberg/#lance-table-identification","title":"Lance Table Identification","text":"<p>A table in Iceberg REST Catalog is identified as a Lance table when the <code>properties</code> map contains a key <code>table_type</code> with value <code>lance</code> (case insensitive). The <code>location</code> must point to a valid Lance table root directory. The Iceberg table itself serves as a metadata wrapper, with the actual data stored in Lance format.</p>"},{"location":"format/namespace/integrations/iceberg/#basic-operations","title":"Basic Operations","text":""},{"location":"format/namespace/integrations/iceberg/#createnamespace","title":"CreateNamespace","text":"<p>Creates a new namespace in the Iceberg catalog.</p> <p>The implementation:</p> <ol> <li>Extract the warehouse from the first element of the namespace identifier</li> <li>Resolve the API prefix from the warehouse config cache</li> <li>Extract the namespace path from the remaining elements</li> <li>Construct a CreateNamespaceRequest with the namespace array and properties</li> <li>POST to <code>/v1/{prefix}/namespaces</code> endpoint</li> <li>Return the created namespace properties</li> </ol> <p>Error Handling:</p> <p>If the namespace already exists, return error code <code>2</code> (NamespaceAlreadyExists).</p> <p>If the parent namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the server returns an error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/integrations/iceberg/#listnamespaces","title":"ListNamespaces","text":"<p>Lists child namespaces under a given parent namespace.</p> <p>The implementation:</p> <ol> <li>Extract the warehouse from the first element of the namespace identifier</li> <li>Resolve the API prefix from the warehouse config cache</li> <li>Extract the parent namespace path from the remaining elements (if any)</li> <li>GET <code>/v1/{prefix}/namespaces</code> with <code>parent</code> query parameter</li> <li>Extract namespace names from the response</li> </ol> <p>Error Handling:</p> <p>If the parent namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the server returns an error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/integrations/iceberg/#describenamespace","title":"DescribeNamespace","text":"<p>Retrieves properties and metadata for a namespace.</p> <p>The implementation:</p> <ol> <li>Extract the warehouse from the first element of the namespace identifier</li> <li>Resolve the API prefix from the warehouse config cache</li> <li>Extract the namespace path from the remaining elements</li> <li>GET <code>/v1/{prefix}/namespaces/{namespace}</code> with URL-encoded namespace path</li> <li>Return the namespace properties</li> </ol> <p>Error Handling:</p> <p>If the namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the server returns an error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/integrations/iceberg/#dropnamespace","title":"DropNamespace","text":"<p>Removes a namespace from the Iceberg catalog. Only RESTRICT mode is supported; CASCADE mode is not implemented.</p> <p>The implementation:</p> <ol> <li>Extract the warehouse from the first element of the namespace identifier</li> <li>Resolve the API prefix from the warehouse config cache</li> <li>Extract the namespace path from the remaining elements</li> <li>DELETE <code>/v1/{prefix}/namespaces/{namespace}</code> with URL-encoded namespace path</li> </ol> <p>Error Handling:</p> <p>If the namespace does not exist, the operation succeeds (idempotent behavior).</p> <p>If the namespace is not empty, return error code <code>3</code> (NamespaceNotEmpty).</p> <p>If the server returns an error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/integrations/iceberg/#declaretable","title":"DeclareTable","text":"<p>Declares a new Lance table in the Iceberg catalog without creating the underlying data.</p> <p>The implementation:</p> <ol> <li>Extract the warehouse from the first element of the table identifier</li> <li>Resolve the API prefix from the warehouse config cache</li> <li>Extract the namespace path from the middle elements</li> <li>Extract the table name from the last element</li> <li>Construct a CreateTableRequest with:<ul> <li><code>name</code>: the table name</li> <li><code>location</code>: the specified or default location (defaults to <code>{root}/{warehouse}/{namespace}/{table_name}</code>)</li> <li><code>schema</code>: a dummy Iceberg schema with a single nullable string column <code>dummy</code></li> <li><code>properties</code>: table properties including <code>table_type=lance</code></li> </ul> </li> <li>POST to <code>/v1/{prefix}/namespaces/{namespace}/tables</code></li> <li>Return the declared table location</li> </ol> <p>Error Handling:</p> <p>If the parent namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the table already exists, return error code <code>5</code> (TableAlreadyExists).</p> <p>If the server returns an error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/integrations/iceberg/#listtables","title":"ListTables","text":"<p>Lists all Lance tables in a namespace.</p> <p>The implementation:</p> <ol> <li>Extract the warehouse from the first element of the namespace identifier</li> <li>Resolve the API prefix from the warehouse config cache</li> <li>Extract the namespace path from the remaining elements</li> <li>GET <code>/v1/{prefix}/namespaces/{namespace}/tables</code></li> <li>For each table, load its metadata and filter tables where <code>properties.table_type=lance</code></li> <li>Extract table names from the response identifiers</li> </ol> <p>Error Handling:</p> <p>If the namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the server returns an error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/integrations/iceberg/#describetable","title":"DescribeTable","text":"<p>Retrieves metadata for a Lance table. Only <code>load_detailed_metadata=false</code> is supported. When <code>load_detailed_metadata=false</code>, only the table location and storage_options are returned; other fields (version, table_uri, schema, stats) are null.</p> <p>The implementation:</p> <ol> <li>Extract the warehouse from the first element of the table identifier</li> <li>Resolve the API prefix from the warehouse config cache</li> <li>Extract the namespace path from the middle elements</li> <li>Extract the table name from the last element</li> <li>GET <code>/v1/{prefix}/namespaces/{namespace}/tables/{table}</code></li> <li>Verify the table has <code>table_type=lance</code> property</li> <li>Return the table location and storage_options from <code>properties</code></li> </ol> <p>Error Handling:</p> <p>If the table does not exist, return error code <code>4</code> (TableNotFound).</p> <p>If the table is not a Lance table, return error code <code>13</code> (InvalidInput).</p> <p>If the server returns an error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/integrations/iceberg/#deregistertable","title":"DeregisterTable","text":"<p>Removes a Lance table registration from the Iceberg catalog without deleting the underlying data.</p> <p>The implementation:</p> <ol> <li>Extract the warehouse from the first element of the table identifier</li> <li>Resolve the API prefix from the warehouse config cache</li> <li>Extract the namespace path from the middle elements</li> <li>Extract the table name from the last element</li> <li>DELETE <code>/v1/{prefix}/namespaces/{namespace}/tables/{table}?purgeRequested=false</code></li> </ol> <p>Error Handling:</p> <p>If the table does not exist, return error code <code>4</code> (TableNotFound).</p> <p>If the server returns an error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/integrations/onelake/","title":"Azure OneLake Table API","text":"<p>Microsoft OneLake is a unified, logical data lake for Microsoft Fabric that provides a single SaaS experience and a tenant-wide store for data that serves both professional and citizen data integration needs.</p> <p>Lance at this moment does not provide a dedicated OneLake namespace implementation. However, OneLake provides a Unity Catalog-compatible endpoint through its Table APIs, which allows you to use OneLake through the Lance Unity Namespace.</p>"},{"location":"format/namespace/integrations/onelake/#using-onelake-with-lance","title":"Using OneLake with Lance","text":"<p>OneLake provides two catalog-compatible endpoints through its Table APIs, allowing you to use OneLake with Lance via either the Unity Namespace or the Iceberg REST Catalog Namespace.</p>"},{"location":"format/namespace/integrations/onelake/#option-1-unity-catalog-endpoint","title":"Option 1: Unity Catalog Endpoint","text":"<p>To use Microsoft OneLake with Lance via Unity Catalog, leverage OneLake's Delta Lake REST API, which exposes a Unity Catalog-compatible interface.</p> <p>Configure your Lance Unity namespace to connect to OneLake's Unity Catalog endpoint at:</p> <pre><code>https://onelake.table.fabric.microsoft.com/delta\n</code></pre> <p>All the features and configurations of the Lance Unity Namespace apply when using OneLake.</p>"},{"location":"format/namespace/integrations/onelake/#option-2-iceberg-rest-catalog-endpoint","title":"Option 2: Iceberg REST Catalog Endpoint","text":"<p>OneLake also provides an Iceberg REST Catalog API, which allows you to use OneLake through the Lance Iceberg REST Catalog Namespace.</p> <p>Configure your Lance Iceberg REST Catalog namespace to connect to OneLake's Iceberg endpoint at:</p> <pre><code>https://onelake.table.fabric.microsoft.com/iceberg\n</code></pre> <p>All the features and configurations of the Lance Iceberg REST Catalog Namespace apply when using OneLake.</p>"},{"location":"format/namespace/integrations/polaris/","title":"Apache Polaris Lance Namespace Implementation Spec","text":"<p>This document describes how the Polaris Catalog implements the Lance Namespace client spec.</p>"},{"location":"format/namespace/integrations/polaris/#background","title":"Background","text":"<p>Apache Polaris is an open-source catalog implementation for Apache Iceberg that provides a REST API for managing tables and namespaces.  Polaris supports the Generic Table API which allows registering non-Iceberg table formats.  For details on Polaris Catalog, see the Polaris Catalog Documentation.</p>"},{"location":"format/namespace/integrations/polaris/#namespace-implementation-configuration-properties","title":"Namespace Implementation Configuration Properties","text":"<p>The Lance Polaris namespace implementation accepts the following configuration properties:</p> <p>The endpoint property is required and specifies the Polaris server endpoint URL (e.g., <code>http://localhost:8181</code>). Must start with <code>http://</code> or <code>https://</code>.</p> <p>The auth_token property is optional and specifies the bearer token for authentication.</p> <p>The connect_timeout property is optional and specifies the connection timeout in milliseconds. Default value is <code>10000</code> (10 seconds).</p> <p>The read_timeout property is optional and specifies the read timeout in milliseconds. Default value is <code>30000</code> (30 seconds).</p> <p>The max_retries property is optional and specifies the maximum number of retries for failed requests. Default value is <code>3</code>.</p>"},{"location":"format/namespace/integrations/polaris/#object-mapping","title":"Object Mapping","text":""},{"location":"format/namespace/integrations/polaris/#namespace","title":"Namespace","text":"<p>The root namespace (empty identifier) represents the Polaris server itself.</p> <p>The catalog is the first level of the namespace hierarchy. It determines which Polaris catalog the operations target. A single-element identifier (e.g., <code>[\"my-catalog\"]</code>) lists all top-level namespaces under that catalog.</p> <p>A child namespace is a nested namespace in Polaris. Polaris supports arbitrary nesting depth. The namespace identifier format is <code>[catalog, namespace1, namespace2, ...]</code> (e.g., <code>[\"my-catalog\", \"schema\", \"subschema\"]</code>).</p> <p>For API calls, namespace levels (excluding the catalog) are joined with the <code>.</code> delimiter. The catalog is used in the API path as <code>/api/catalog/v1/{catalog}/namespaces</code>.</p> <p>Namespace properties are stored in the namespace's properties map, returned by the Polaris namespace API.</p>"},{"location":"format/namespace/integrations/polaris/#table","title":"Table","text":"<p>A table is represented as a Generic Table object in Polaris with <code>format</code> set to <code>lance</code>.</p> <p>The table identifier format is <code>[catalog, namespace1, namespace2, ..., table_name]</code> (e.g., <code>[\"my-catalog\", \"schema\", \"my_table\"]</code>).</p> <p>The table location is stored in the <code>base-location</code> field of the Generic Table, pointing to the root location of the Lance table.</p> <p>Table properties are stored in the Generic Table's <code>properties</code> map. An optional <code>doc</code> field can store a table description.</p>"},{"location":"format/namespace/integrations/polaris/#lance-table-identification","title":"Lance Table Identification","text":"<p>A table in Polaris is identified as a Lance table when it is a Generic Table with <code>format</code> set to <code>lance</code>. The <code>base-location</code> must point to a valid Lance table root directory. The table <code>properties</code> should contain <code>table_type=lance</code> for consistency with other catalog implementations.</p>"},{"location":"format/namespace/integrations/polaris/#basic-operations","title":"Basic Operations","text":""},{"location":"format/namespace/integrations/polaris/#createnamespace","title":"CreateNamespace","text":"<p>Creates a new namespace in Polaris.</p> <p>The implementation:</p> <ol> <li>Parse the namespace identifier to extract the catalog (first level) and namespace levels</li> <li>Validate that at least 2 levels are provided (catalog + namespace)</li> <li>Construct a CreateNamespaceRequest with the namespace array and properties</li> <li>POST to <code>/api/catalog/v1/{catalog}/namespaces</code> endpoint</li> <li>Return the created namespace properties</li> </ol> <p>Error Handling:</p> <p>If the namespace already exists, return error code <code>2</code> (NamespaceAlreadyExists).</p> <p>If the parent namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the server returns an error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/integrations/polaris/#listnamespaces","title":"ListNamespaces","text":"<p>Lists child namespaces under a given parent namespace.</p> <p>The implementation:</p> <ol> <li>Parse the parent namespace identifier to extract the catalog (first level)</li> <li>Validate that at least 1 level (catalog) is provided</li> <li>For catalog-level listing: GET <code>/api/catalog/v1/{catalog}/namespaces</code></li> <li>For nested namespace listing: GET <code>/api/catalog/v1/{catalog}/namespaces/{parent}/namespaces</code></li> <li>Convert the response namespace arrays to dot-separated strings, prefixing with the catalog name</li> </ol> <p>Error Handling:</p> <p>If the parent namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the server returns an error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/integrations/polaris/#describenamespace","title":"DescribeNamespace","text":"<p>Retrieves properties and metadata for a namespace.</p> <p>The implementation:</p> <ol> <li>Parse the namespace identifier to extract the catalog (first level) and namespace path</li> <li>Validate that at least 2 levels are provided (catalog + namespace)</li> <li>GET <code>/api/catalog/v1/{catalog}/namespaces/{namespace}</code> with URL-encoded namespace path</li> <li>Return the namespace properties</li> </ol> <p>Error Handling:</p> <p>If the namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the server returns an error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/integrations/polaris/#dropnamespace","title":"DropNamespace","text":"<p>Removes a namespace from Polaris. Only RESTRICT mode is supported; CASCADE mode is not implemented.</p> <p>The implementation:</p> <ol> <li>Parse the namespace identifier to extract the catalog (first level) and namespace path</li> <li>Validate that at least 2 levels are provided (catalog + namespace)</li> <li>DELETE <code>/api/catalog/v1/{catalog}/namespaces/{namespace}</code> with URL-encoded namespace path</li> </ol> <p>Error Handling:</p> <p>If the namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the namespace is not empty, return error code <code>3</code> (NamespaceNotEmpty).</p> <p>If the server returns an error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/integrations/polaris/#declaretable","title":"DeclareTable","text":"<p>Declares a new Lance table in Polaris without creating the underlying data.</p> <p>The implementation:</p> <ol> <li>Parse the table identifier to extract catalog (first level), namespace (middle levels), and table name (last level)</li> <li>Validate that at least 3 levels are provided (catalog + namespace + table)</li> <li>Construct a CreateGenericTableRequest with:<ul> <li><code>name</code>: the table name</li> <li><code>format</code>: <code>lance</code></li> <li><code>base-location</code>: the specified location</li> <li><code>doc</code>: optional description from properties</li> <li><code>properties</code>: table properties including <code>table_type=lance</code></li> </ul> </li> <li>POST to <code>/api/catalog/polaris/v1/{catalog}/namespaces/{namespace}/generic-tables</code></li> <li>Return the created table location and properties</li> </ol> <p>Error Handling:</p> <p>If the parent namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the table already exists, return error code <code>5</code> (TableAlreadyExists).</p> <p>If the server returns an error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/integrations/polaris/#listtables","title":"ListTables","text":"<p>Lists all Lance tables in a namespace.</p> <p>The implementation:</p> <ol> <li>Parse the namespace identifier to extract the catalog (first level) and namespace path</li> <li>Validate that at least 2 levels are provided (catalog + namespace)</li> <li>GET <code>/api/catalog/polaris/v1/{catalog}/namespaces/{namespace}/generic-tables</code></li> <li>Extract table names from the response identifiers</li> </ol> <p>Error Handling:</p> <p>If the namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the server returns an error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/integrations/polaris/#describetable","title":"DescribeTable","text":"<p>Retrieves metadata for a Lance table. Only <code>load_detailed_metadata=false</code> is supported. When <code>load_detailed_metadata=false</code>, only the table location and storage_options are returned; other fields (version, table_uri, schema, stats) are null.</p> <p>The implementation:</p> <ol> <li>Parse the table identifier to extract catalog (first level), namespace (middle levels), and table name (last level)</li> <li>Validate that at least 3 levels are provided (catalog + namespace + table)</li> <li>GET <code>/api/catalog/polaris/v1/{catalog}/namespaces/{namespace}/generic-tables/{table}</code></li> <li>Verify the table format is <code>lance</code></li> <li>Return the table location from <code>base-location</code> and storage_options from <code>properties</code></li> </ol> <p>Error Handling:</p> <p>If the table does not exist, return error code <code>4</code> (TableNotFound).</p> <p>If the table format is not <code>lance</code>, return error code <code>13</code> (InvalidInput).</p> <p>If the server returns an error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/integrations/polaris/#deregistertable","title":"DeregisterTable","text":"<p>Removes a Lance table registration from Polaris without deleting the underlying data.</p> <p>The implementation:</p> <ol> <li>Parse the table identifier to extract catalog (first level), namespace (middle levels), and table name (last level)</li> <li>Validate that at least 3 levels are provided (catalog + namespace + table)</li> <li>DELETE <code>/api/catalog/polaris/v1/{catalog}/namespaces/{namespace}/generic-tables/{table}</code></li> </ol> <p>Error Handling:</p> <p>If the table does not exist, return error code <code>4</code> (TableNotFound).</p> <p>If the server returns an error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/integrations/template/","title":"Lance Namespace Implementation Spec Template","text":"<p>This template defines the standard structure for Lance Namespace implementation specs.  Each implementation spec describes how a specific catalog system integrates with the Lance Namespace client spec.</p>"},{"location":"format/namespace/integrations/template/#required-sections","title":"Required Sections","text":""},{"location":"format/namespace/integrations/template/#1-background","title":"1. Background","text":"<p>Provide a brief introduction to the catalog system being integrated:</p> <ul> <li>What the catalog system is and its purpose</li> <li>Link to the catalog spec for detailed design information</li> <li>Any important context for understanding the implementation</li> </ul>"},{"location":"format/namespace/integrations/template/#2-namespace-implementation-configuration-properties","title":"2. Namespace Implementation Configuration Properties","text":"<p>List all configuration properties accepted by the namespace implementation:</p> <ul> <li>Required vs optional properties</li> <li>Property descriptions and default values</li> <li>Prefix-based properties (e.g., <code>storage.*</code>, <code>headers.*</code>)</li> </ul>"},{"location":"format/namespace/integrations/template/#3-object-mapping","title":"3. Object Mapping","text":"<p>Describe how objects in the catalog system map to Lance Namespace concepts using paragraphs (not tables):</p> <p>Namespace Mapping: - How the root namespace is represented - How child namespaces are organized - How namespace identifiers are constructed - Where namespace properties are stored</p> <p>Table Mapping: - How tables are represented - How table identifiers are constructed - Where table data is stored (location) - Where table properties are stored</p>"},{"location":"format/namespace/integrations/template/#4-lance-table-identification","title":"4. Lance Table Identification","text":"<p>Describe how to determine if a table in the catalog is a Lance table:</p> <ul> <li>Required properties, markers, or naming conventions</li> <li>Storage location requirements</li> <li>How the implementation verifies table validity</li> </ul>"},{"location":"format/namespace/integrations/template/#5-basic-operations","title":"5. Basic Operations","text":"<p>For each of the 8 recommended basic operations, provide a detailed subsection. Each operation subsection should include:</p> <ul> <li>A brief description of what the operation does</li> <li>Step-by-step implementation details</li> <li>An Error Handling paragraph describing which errors can occur and under what conditions</li> </ul> <p>The 8 basic operations are:</p> <p>Namespace Operations:</p> <ul> <li>CreateNamespace</li> <li>ListNamespaces</li> <li>DescribeNamespace</li> <li>DropNamespace (only <code>Restrict</code> behavior mode required)</li> </ul> <p>Table Operations:</p> <ul> <li>DeclareTable</li> <li>ListTables</li> <li>DescribeTable (only <code>load_detailed_metadata=false</code> required)</li> <li>DeregisterTable</li> </ul> <p>Note: For basic implementations, DropNamespace only needs to support the <code>Restrict</code> behavior mode (namespace must be empty before dropping). DescribeTable only needs to support <code>load_detailed_metadata=false</code> (only return table <code>location</code> without opening the dataset).</p>"},{"location":"format/namespace/integrations/template/#template-structure","title":"Template Structure","text":"<pre><code># Lance {Catalog Name} Namespace Implementation Spec\n\nThis document describes how the {Catalog Name} implements the Lance Namespace client spec.\n\n## Background\n\n{Brief description of the catalog system and its purpose}. For details on the catalog design, see the [{Catalog Name} Catalog Spec](link-to-the-spec).\n\n## Namespace Implementation Configuration Properties\n\nThe Lance {Catalog Name} namespace implementation accepts the following configuration properties:\n\nThe **{property_name}** property is {required/optional} and {description}. {Default value if optional}.\n\n{Additional properties...}\n\n## Object Mapping\n\n### Namespace\n\nThe **root namespace** is {description of how root namespace maps}.\n\nA **child namespace** is {description of child namespace representation}.\n\nThe **namespace identifier** is {description of identifier format}.\n\n**Namespace properties** are {description of where/how properties are stored}.\n\n### Table\n\nA **table** is {description of table representation}.\n\nThe **table identifier** is {description of identifier format}.\n\nThe **table location** is {description of where table data is stored}.\n\n**Table properties** are {description of where/how properties are stored}.\n\n## Lance Table Identification\n\n{Paragraph describing how to identify a Lance table in this catalog system}\n\n## Basic Operations\n\n### CreateNamespace\n\n{Brief description of operation}\n\nThe implementation:\n\n1. {Step 1}\n2. {Step 2}\n3. {Step N}\n\n**Error Handling:**\n\nIf {condition}, return error code `N` ({ErrorName}).\n\n{Additional error conditions...}\n\n### ListNamespaces\n\n{Same structure as above}\n\n### DescribeNamespace\n\n{Same structure as above}\n\n### DropNamespace\n\n{Same structure as above}\n\n**Note:** Basic implementations only need to support `Restrict` behavior mode.\n\n### DeclareTable\n\n{Same structure as above}\n\n### ListTables\n\n{Same structure as above}\n\n### DescribeTable\n\n{Same structure as above}\n\n**Note:** Basic implementations only need to support `load_detailed_metadata=false` (only return table `location`).\n\n### DeregisterTable\n\n{Same structure as above}\n</code></pre>"},{"location":"format/namespace/integrations/template/#error-handling-guidelines","title":"Error Handling Guidelines","text":"<p>Each operation's error handling section should describe errors in paragraph form, one paragraph per error condition. Include:</p> <ul> <li>The condition that triggers the error</li> <li>The error code number</li> <li>The error name in parentheses</li> </ul> <p>Example:</p> <p>If the namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the namespace contains tables or child namespaces, return error code <code>3</code> (NamespaceNotEmpty).</p> <p>For catalog specs that map to HTTP (like REST), also include the HTTP status code:</p> <p>If the namespace does not exist, return HTTP <code>404 Not Found</code> with error code <code>1</code> (NamespaceNotFound).</p>"},{"location":"format/namespace/integrations/unity/","title":"Unity Catalog Lance Namespace Implementation Spec","text":"<p>This document describes how the Unity Catalog implements the Lance Namespace client spec.</p>"},{"location":"format/namespace/integrations/unity/#background","title":"Background","text":"<p>Unity Catalog is an open-source data catalog that provides unified governance for data and AI assets. It supports external tables and can be used to manage Lance tables through its REST API. For details on Unity Catalog, see the Unity Catalog Documentation.</p>"},{"location":"format/namespace/integrations/unity/#namespace-implementation-configuration-properties","title":"Namespace Implementation Configuration Properties","text":"<p>The Lance Unity namespace implementation accepts the following configuration properties:</p> <p>The endpoint property is required and specifies the Unity Catalog REST API endpoint (e.g., <code>http://localhost:8080</code>).</p> <p>The auth_token property is optional and specifies the bearer token for authentication.</p> <p>The connect_timeout property is optional and specifies the HTTP connection timeout in milliseconds. Default value is <code>10000</code> (10 seconds).</p> <p>The read_timeout property is optional and specifies the HTTP read timeout in milliseconds. Default value is <code>300000</code> (5 minutes).</p> <p>The max_retries property is optional and specifies the maximum number of retries for failed requests. Default value is <code>3</code>.</p> <p>The root property is optional and specifies the storage root location for new tables. Default value is <code>/tmp/lance</code>.</p>"},{"location":"format/namespace/integrations/unity/#object-mapping","title":"Object Mapping","text":""},{"location":"format/namespace/integrations/unity/#namespace","title":"Namespace","text":"<p>The root namespace (empty identifier) represents the Unity Catalog server itself.</p> <p>The catalog is the first level of the namespace hierarchy. A single-element identifier (e.g., <code>[\"my-catalog\"]</code>) lists all schemas within that catalog.</p> <p>A schema is the second level of the namespace hierarchy. Unity supports a fixed 3-level hierarchy: catalog.schema.table. The namespace identifier format is <code>[catalog, schema]</code> (e.g., <code>[\"my-catalog\", \"my_schema\"]</code>).</p> <p>Namespace properties are stored in the Unity schema's properties map.</p>"},{"location":"format/namespace/integrations/unity/#table","title":"Table","text":"<p>A table is represented as a Table object in Unity Catalog with <code>table_type</code> set to <code>EXTERNAL</code>.</p> <p>The table identifier is constructed by joining catalog, schema, and table name (e.g., <code>catalog.schema.table</code>).</p> <p>The table location is stored in the <code>storage_location</code> field of the Unity Table, pointing to the root location of the Lance table.</p> <p>Table properties are stored in the Unity Table's <code>properties</code> map. The <code>columns</code> field stores the table schema converted from Lance's Arrow schema to Unity's column format.</p>"},{"location":"format/namespace/integrations/unity/#lance-table-identification","title":"Lance Table Identification","text":"<p>A table in Unity Catalog is identified as a Lance table when it meets the following criteria: the <code>table_type</code> is <code>EXTERNAL</code>, and the <code>properties</code> map contains a key <code>table_type</code> with value <code>lance</code> (case insensitive). The <code>storage_location</code> must point to a valid Lance table root directory.</p> <p>Note: Unity Catalog does not natively recognize the <code>LANCE</code> data source format, so <code>data_source_format</code> is set to <code>TEXT</code> as a generic format for external tables. The actual format is determined by the <code>table_type=lance</code> property.</p>"},{"location":"format/namespace/integrations/unity/#basic-operations","title":"Basic Operations","text":""},{"location":"format/namespace/integrations/unity/#createnamespace","title":"CreateNamespace","text":"<p>Creates a new schema in Unity Catalog.</p> <p>The implementation:</p> <ol> <li>Parse the namespace identifier (must be 2-level: catalog.schema)</li> <li>Construct a CreateSchema request with name, catalog name, and properties</li> <li>POST to <code>/schemas</code> endpoint</li> <li>Return the created schema properties</li> </ol> <p>Error Handling:</p> <p>If the schema already exists, return error code <code>2</code> (NamespaceAlreadyExists).</p> <p>If the server returns an error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/integrations/unity/#listnamespaces","title":"ListNamespaces","text":"<p>Lists catalogs or schemas in the Unity Catalog.</p> <p>The implementation:</p> <ol> <li>Parse the parent namespace identifier</li> <li>For root namespace (level 0): GET <code>/catalogs</code> to list all available catalogs</li> <li>For catalog namespace (level 1): GET <code>/schemas</code> with catalog_name parameter to list schemas</li> <li>Sort the results</li> </ol> <p>Error Handling:</p> <p>If the catalog does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the server returns an error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/integrations/unity/#describenamespace","title":"DescribeNamespace","text":"<p>Retrieves properties and metadata for a schema.</p> <p>The implementation:</p> <ol> <li>Parse the namespace identifier (must be 2-level: catalog.schema)</li> <li>GET <code>/schemas/{catalog}.{schema}</code></li> <li>Return the schema properties</li> </ol> <p>Error Handling:</p> <p>If the namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the server returns an error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/integrations/unity/#dropnamespace","title":"DropNamespace","text":"<p>Removes a schema from Unity Catalog. Only RESTRICT mode is supported; CASCADE mode is not implemented.</p> <p>The implementation:</p> <ol> <li>Parse the namespace identifier (must be 2-level: catalog.schema)</li> <li>DELETE <code>/schemas/{catalog}.{schema}</code></li> </ol> <p>Error Handling:</p> <p>If the namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the namespace is not empty, return error code <code>3</code> (NamespaceNotEmpty).</p> <p>If the server returns an error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/integrations/unity/#declaretable","title":"DeclareTable","text":"<p>Declares a new Lance table in Unity Catalog without creating the underlying data.</p> <p>The implementation:</p> <ol> <li>Parse the table identifier (must be 3-level: catalog.schema.table)</li> <li>Construct a CreateTable request with:<ul> <li><code>name</code>: the table name</li> <li><code>catalog_name</code>: the catalog</li> <li><code>schema_name</code>: the schema</li> <li><code>table_type</code>: <code>EXTERNAL</code></li> <li><code>data_source_format</code>: <code>TEXT</code></li> <li><code>storage_location</code>: the specified or default location</li> <li><code>properties</code>: including <code>table_type=lance</code></li> </ul> </li> <li>POST to <code>/tables</code> endpoint</li> <li>Return the created table location and properties</li> </ol> <p>Error Handling:</p> <p>If the parent namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the table already exists, return error code <code>5</code> (TableAlreadyExists).</p> <p>If the server returns an error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/integrations/unity/#listtables","title":"ListTables","text":"<p>Lists all Lance tables in a schema.</p> <p>The implementation:</p> <ol> <li>Parse the namespace identifier (must be 2-level: catalog.schema)</li> <li>GET <code>/tables</code> with catalog_name and schema_name parameters</li> <li>Filter tables where <code>properties.table_type=lance</code></li> <li>Sort the results</li> </ol> <p>Error Handling:</p> <p>If the namespace does not exist, return error code <code>1</code> (NamespaceNotFound).</p> <p>If the server returns an error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/integrations/unity/#describetable","title":"DescribeTable","text":"<p>Retrieves metadata for a Lance table. Only <code>load_detailed_metadata=false</code> is supported. When <code>load_detailed_metadata=false</code>, only the table location and storage_options are returned; other fields (version, table_uri, schema, stats) are null.</p> <p>The implementation:</p> <ol> <li>Parse the table identifier (must be 3-level: catalog.schema.table)</li> <li>GET <code>/tables/{catalog}.{schema}.{table}</code></li> <li>Verify the table is a Lance table (check <code>properties.table_type=lance</code>)</li> <li>Return the table location from <code>storage_location</code> and storage_options from <code>properties</code></li> </ol> <p>Error Handling:</p> <p>If the table does not exist, return error code <code>4</code> (TableNotFound).</p> <p>If the table is not a Lance table, return error code <code>13</code> (InvalidInput).</p> <p>If the server returns an error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/integrations/unity/#deregistertable","title":"DeregisterTable","text":"<p>Removes a Lance table registration from Unity Catalog without deleting the underlying data.</p> <p>The implementation:</p> <ol> <li>Parse the table identifier (must be 3-level: catalog.schema.table)</li> <li>GET the table and verify it is a Lance table</li> <li>DELETE <code>/tables/{catalog}.{schema}.{table}</code></li> </ol> <p>Error Handling:</p> <p>If the table does not exist, return error code <code>4</code> (TableNotFound).</p> <p>If the table is not a Lance table, return error code <code>13</code> (InvalidInput).</p> <p>If the server returns an error, return error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/rest/catalog-spec/","title":"Lance REST Namespace Catalog Spec","text":"<p>In an enterprise environment, typically there is a requirement to store tables in a metadata service for more advanced governance features around access control, auditing, lineage tracking, etc. Lance REST Namespace is an OpenAPI catalog protocol that enables reading, writing and managing Lance tables by connecting those metadata services or building a custom metadata server in a standardized way. The REST server definition can be found in the OpenAPI specification.</p>"},{"location":"format/namespace/rest/catalog-spec/#duality-with-client-side-access-spec","title":"Duality with Client-Side Access Spec","text":"<p>The Lance Namespace client-side access spec defines request and response models using OpenAPI. The REST namespace spec leverages this fact \u2014 the REST API is largely identical to the client-side access spec, with the request and response schemas directly used as HTTP request and response bodies.</p> <p>This duality minimizes data conversion between client and server: a client can serialize its request model directly to JSON for the HTTP body, and deserialize the HTTP response body directly into the response model.</p> <p>There are a few exceptions where the REST spec diverges from the client-side access spec. For example, for some operations like <code>InsertIntoTable</code>, <code>CreateTable</code>, <code>MergeInsertIntoTable</code>, the HTTP request body is used for transmitting Arrow IPC binary data, and the operation request fields are transmitted through query parameters instead. For some list operations like <code>ListNamespaces</code> and <code>ListTables</code>, pagination tokens and limits may be passed as query parameters for easier URL construction and caching.</p> <p>These non-standard operations are documented in the Non-Standard Operations section below.</p>"},{"location":"format/namespace/rest/catalog-spec/#rest-routes","title":"REST Routes","text":"<p>The REST route for an operation typically follows the pattern of <code>POST /&lt;version&gt;/&lt;object&gt;/{id}/&lt;action&gt;</code>, for example <code>POST /v1/namespace/{id}/list</code> for <code>ListNamespace</code>. The request and response schemas are used as the actual request and response of the route.</p> <p>The key design principle of the REST route is that all the necessary information for a reverse proxy (e.g. load balancing, authN, authZ) should be available for access without the need to deserialize request body. For example, the route for <code>CreateTable</code> is <code>POST /v1/table/{id}/create</code> instead of <code>POST /v1/table</code> so that the table identifier is visible to the reverse proxy without parsing the request body.</p>"},{"location":"format/namespace/rest/catalog-spec/#standard-operations","title":"Standard Operations","text":"<p>Standard operations should take the same request and return the same response as any other implementation.</p> <p>The information in the route could also present in the request body. When the information in the route and request body both present but do not match, the server must throw a 400 Bad Request error. When the information in the request body is missing, the server must use the information in the route instead.</p>"},{"location":"format/namespace/rest/catalog-spec/#identity-header-mapping","title":"Identity Header Mapping","text":"<p>All request schemas include an optional <code>identity</code> field for authentication. For REST Namespace, the identity fields are mapped to HTTP headers:</p> Identity Field REST Form Location <code>api_key</code> <code>x-api-key</code> Header <code>auth_token</code> <code>Authorization</code> Header <p>The <code>auth_token</code> is sent using the Bearer scheme (e.g., <code>Authorization: Bearer &lt;token&gt;</code>).</p> <p>When identity information is provided in both the request body and headers, the header values take precedence.</p>"},{"location":"format/namespace/rest/catalog-spec/#context-header-mapping","title":"Context Header Mapping","text":"<p>All request schemas include an optional <code>context</code> field for passing arbitrary key-value pairs. This allows clients to send implementation-specific context that can be used by the server or forwarded to downstream services.</p> <p>For REST Namespace, context entries are mapped to HTTP headers using the naming convention:</p> Context Entry REST Form Location <code>{\"&lt;key&gt;\": \"&lt;value&gt;\"}</code> <code>x-lance-ctx-&lt;key&gt;</code> Header <p>For example, a context entry <code>{\"trace_id\": \"abc123\", \"user_region\": \"us-west\"}</code> would be sent as:</p> <pre><code>x-lance-ctx-trace_id: abc123\nx-lance-ctx-user_region: us-west\n</code></pre> <p>How to use the context is custom to the specific implementation. Common use cases include:</p> <ul> <li>Passing trace IDs for distributed tracing</li> <li>Forwarding user context to downstream services</li> <li>Providing hints to the implementation for optimization</li> </ul> <p>When context is provided in both the request body and headers, the header values take precedence.</p>"},{"location":"format/namespace/rest/catalog-spec/#non-standard-operations","title":"Non-Standard Operations","text":"<p>For request and response that cannot be simply described as a JSON object the REST server needs to perform special handling to describe equivalent information through path parameters, query parameters and headers.</p>"},{"location":"format/namespace/rest/catalog-spec/#listnamespaces","title":"ListNamespaces","text":"<p>Route: <code>GET /v1/namespace/{id}/list</code></p> <p>Uses GET without a request body. Pagination parameters are passed as query parameters.</p> Request Field REST Form Location <code>id</code> <code>{id}</code> Path parameter <code>page_token</code> <code>page_token</code> Query parameter <code>limit</code> <code>limit</code> Query parameter"},{"location":"format/namespace/rest/catalog-spec/#listtables","title":"ListTables","text":"<p>Route: <code>GET /v1/namespace/{id}/table/list</code></p> <p>Uses GET without a request body. Pagination parameters are passed as query parameters.</p> Request Field REST Form Location <code>id</code> <code>{id}</code> Path parameter <code>page_token</code> <code>page_token</code> Query parameter <code>limit</code> <code>limit</code> Query parameter"},{"location":"format/namespace/rest/catalog-spec/#listalltables","title":"ListAllTables","text":"<p>Route: <code>GET /v1/table/</code></p> <p>Uses GET without a request body. Pagination parameters are passed as query parameters.</p> Request Field REST Form Location <code>page_token</code> <code>page_token</code> Query parameter <code>limit</code> <code>limit</code> Query parameter <code>delimiter</code> <code>delimiter</code> Query parameter"},{"location":"format/namespace/rest/catalog-spec/#describetable","title":"DescribeTable","text":"<p>Route: <code>POST /v1/table/{id}/describe</code></p> <p>The <code>with_table_uri</code> field is passed as a query parameter instead of in the request body.</p> Request Field REST Form Location <code>id</code> <code>{id}</code> Path parameter <code>with_table_uri</code> <code>with_table_uri</code> Query parameter"},{"location":"format/namespace/rest/catalog-spec/#createtable","title":"CreateTable","text":"<p>Route: <code>POST /v1/table/{id}/create</code></p> <p>Content-Type: <code>application/vnd.apache.arrow.stream</code></p> <p>The request body contains Arrow IPC stream data. The table schema is derived from the Arrow stream schema. If the stream is empty, an empty table is created.</p> Request Field REST Form Location <code>id</code> <code>{id}</code> Path parameter <code>mode</code> <code>mode</code> Query parameter <code>location</code> <code>x-lance-table-location</code> Header <code>properties</code> <code>x-lance-table-properties</code> Header (JSON-encoded string map) <code>data</code> Request body Body (Arrow IPC stream)"},{"location":"format/namespace/rest/catalog-spec/#insertintotable","title":"InsertIntoTable","text":"<p>Route: <code>POST /v1/table/{id}/insert</code></p> <p>Content-Type: <code>application/vnd.apache.arrow.stream</code></p> <p>The request body contains Arrow IPC stream data with records to insert.</p> Request Field REST Form Location <code>id</code> <code>{id}</code> Path parameter <code>mode</code> <code>mode</code> Query parameter (<code>append</code> or <code>overwrite</code>, default: <code>append</code>) <code>data</code> Request body Body (Arrow IPC stream)"},{"location":"format/namespace/rest/catalog-spec/#mergeinsertintotable","title":"MergeInsertIntoTable","text":"<p>Route: <code>POST /v1/table/{id}/merge_insert</code></p> <p>Content-Type: <code>application/vnd.apache.arrow.stream</code></p> <p>The request body contains Arrow IPC stream data. Performs a merge insert (upsert) operation that updates existing rows based on a matching column and inserts new rows that don't match.</p> Request Field REST Form Location <code>id</code> <code>{id}</code> Path parameter <code>on</code> <code>on</code> Query parameter (required) <code>when_matched_update_all</code> <code>when_matched_update_all</code> Query parameter (boolean) <code>when_matched_update_all_filt</code> <code>when_matched_update_all_filt</code> Query parameter (SQL expression) <code>when_not_matched_insert_all</code> <code>when_not_matched_insert_all</code> Query parameter (boolean) <code>when_not_matched_by_source_delete</code> <code>when_not_matched_by_source_delete</code> Query parameter (boolean) <code>when_not_matched_by_source_delete_filt</code> <code>when_not_matched_by_source_delete_filt</code> Query parameter (SQL expression) <code>timeout</code> <code>timeout</code> Query parameter (duration string, e.g., \"30s\", \"5m\") <code>use_index</code> <code>use_index</code> Query parameter (boolean) <code>data</code> Request body Body (Arrow IPC stream)"},{"location":"format/namespace/rest/catalog-spec/#querytable","title":"QueryTable","text":"<p>Route: <code>POST /v1/table/{id}/query</code></p> <p>Response Content-Type: <code>application/vnd.apache.arrow.file</code></p> <p>The response body contains Arrow IPC file data instead of JSON.</p> Response Field REST Form Notes (results) Response body Arrow IPC file (binary, not JSON)"},{"location":"format/namespace/rest/catalog-spec/#counttablerows","title":"CountTableRows","text":"<p>Route: <code>POST /v1/table/{id}/count_rows</code></p> <p>The response is returned as a plain integer instead of a JSON object.</p> Response Field REST Form Notes (count) Response body Plain integer (not JSON wrapped)"},{"location":"format/namespace/rest/catalog-spec/#droptable","title":"DropTable","text":"<p>Route: <code>POST /v1/table/{id}/drop</code></p> <p>No request body. All parameters are in the path.</p>"},{"location":"format/namespace/rest/catalog-spec/#droptableindex","title":"DropTableIndex","text":"<p>Route: <code>POST /v1/table/{id}/index/{index_name}/drop</code></p> <p>No request body. All parameters are in the path.</p>"},{"location":"format/namespace/rest/catalog-spec/#listtableversions","title":"ListTableVersions","text":"<p>Route: <code>POST /v1/table/{id}/version/list</code></p> <p>No request body. Pagination parameters are passed as query parameters.</p> Request Field REST Form Location <code>id</code> <code>{id}</code> Path parameter <code>page_token</code> <code>page_token</code> Query parameter <code>limit</code> <code>limit</code> Query parameter"},{"location":"format/namespace/rest/catalog-spec/#listtabletags","title":"ListTableTags","text":"<p>Route: <code>POST /v1/table/{id}/tags/list</code></p> <p>No request body. Pagination parameters are passed as query parameters.</p> Request Field REST Form Location <code>id</code> <code>{id}</code> Path parameter <code>page_token</code> <code>page_token</code> Query parameter <code>limit</code> <code>limit</code> Query parameter"},{"location":"format/namespace/rest/catalog-spec/#explaintablequeryplan","title":"ExplainTableQueryPlan","text":"<p>Route: <code>POST /v1/table/{id}/explain_plan</code></p> <p>The response is returned as a plain string instead of a JSON object.</p> Request Field REST Form Location <code>id</code> <code>{id}</code> Path parameter <code>query</code> <code>query</code> Request body field <code>verbose</code> <code>verbose</code> Request body field Response Field REST Form Notes <code>plan</code> Response body Plain string (not JSON wrapped)"},{"location":"format/namespace/rest/catalog-spec/#analyzetablequeryplan","title":"AnalyzeTableQueryPlan","text":"<p>Route: <code>POST /v1/table/{id}/analyze_plan</code></p> <p>The response is returned as a plain string instead of a JSON object.</p> Request Field REST Form Location <code>id</code> <code>{id}</code> Path parameter <code>query</code> <code>query</code> Request body field Response Field REST Form Notes <code>analysis</code> Response body Plain string (not JSON wrapped)"},{"location":"format/namespace/rest/catalog-spec/#updatetableschemametadata","title":"UpdateTableSchemaMetadata","text":"<p>Route: <code>POST /v1/table/{id}/schema_metadata/update</code></p> <p>Both request and response bodies are direct objects (map of string to string) instead of being wrapped in a <code>metadata</code> field.</p> Request Field REST Form Location <code>id</code> <code>{id}</code> Path parameter <code>metadata</code> Request body Direct object <code>{\"key\": \"value\", ...}</code> (not <code>{\"metadata\": {...}}</code>) Response Field REST Form Notes <code>metadata</code> Response body Direct object <code>{\"key\": \"value\", ...}</code> (not <code>{\"metadata\": {...}}</code>)"},{"location":"format/namespace/rest/catalog-spec/#namespace-server-and-adapter","title":"Namespace Server and Adapter","text":"<p>Any REST HTTP server that implements this OpenAPI protocol is called a Lance Namespace server. If you are a metadata service provider that is building a custom implementation of Lance namespace, building a REST server gives you standardized integration to Lance without the need to worry about tool support and continuously distribute newer library versions compared to using an implementation.</p> <p>If the main purpose of this server is to be a proxy on top of an existing metadata service, converting back and forth between Lance REST API models and native API models of the metadata service, then this Lance namespace server is called a Lance Namespace adapter.</p>"},{"location":"format/namespace/rest/catalog-spec/#choosing-between-an-adapter-vs-an-implementation","title":"Choosing between an Adapter vs an Implementation","text":"<p>Any adapter can always be directly a Lance namespace implementation bypassing the REST server, and vise versa. In fact, an implementation is basically the backend of an adapter. For example, we natively support a Lance HMS Namespace implementation, as well as a Lance namespace adapter for HMS by using the HMS Namespace implementation to fulfill requests in the Lance REST server.</p> <p>If you are considering between a Lance namespace adapter vs implementation to build or use in your environment, here are some criteria to consider:</p> <ol> <li>Multi-Language Feasibility &amp; Maintenance Cost: If you want a single strategy that works across all Lance language bindings, an adapter is preferred.    Sometimes it is not even possible for an integration to go with the implementation approach since it cannot support all the languages.    Sometimes an integration is popular or important enough that it is viable to build an implementation and maintain one library per language.</li> <li>Tooling Support: each tool needs to declare the Lance namespace implementations it supports.    That means there will be a preference for tools to always support a REST namespace,    but it might not always support a specific implementation. This favors the adapter approach.</li> <li>Security: if you have security concerns about the adapter being a man-in-the-middle, you should choose an implementation</li> <li>Performance: after all, adapter adds one layer of indirection and is thus not the most performant solution.    If you are performance sensitive, you should choose an implementation</li> </ol>"},{"location":"format/namespace/rest/impl-spec/","title":"Lance REST Namespace Implementation Spec","text":"<p>This document describes how the Lance REST Namespace implements the Lance Namespace client spec.</p>"},{"location":"format/namespace/rest/impl-spec/#background","title":"Background","text":"<p>The Lance REST Namespace is a catalog that provides access to Lance tables via a REST API. For details on the API design, endpoints, and data models, see the REST Namespace Catalog Spec.</p>"},{"location":"format/namespace/rest/impl-spec/#namespace-implementation-configuration-properties","title":"Namespace Implementation Configuration Properties","text":"<p>The Lance REST namespace implementation accepts the following configuration properties:</p> <p>The uri property is required and specifies the URI endpoint for the REST API, for example <code>https://api.example.com/lance</code>.</p> <p>The delimiter property specifies the delimiter used to parse object string identifiers in REST routes. Defaults to <code>$</code>. Other examples include <code>::</code> or <code>__delim__</code>.</p> <p>Properties with the headers. prefix are passed as HTTP headers with every request to the REST server after removing the prefix. For example, <code>headers.Authorization</code> becomes the <code>Authorization</code> header. Common configurations include <code>headers.Authorization</code> for authentication tokens, <code>headers.X-API-Key</code> for API key authentication, and <code>headers.X-Request-ID</code> for request tracking.</p>"},{"location":"format/namespace/rest/impl-spec/#object-mapping","title":"Object Mapping","text":""},{"location":"format/namespace/rest/impl-spec/#namespace","title":"Namespace","text":"<p>The root namespace is represented by the delimiter character itself in REST routes (e.g., <code>$</code>). All REST API calls are made relative to the base URI.</p> <p>A child namespace is managed by the REST server and accessed via namespace routes. The server is responsible for storing and organizing namespace metadata.</p> <p>The namespace identifier is a list of strings representing the namespace path. For example, a namespace <code>[\"prod\", \"analytics\"]</code> is serialized to <code>prod$analytics</code> in the REST route path using the configured delimiter (default <code>$</code>).</p> <p>Namespace properties are managed by the REST server and accessed via the DescribeNamespace operation.</p>"},{"location":"format/namespace/rest/impl-spec/#table","title":"Table","text":"<p>A table is managed by the REST server. The server handles table storage, versioning, and metadata management.</p> <p>The table identifier is a list of strings representing the namespace path followed by the table name. For example, a table <code>[\"prod\", \"analytics\", \"users\"]</code> represents a table named <code>users</code> in namespace <code>[\"prod\", \"analytics\"]</code>. This is serialized to <code>prod$analytics$users</code> in the REST route path using the configured delimiter.</p> <p>The table location is managed by the REST server and returned in the DescribeTable response. This location points to where the Lance table data is stored (e.g., an S3 path).</p> <p>Table properties are managed by the REST server and accessed via table operations.</p>"},{"location":"format/namespace/rest/impl-spec/#lance-table-identification","title":"Lance Table Identification","text":"<p>In a REST Namespace, the server is responsible for managing Lance tables. The client identifies tables by their string identifier and delegates all table operations to the server.</p> <p>The server implementation must ensure that:</p> <ul> <li>Tables are stored as valid Lance table directories on the underlying storage</li> <li>The <code>location</code> field in DescribeTable response points to the Lance table root directory</li> <li>Table properties include any Lance-specific metadata required by the Lance SDK</li> </ul>"},{"location":"format/namespace/rest/impl-spec/#basic-operations","title":"Basic Operations","text":""},{"location":"format/namespace/rest/impl-spec/#createnamespace","title":"CreateNamespace","text":"<p>Creates a new namespace.</p> <p>HTTP Request:</p> <pre><code>POST /v1/namespace/{id}/create\nContent-Type: application/json\n</code></pre> <p>The request body contains optional namespace properties:</p> <pre><code>{\n  \"properties\": {\n    \"description\": \"Production analytics namespace\"\n  }\n}\n</code></pre> <p>The implementation:</p> <ol> <li>Parse the namespace identifier from the route path <code>{id}</code></li> <li>Validate the request body format</li> <li>Check if the parent namespace exists (for nested namespaces)</li> <li>Check if a namespace with this identifier already exists</li> <li>Create the namespace in the server's storage</li> <li>Return the created namespace details</li> </ol> <p>Response:</p> <pre><code>{\n  \"name\": \"analytics\",\n  \"properties\": {\n    \"description\": \"Production analytics namespace\"\n  }\n}\n</code></pre> <p>Error Handling:</p> <p>If the request body is malformed, return HTTP <code>400 Bad Request</code> with error code <code>13</code> (InvalidInput).</p> <p>If a namespace with the same identifier already exists, return HTTP <code>409 Conflict</code> with error code <code>2</code> (NamespaceAlreadyExists).</p> <p>If the parent namespace does not exist, return HTTP <code>404 Not Found</code> with error code <code>1</code> (NamespaceNotFound).</p>"},{"location":"format/namespace/rest/impl-spec/#listnamespaces","title":"ListNamespaces","text":"<p>Lists child namespaces within a parent namespace.</p> <p>HTTP Request:</p> <pre><code>GET /v1/namespace/{id}/list?page_token=xxx&amp;limit=100\n</code></pre> <p>The <code>page_token</code> and <code>limit</code> query parameters support pagination.</p> <p>The implementation:</p> <ol> <li>Parse the parent namespace identifier from the route path <code>{id}</code></li> <li>Validate the parent namespace exists</li> <li>Query the server's storage for child namespaces</li> <li>Apply pagination using <code>page_token</code> and <code>limit</code></li> <li>Return the list of namespace names</li> </ol> <p>Response:</p> <pre><code>{\n  \"namespaces\": [\"analytics\", \"ml\", \"reporting\"],\n  \"next_page_token\": \"abc123\"\n}\n</code></pre> <p>The <code>next_page_token</code> field is only present if there are more results.</p> <p>Error Handling:</p> <p>If the parent namespace does not exist, return HTTP <code>404 Not Found</code> with error code <code>1</code> (NamespaceNotFound).</p>"},{"location":"format/namespace/rest/impl-spec/#describenamespace","title":"DescribeNamespace","text":"<p>Returns namespace metadata.</p> <p>HTTP Request:</p> <pre><code>POST /v1/namespace/{id}/describe\nContent-Type: application/json\n</code></pre> <p>The request body is empty:</p> <pre><code>{}\n</code></pre> <p>The implementation:</p> <ol> <li>Parse the namespace identifier from the route path <code>{id}</code></li> <li>Look up the namespace in the server's storage</li> <li>Return the namespace name and properties</li> </ol> <p>Response:</p> <pre><code>{\n  \"name\": \"analytics\",\n  \"properties\": {\n    \"description\": \"Production analytics namespace\",\n    \"created_at\": \"2024-01-15T10:30:00Z\"\n  }\n}\n</code></pre> <p>Error Handling:</p> <p>If the namespace does not exist, return HTTP <code>404 Not Found</code> with error code <code>1</code> (NamespaceNotFound).</p>"},{"location":"format/namespace/rest/impl-spec/#dropnamespace","title":"DropNamespace","text":"<p>Removes a namespace.</p> <p>HTTP Request:</p> <pre><code>POST /v1/namespace/{id}/drop\nContent-Type: application/json\n</code></pre> <p>The request body is empty:</p> <pre><code>{}\n</code></pre> <p>The implementation:</p> <ol> <li>Parse the namespace identifier from the route path <code>{id}</code></li> <li>Check that the namespace exists</li> <li>Check that the namespace is empty (no child namespaces or tables)</li> <li>Delete the namespace from the server's storage</li> </ol> <p>Response:</p> <pre><code>{}\n</code></pre> <p>Error Handling:</p> <p>If the namespace does not exist, return HTTP <code>404 Not Found</code> with error code <code>1</code> (NamespaceNotFound).</p> <p>If the namespace contains tables or child namespaces, return HTTP <code>409 Conflict</code> with error code <code>3</code> (NamespaceNotEmpty).</p>"},{"location":"format/namespace/rest/impl-spec/#declaretable","title":"DeclareTable","text":"<p>Declares a new Lance table, reserving the table name and location without creating actual data files.</p> <p>HTTP Request:</p> <pre><code>POST /v1/table/{id}/declare\nContent-Type: application/json\n</code></pre> <p>The request body contains an optional location:</p> <pre><code>{\n  \"location\": \"s3://bucket/data/users.lance\"\n}\n</code></pre> <p>The implementation:</p> <ol> <li>Parse the table identifier from the route path <code>{id}</code></li> <li>Extract the parent namespace from the identifier</li> <li>Validate the parent namespace exists</li> <li>Check if a table with this identifier already exists</li> <li>Determine the table location (use provided location or generate one)</li> <li>Reserve the table in the server's storage</li> <li>Register the table in the namespace</li> </ol> <p>Response:</p> <pre><code>{\n  \"location\": \"s3://bucket/data/users.lance\",\n  \"storage_options\": {\n    \"aws_access_key_id\": \"...\",\n    \"aws_secret_access_key\": \"...\"\n  }\n}\n</code></pre> <p>Error Handling:</p> <p>If the parent namespace does not exist, return HTTP <code>404 Not Found</code> with error code <code>1</code> (NamespaceNotFound).</p> <p>If a table with the same identifier already exists, return HTTP <code>409 Conflict</code> with error code <code>5</code> (TableAlreadyExists).</p> <p>If there is a concurrent creation attempt, return HTTP <code>409 Conflict</code> with error code <code>14</code> (ConcurrentModification).</p>"},{"location":"format/namespace/rest/impl-spec/#listtables","title":"ListTables","text":"<p>Lists tables within a namespace.</p> <p>HTTP Request:</p> <pre><code>GET /v1/namespace/{id}/table/list?page_token=xxx&amp;limit=100\n</code></pre> <p>The <code>page_token</code> and <code>limit</code> query parameters support pagination.</p> <p>The implementation:</p> <ol> <li>Parse the namespace identifier from the route path <code>{id}</code></li> <li>Validate the namespace exists</li> <li>Query the server's storage for tables in the namespace</li> <li>Apply pagination using <code>page_token</code> and <code>limit</code></li> <li>Return the list of table names</li> </ol> <p>Response:</p> <pre><code>{\n  \"tables\": [\"users\", \"orders\", \"products\"],\n  \"next_page_token\": \"def456\"\n}\n</code></pre> <p>The <code>next_page_token</code> field is only present if there are more results.</p> <p>Error Handling:</p> <p>If the namespace does not exist, return HTTP <code>404 Not Found</code> with error code <code>1</code> (NamespaceNotFound).</p>"},{"location":"format/namespace/rest/impl-spec/#describetable","title":"DescribeTable","text":"<p>Returns table metadata including schema and version.</p> <p>HTTP Request:</p> <pre><code>POST /v1/table/{id}/describe\nContent-Type: application/json\n</code></pre> <p>The request body can optionally specify a version:</p> <pre><code>{\n  \"version\": 5\n}\n</code></pre> <p>The implementation:</p> <ol> <li>Parse the table identifier from the route path <code>{id}</code></li> <li>Extract the parent namespace from the identifier</li> <li>Validate the parent namespace exists</li> <li>Look up the table in the server's storage</li> <li>If <code>version</code> is specified, retrieve that specific version's metadata</li> <li>Return the table metadata</li> </ol> <p>Response:</p> <pre><code>{\n  \"name\": \"users\",\n  \"location\": \"s3://bucket/data/users.lance\",\n  \"schema\": {\n    \"fields\": [\n      {\"name\": \"id\", \"type\": {\"name\": \"int64\"}, \"nullable\": false},\n      {\"name\": \"name\", \"type\": {\"name\": \"utf8\"}, \"nullable\": true}\n    ]\n  },\n  \"version\": 5\n}\n</code></pre> <p>Error Handling:</p> <p>If the parent namespace does not exist, return HTTP <code>404 Not Found</code> with error code <code>1</code> (NamespaceNotFound).</p> <p>If the table does not exist, return HTTP <code>404 Not Found</code> with error code <code>4</code> (TableNotFound).</p> <p>If the specified version does not exist, return HTTP <code>404 Not Found</code> with error code <code>11</code> (TableVersionNotFound).</p>"},{"location":"format/namespace/rest/impl-spec/#droptable","title":"DropTable","text":"<p>Removes a table and its data.</p> <p>HTTP Request:</p> <pre><code>POST /v1/table/{id}/drop\nContent-Type: application/json\n</code></pre> <p>The request body is empty:</p> <pre><code>{}\n</code></pre> <p>The implementation:</p> <ol> <li>Parse the table identifier from the route path <code>{id}</code></li> <li>Extract the parent namespace from the identifier</li> <li>Validate the parent namespace exists</li> <li>Look up the table in the server's storage</li> <li>Delete the table data from storage</li> <li>Remove the table registration from the namespace</li> </ol> <p>Response:</p> <pre><code>{}\n</code></pre> <p>Error Handling:</p> <p>If the parent namespace does not exist, return HTTP <code>404 Not Found</code> with error code <code>1</code> (NamespaceNotFound).</p> <p>If the table does not exist, return HTTP <code>404 Not Found</code> with error code <code>4</code> (TableNotFound).</p> <p>If there is a storage permission error, return HTTP <code>403 Forbidden</code> with error code <code>15</code> (PermissionDenied).</p> <p>If there is an unexpected server error, return HTTP <code>500 Internal Server Error</code> with error code <code>18</code> (Internal).</p>"},{"location":"format/namespace/rest/impl-spec/#deregistertable","title":"DeregisterTable","text":"<p>Deregisters a table from the namespace while preserving its data on storage. The table metadata is removed from the namespace catalog but the table files remain at their storage location.</p> <p>HTTP Request:</p> <pre><code>POST /v1/table/{id}/deregister\nContent-Type: application/json\n</code></pre> <p>The request body is empty:</p> <pre><code>{}\n</code></pre> <p>The implementation:</p> <ol> <li>Parse the table identifier from the route path <code>{id}</code></li> <li>Extract the parent namespace from the identifier</li> <li>Validate the parent namespace exists</li> <li>Look up the table in the server's storage</li> <li>Remove the table registration from the namespace catalog</li> <li>Return the table location and properties for reference</li> </ol> <p>Response:</p> <pre><code>{\n  \"location\": \"s3://bucket/data/users.lance\",\n  \"properties\": {\n    \"created_at\": \"2024-01-15T10:30:00Z\"\n  }\n}\n</code></pre> <p>Error Handling:</p> <p>If the parent namespace does not exist, return HTTP <code>404 Not Found</code> with error code <code>1</code> (NamespaceNotFound).</p> <p>If the table does not exist, return HTTP <code>404 Not Found</code> with error code <code>4</code> (TableNotFound).</p>"},{"location":"format/namespace/rest/impl-spec/#error-response-format","title":"Error Response Format","text":"<p>All error responses follow the JSON error response model based on RFC-7807.</p> <p>The response body contains an ErrorResponse with a <code>code</code> field containing the Lance Namespace error code. See Error Handling for the complete list of error codes.</p> <p>Example error response:</p> <pre><code>{\n  \"error\": \"Table 'users' not found in namespace 'production'\",\n  \"code\": 4,\n  \"detail\": \"java.lang.RuntimeException: Table not found\\n\\tat com.example.TableService.describe(TableService.java:42)\\n\\tat ...\",\n  \"instance\": \"/v1/table/production$users/describe\"\n}\n</code></pre> <p>The <code>detail</code> field contains detailed error information such as stack traces for debugging purposes.</p>"},{"location":"format/namespace/rest/impl-spec/#error-code-to-http-status-mapping","title":"Error Code to HTTP Status Mapping","text":"<p>REST namespace implementations must map Lance error codes to HTTP status codes as follows:</p> <ul> <li>Error code <code>0</code> (Unsupported) maps to HTTP <code>406 Not Acceptable</code></li> <li>Error codes <code>1</code>, <code>4</code>, <code>6</code>, <code>8</code>, <code>10</code>, <code>11</code>, <code>12</code> (not found errors) map to HTTP <code>404 Not Found</code></li> <li>Error codes <code>2</code>, <code>3</code>, <code>5</code>, <code>7</code>, <code>9</code>, <code>14</code>, <code>19</code> (conflict errors) map to HTTP <code>409 Conflict</code></li> <li>Error codes <code>13</code>, <code>20</code> (input validation errors) map to HTTP <code>400 Bad Request</code></li> <li>Error code <code>15</code> (PermissionDenied) maps to HTTP <code>403 Forbidden</code></li> <li>Error code <code>16</code> (Unauthenticated) maps to HTTP <code>401 Unauthorized</code></li> <li>Error code <code>17</code> (ServiceUnavailable) maps to HTTP <code>503 Service Unavailable</code></li> <li>Error code <code>18</code> (Internal) maps to HTTP <code>500 Internal Server Error</code></li> </ul>"},{"location":"format/table/","title":"Lance Table Format","text":""},{"location":"format/table/#overview","title":"Overview","text":"<p>The Lance table format organizes datasets as versioned collections of fragments and indices. Each version is described by an immutable manifest file that references data files, deletion files, transaction file and indices. The format supports ACID transactions, schema evolution, and efficient incremental updates through Multi-Version Concurrency Control (MVCC).</p>"},{"location":"format/table/#manifest","title":"Manifest","text":"<p>A manifest describes a single version of the dataset. It contains the complete schema definition including nested fields, the list of data fragments comprising this version,  a monotonically increasing version number, and an optional reference to the index section that describes a list of index metadata.</p> Manifest protobuf message <pre><code>message Manifest {\n  // All fields of the dataset, including the nested fields.\n  repeated lance.file.Field fields = 1;\n\n  // Schema metadata.\n  map&lt;string, bytes&gt; schema_metadata = 5;\n\n  // Fragments of the dataset.\n  repeated DataFragment fragments = 2;\n\n  // Snapshot version number.\n  uint64 version = 3;\n\n  // The file position of the version auxiliary data.\n  //  * It is not inheritable between versions.\n  //  * It is not loaded by default during query.\n  uint64 version_aux_data = 4;\n\n  message WriterVersion {\n    // The name of the library that created this file.\n    string library = 1;\n    // The version of the library that created this file. Because we cannot assume\n    // that the library is semantically versioned, this is a string. However, if it\n    // is semantically versioned, it should be a valid semver string without any 'v'\n    // prefix. For example: `2.0.0`, `2.0.0-rc.1`.\n    //\n    // For forward compatibility with older readers, when writing new manifests this\n    // field should contain only the core version (major.minor.patch) without any\n    // prerelease or build metadata. The prerelease/build info should be stored in\n    // the separate prerelease and build_metadata fields instead.\n    string version = 2;\n    // Optional semver prerelease identifier.\n    //\n    // This field stores the prerelease portion of a semantic version separately\n    // from the core version number. For example, if the full version is \"2.0.0-rc.1\",\n    // the version field would contain \"2.0.0\" and prerelease would contain \"rc.1\".\n    //\n    // This separation ensures forward compatibility: older readers can parse the\n    // clean version field without errors, while newer readers can reconstruct the\n    // full semantic version by combining version, prerelease, and build_metadata.\n    //\n    // If absent, the version field is used as-is.\n    optional string prerelease = 3;\n    // Optional semver build metadata.\n    //\n    // This field stores the build metadata portion of a semantic version separately\n    // from the core version number. For example, if the full version is\n    // \"2.0.0-rc.1+build.123\", the version field would contain \"2.0.0\", prerelease\n    // would contain \"rc.1\", and build_metadata would contain \"build.123\".\n    //\n    // If absent, no build metadata is present.\n    optional string build_metadata = 4;\n  }\n\n  // The version of the writer that created this file.\n  //\n  // This information may be used to detect whether the file may have known bugs\n  // associated with that writer.\n  WriterVersion writer_version = 13;\n\n  // If present, the file position of the index metadata.\n  optional uint64 index_section = 6;\n\n  // Version creation Timestamp, UTC timezone\n  google.protobuf.Timestamp timestamp = 7;\n\n  // Optional version tag\n  string tag = 8;\n\n  // Feature flags for readers.\n  //\n  // A bitmap of flags that indicate which features are required to be able to\n  // read the table. If a reader does not recognize a flag that is set, it\n  // should not attempt to read the dataset.\n  //\n  // Known flags:\n  // * 1: deletion files are present\n  // * 2: row ids are stable and stored as part of the fragment metadata.\n  // * 4: use v2 format (deprecated)\n  // * 8: table config is present\n  uint64 reader_feature_flags = 9;\n\n  // Feature flags for writers.\n  //\n  // A bitmap of flags that indicate which features must be used when writing to the\n  // dataset. If a writer does not recognize a flag that is set, it should not attempt to\n  // write to the dataset.\n  //\n  // The flag identities are the same as for reader_feature_flags, but the values of\n  // reader_feature_flags and writer_feature_flags are not required to be identical.\n  uint64 writer_feature_flags = 10;\n\n  // The highest fragment ID that has been used so far.\n  //\n  // This ID is not guaranteed to be present in the current version, but it may\n  // have been used in previous versions.\n  //\n  // For a single fragment, will be zero. For no fragments, will be absent.\n  optional uint32 max_fragment_id = 11;\n\n  // Path to the transaction file, relative to `{root}/_transactions`. The file at that\n  // location contains a wire-format serialized Transaction message representing the\n  // transaction that created this version.\n  //\n  // This string field \"transaction_file\" may be empty if no transaction file was written.\n  //\n  // The path format is \"{read_version}-{uuid}.txn\" where {read_version} is the version of\n  // the table the transaction read from (serialized to decimal with no padding digits),\n  // and {uuid} is a hyphen-separated UUID.\n  string transaction_file = 12;\n\n  // The file position of the transaction content. None if transaction is empty\n  // This transaction content begins with the transaction content length as u32\n  // If the transaction proto message has a length of `len`, the message ends at `len` + 4\n  optional uint64 transaction_section = 21;\n\n  // The next unused row id. If zero, then the table does not have any rows.\n  //\n  // This is only used if the \"stable_row_ids\" feature flag is set.\n  uint64 next_row_id = 14;\n\n  message DataStorageFormat {\n    // The format of the data files (e.g. \"lance\")\n    string file_format = 1;\n    // The max format version of the data files. The format of the version can vary by\n    // file_format and is not required to follow semver.\n    //\n    // Every file in this version of the dataset has the same file_format version.\n    string version = 2;\n  }\n\n  // The data storage format\n  //\n  // This specifies what format is used to store the data files.\n  DataStorageFormat data_format = 15;\n\n  // Table config.\n  //\n  // Keys with the prefix \"lance.\" are reserved for the Lance library. Other\n  // libraries may wish to similarly prefix their configuration keys\n  // appropriately.\n  map&lt;string, string&gt; config = 16;\n\n  // Metadata associated with the table.\n  //\n  // This is a key-value map that can be used to store arbitrary metadata\n  // associated with the table.\n  //\n  // This is different than configuration, which is used to tell libraries how\n  // to read, write, or manage the table.\n  //\n  // This is different than schema metadata, which is used to describe the\n  // data itself and is attached to the output schema of scans.\n  map&lt;string, string&gt; table_metadata = 19;\n\n  // Field number 17 (`blob_dataset_version`) was used for a secondary blob dataset.\n  reserved 17;\n  reserved \"blob_dataset_version\";\n\n  // The base paths of data files.\n  //\n  // This is used to determine the base path of a data file. In common cases data file paths are under current dataset base path.\n  // But for shallow cloning, importing file and other multi-tier storage cases, the actual data files could be outside of the current dataset.\n  // This field is used with the `base_id` in `lance.file.File` and `lance.file.DeletionFile`.\n  //\n  // For example, if we have a dataset with base path `s3://bucket/dataset`, we have a DataFile with base_id 0, we get the actual data file path by:\n  // base_paths[id = 0] + /data/ + file.path\n  // the key(a.k.a index) starts from 0, increased by 1 for each new base path.\n  repeated BasePath base_paths = 18;\n\n  // The branch of the dataset. None means main branch.\n  optional string branch = 20;\n\n}\n</code></pre>"},{"location":"format/table/#schema-fields","title":"Schema &amp; Fields","text":"<p>The schema of the table is written as a series of fields, plus a schema metadata map. The data types generally have a 1-1 correspondence with the Apache Arrow data types. Each field, including nested fields, have a unique integer id. At initial table creation time, fields are assigned ids in depth-first order. Afterwards, field IDs are assigned incrementally for newly added fields.</p> <p>Column encoding configurations are specified through field metadata using the <code>lance-encoding:</code> prefix. See File Format Encoding Specification for details on available encodings, compression schemes, and configuration options.</p> Field protobuf message <pre><code>message Field {\n  enum Type {\n    PARENT = 0;\n    REPEATED = 1;\n    LEAF = 2;\n  }\n  Type type = 1;\n\n  // Fully qualified name.\n  string name = 2;\n  /// Field Id.\n  ///\n  /// See the comment in `DataFile.fields` for how field ids are assigned.\n  int32 id = 3;\n  /// Parent Field ID. If not set, this is a top-level column.\n  int32 parent_id = 4;\n\n  // Logical types, support parameterized Arrow Type.\n  //\n  // PARENT types will always have logical type \"struct\".\n  //\n  // REPEATED types may have logical types:\n  // * \"list\"\n  // * \"large_list\"\n  // * \"list.struct\"\n  // * \"large_list.struct\"\n  // The final two are used if the list values are structs, and therefore the\n  // field is both implicitly REPEATED and PARENT.\n  //\n  // LEAF types may have logical types:\n  // * \"null\"\n  // * \"bool\"\n  // * \"int8\" / \"uint8\"\n  // * \"int16\" / \"uint16\"\n  // * \"int32\" / \"uint32\"\n  // * \"int64\" / \"uint64\"\n  // * \"halffloat\" / \"float\" / \"double\"\n  // * \"string\" / \"large_string\"\n  // * \"binary\" / \"large_binary\"\n  // * \"date32:day\"\n  // * \"date64:ms\"\n  // * \"decimal:128:{precision}:{scale}\" / \"decimal:256:{precision}:{scale}\"\n  // * \"time:{unit}\" / \"timestamp:{unit}\" / \"duration:{unit}\", where unit is\n  // \"s\", \"ms\", \"us\", \"ns\"\n  // * \"dict:{value_type}:{index_type}:false\"\n  string logical_type = 5;\n  // If this field is nullable.\n  bool nullable = 6;\n\n  // optional field metadata (e.g. extension type name/parameters)\n  map&lt;string, bytes&gt; metadata = 10;  \n\n  bool unenforced_primary_key = 12;\n\n  // Position of this field in the primary key (1-based).\n  // 0 means the field is part of the primary key but uses schema field id for ordering.\n  // When set to a positive value, primary key fields are ordered by this position.\n  uint32 unenforced_primary_key_position = 13;\n\n  // DEPRECATED ----------------------------------------------------------------\n\n  // Deprecated: Only used in V1 file format. V2 uses variable encodings defined\n  // per page.\n  //\n  // The global encoding to use for this field.\n  Encoding encoding = 7;\n\n  // Deprecated: Only used in V1 file format. V2 dynamically chooses when to\n  // do dictionary encoding and keeps the dictionary in the data files.\n  //\n  // The file offset for storing the dictionary value.\n  // It is only valid if encoding is DICTIONARY.\n  //\n  // The logic type presents the value type of the column, i.e., string value.\n  Dictionary dictionary = 8;\n\n  // Deprecated: optional extension type name, use metadata field\n  // ARROW:extension:name\n  string extension_name = 9;\n\n  // Field number 11 was previously `string storage_class`.\n  // Keep it reserved so older manifests remain compatible while new writers\n  // avoid reusing the slot.\n  reserved 11;\n  reserved \"storage_class\";\n\n}\n</code></pre>"},{"location":"format/table/#unenforced-primary-key","title":"Unenforced Primary Key","text":"<p>Lance supports defining an unenforced primary key through field metadata. This is useful for deduplication during merge-insert operations and other use cases that benefit from logical row identity. The primary key is \"unenforced\" meaning Lance does not always validate uniqueness constraints. Users can use specific workloads like merge-insert to enforce it if necessary. The primary key is fixed after initial setting and must not be updated or removed.</p> <p>A primary key field must satisfy:</p> <ul> <li>The field, and all its ancestors, must not be nullable.</li> <li>The field must be a leaf field (primitive data type without children).</li> <li>The field must not be within a list or map type.</li> </ul> <p>When using an Arrow schema to create a Lance table, add the following metadata to the Arrow field to mark it as part of the primary key:</p> <ul> <li><code>lance-schema:unenforced-primary-key</code>: Set to <code>true</code>, <code>1</code>, or <code>yes</code> (case-insensitive) to indicate the field is part of the primary key.</li> <li><code>lance-schema:unenforced-primary-key:position</code> (optional): A 1-based integer specifying the position within a composite primary key.</li> </ul> <p>For composite primary keys with multiple columns, the position determines the primary key field ordering:</p> <ul> <li>When positions are specified, fields are ordered by their position values (1, 2, 3, ...).</li> <li>When positions are not specified, fields are ordered by their schema field id.</li> <li>Fields with explicit positions are ordered before fields without.</li> </ul>"},{"location":"format/table/#fragments","title":"Fragments","text":"<p>A fragment represents a horizontal partition of the dataset containing a subset of rows. Each fragment has a unique <code>uint32</code> identifier assigned incrementally based on the dataset's maximum fragment ID. Each fragment consists of one or more data files storing columns, plus an optional deletion file. If present, the deletion file stores the positions (0-based) of the rows that have been deleted from the fragment. The fragment tracks the total row count including deleted rows in its physical rows field. Column subsets can be read without accessing all data files, and each data file is independently compressed and encoded.</p> DataFragment protobuf message <pre><code>message DataFragment {\n  // The ID of a DataFragment is unique within a dataset.\n  uint64 id = 1;\n\n  repeated DataFile files = 2;\n\n  // File that indicates which rows, if any, should be considered deleted.\n  DeletionFile deletion_file = 3;\n\n  // TODO: What's the simplest way we can allow an inline tombstone bitmap?\n\n  // A serialized RowIdSequence message (see rowids.proto).\n  //\n  // These are the row ids for the fragment, in order of the rows as they appear.\n  // That is, if a fragment has 3 rows, and the row ids are [1, 42, 3], then the\n  // first row is row 1, the second row is row 42, and the third row is row 3.\n  oneof row_id_sequence {\n    // If small (&lt; 200KB), the row ids are stored inline.\n    bytes inline_row_ids = 5;\n    // Otherwise, stored as part of a file.\n    ExternalFile external_row_ids = 6;\n  } // row_id_sequence\n\n  oneof last_updated_at_version_sequence {\n    // If small (&lt; 200KB), the row latest updated versions are stored inline.\n    bytes inline_last_updated_at_versions = 7;\n    // Otherwise, stored as part of a file.\n    ExternalFile external_last_updated_at_versions = 8;\n  } // last_updated_at_version_sequence\n\n  oneof created_at_version_sequence {\n    // If small (&lt; 200KB), the row created at versions are stored inline.\n    bytes inline_created_at_versions = 9;\n    // Otherwise, stored as part of a file.\n    ExternalFile external_created_at_versions = 10;\n  } // created_at_version_sequence\n\n  // Number of original rows in the fragment, this includes rows that are now marked with\n  // deletion tombstones. To compute the current number of rows, subtract\n  // `deletion_file.num_deleted_rows` from this value.\n  uint64 physical_rows = 4;\n\n}\n</code></pre>"},{"location":"format/table/#data-evolution","title":"Data Evolution","text":"<p>This fragment design enables a new concept called data evolution, which means efficient schema evolution (add column, update column, drop column) with backfill. For example, when adding a new column, new column data are added by appending new data files to each fragment, with values computed for all existing rows in the fragment. There is no need to rewrite the entire table to just add data for a single column. This enables efficient feature engineering and embedding updates for ML/AI workloads.</p> <p>Each data file should contain a distinct set of field ids.  It is not required that all field ids in the dataset schema are found in one of the data files.  If there is no corresponding data file, that column should be read as entirely <code>NULL</code>.</p> <p>Field ids might be replaced with <code>-2</code>, a tombstone value.  In this case that column should be ignored. This used, for example, when rewriting a column:  The old data file replaces the field id with <code>-2</code> to ignore the old data, and a new data file is appended to the fragment.</p>"},{"location":"format/table/#data-files","title":"Data Files","text":"<p>Data files store column data for a fragment using the Lance file format. Each data file stores a subset of the columns in the fragment. Field IDs are assigned either sequentially based on schema position (for Lance file format v1)  or independently of column indices due to variable encoding widths (for Lance file format v2).</p> DataFile protobuf message <pre><code>message DataFile {\n  // Path to the root relative to the dataset's URI.\n  string path = 1;\n  // The ids of the fields/columns in this file.\n  //\n  // When a DataFile object is created in memory, every value in fields is assigned -1 by\n  // default. An object with a value in fields of -1 must not be stored to disk. -2 is\n  // used for \"tombstoned\", meaning a field that is no longer in use. This is often\n  // because the original field id was reassigned to a different data file.\n  //\n  // In Lance v1 IDs are assigned based on position in the file, offset by the max\n  // existing field id in the table (if any already). So when a fragment is first created\n  // with one file of N columns, the field ids will be 1, 2, ..., N. If a second fragment\n  // is created with M columns, the field ids will be N+1, N+2, ..., N+M.\n  //\n  // In Lance v1 there is one field for each field in the input schema, this includes\n  // nested fields (both struct and list).  Fixed size list fields have only a single\n  // field id (these are not considered nested fields in Lance v1).\n  //\n  // This allows column indices to be calculated from field IDs and the input schema.\n  //\n  // In Lance v2 the field IDs generally follow the same pattern but there is no\n  // way to calculate the column index from the field ID.  This is because a given\n  // field could be encoded in many different ways, some of which occupy a different\n  // number of columns.  For example, a struct field could be encoded into N + 1 columns\n  // or it could be encoded into a single packed column.  To determine column indices\n  // the column_indices property should be used instead.\n  //\n  // In Lance v1 these ids must be sorted but might not always be contiguous.\n  repeated int32 fields = 2;\n  // The top-level column indices for each field in the file.\n  //\n  // If the data file is version 1 then this property will be empty\n  //\n  // Otherwise there must be one entry for each field in `fields`.\n  //\n  // Some fields may not correspond to a top-level column in the file.  In these cases\n  // the index will -1.\n  //\n  // For example, consider the schema:\n  //\n  // - dimension: packed-struct (0):\n  //   - x: u32 (1)\n  //   - y: u32 (2)\n  // - path: `list&lt;u32&gt;` (3)\n  // - embedding: `fsl&lt;768&gt;` (4)\n  //   - fp64\n  // - borders: `fsl&lt;4&gt;` (5)\n  //   - simple-struct (6)\n  //     - margin: fp64 (7)\n  //     - padding: fp64 (8)\n  //\n  // One possible column indices array could be:\n  // [0, -1, -1, 1, 3, 4, 5, 6, 7]\n  //\n  // This reflects quite a few phenomenon:\n  // - The packed struct is encoded into a single column and there is no top-level column\n  //   for the x or y fields\n  // - The variable sized list is encoded into two columns\n  // - The embedding is encoded into a single column (common for FSL of primitive) and there\n  //   is not \"FSL column\"\n  // - The borders field actually does have an \"FSL column\"\n  //\n  // The column indices table may not have duplicates (other than -1)\n  repeated int32 column_indices = 3;\n  // The major file version used to create the file\n  uint32 file_major_version = 4;\n  // The minor file version used to create the file\n  //\n  // If both `file_major_version` and `file_minor_version` are set to 0,\n  // then this is a version 0.1 or version 0.2 file.\n  uint32 file_minor_version = 5;\n\n  // The known size of the file on disk in bytes.\n  //\n  // This is used to quickly find the footer of the file.\n  //\n  // When this is zero, it should be interpreted as \"unknown\".\n  uint64 file_size_bytes = 6;\n\n  // The base path index of the data file. Used when the file is imported or referred from another dataset.\n  // Lance use it as key of the base_paths field in Manifest to determine the actual base path of the data file.\n  optional uint32 base_id = 7;\n\n}\n</code></pre>"},{"location":"format/table/#deletion-files","title":"Deletion Files","text":"<p>Deletion files (a.k.a. deletion vectors) track deleted rows without rewriting data files. Each fragment can have at most one deletion file per version.</p> <p>Deletion files support two storage formats. The Arrow IPC format (<code>.arrow</code> extension) stores a flat Int32Array of deleted row offsets and is efficient for sparse deletions. The Roaring Bitmap format (<code>.bin</code> extension) stores a compressed roaring bitmap and is efficient for dense deletions. Readers must filter rows whose offsets appear in the deletion file for the fragment.</p> <p>Deletions can be materialized by rewriting data files with deleted rows removed. However, this invalidates row addresses and requires rebuilding indices, which can be expensive.</p> DeletionFile protobuf message <pre><code>message DeletionFile {\n  // Type of deletion file, intended as a way to increase efficiency of the storage of deleted row\n  // offsets. If there are sparsely deleted rows, then ARROW_ARRAY is the most efficient. If there\n  // are densely deleted rows, then BITMAP is the most efficient.\n  enum DeletionFileType {\n    // A single Int32Array of deleted row offsets, stored as an Arrow IPC file with one batch and\n    // one column. Has a .arrow extension.\n    ARROW_ARRAY = 0;\n    // A Roaring Bitmap of deleted row offsets. Has a .bin extension.\n    BITMAP = 1;\n  }\n\n  // Type of deletion file.\n  DeletionFileType file_type = 1;\n  // The version of the dataset this deletion file was built from.\n  uint64 read_version = 2;\n  // An opaque id used to differentiate this file from others written by concurrent\n  // writers.\n  uint64 id = 3;\n  // The number of rows that are marked as deleted.\n  uint64 num_deleted_rows = 4;\n  // The base path index of the deletion file. Used when the file is imported or referred from another\n  // dataset. Lance uses it as key of the base_paths field in Manifest to determine the actual base\n  // path of the deletion file.\n  optional uint32 base_id = 7;\n\n}\n</code></pre>"},{"location":"format/table/#related-specifications","title":"Related Specifications","text":""},{"location":"format/table/#storage-layout","title":"Storage Layout","text":"<p>File organization, base path system, and multi-location storage.</p> <p>See Storage Layout Specification</p>"},{"location":"format/table/#transactions","title":"Transactions","text":"<p>MVCC, commit protocol, transaction types, and conflict resolution.</p> <p>See Transaction Specification</p>"},{"location":"format/table/#row-lineage","title":"Row Lineage","text":"<p>Row address, Stable row ID, row version tracking, and change data feed.</p> <p>See Row ID &amp; Lineage Specification</p>"},{"location":"format/table/#indices","title":"Indices","text":"<p>Vector indices, scalar indices, full-text search, and index management.</p> <p>See Index Specification</p>"},{"location":"format/table/#versioning","title":"Versioning","text":"<p>Feature flags and format version compatibility.</p> <p>See Format Versioning Specification</p>"},{"location":"format/table/branch_tag/","title":"Branch and Tag Specification","text":""},{"location":"format/table/branch_tag/#overview","title":"Overview","text":"<p>Lance supports branching and tagging for managing multiple independent version histories and creating named references to specific versions. Branches enable parallel development workflows, while tags provide stable named references for important versions.</p>"},{"location":"format/table/branch_tag/#branching","title":"Branching","text":""},{"location":"format/table/branch_tag/#branch-name","title":"Branch Name","text":"<p>Branch names must follow these validation rules:</p> <ol> <li>Cannot be empty</li> <li>Cannot start or end with <code>/</code></li> <li>Cannot contain consecutive <code>//</code></li> <li>Cannot contain <code>..</code> or <code>\\</code></li> <li>Segments must contain only alphanumeric characters, <code>.</code>, <code>-</code>, <code>_</code></li> <li>Cannot end with <code>.lock</code></li> <li>Cannot be named <code>main</code> (reserved for main branch)</li> </ol>"},{"location":"format/table/branch_tag/#branch-metadata-path","title":"Branch Metadata Path","text":"<p>Branch metadata is stored at <code>_refs/branches/{branch-name}.json</code> in the dataset root. Since branch names support hierarchical naming with <code>/</code> characters, the <code>/</code> is URL-encoded as <code>%2F</code> in the filename to distinguish it from directory separators (e.g., <code>bugfix/issue-123</code> becomes <code>bugfix%2Fissue-123.json</code>):</p> <pre><code>{dataset_root}/\n    _refs/\n        branches/\n            feature-a.json\n            bugfix%2Fissue-123.json  # Note: '/' encoded as '%2F'\n</code></pre>"},{"location":"format/table/branch_tag/#branch-metadata-file-format","title":"Branch Metadata File Format","text":"<p>Each branch metadata file is a JSON file with the following fields:</p> JSON Key Type Optional Description <code>parent_branch</code> string Yes Name of the branch this was created from. <code>null</code> indicates branched from main. <code>parent_version</code> number Version number of the parent branch at the time this branch was created. <code>create_at</code> number Unix timestamp (seconds since epoch) when the branch was created. <code>manifest_size</code> number Size of the initial manifest file in bytes."},{"location":"format/table/branch_tag/#branch-dataset-layout","title":"Branch Dataset Layout","text":"<p>Each branch dataset is technically a shallow clone of the source dataset. Branch datasets are organized using the <code>tree/</code> directory at the dataset root:</p> <pre><code>{dataset_root}/\n    tree/\n        {branch_name}/\n            _versions/\n                *.manifest\n            _transactions/\n                *.txn\n            _deletions/\n                *.arrow\n                *.bin\n            _indices/\n                {UUID}/\n                    index.idx\n</code></pre> <p>Named branches store their version-specific files under <code>tree/{branch_name}/</code>, resembling the GitHub branch path convention. It uses the branch name as is to form the path,  which means <code>/</code> would create a logical subdirectory (e.g., <code>bugfix/issue-123</code>, <code>feature/user-auth</code>):</p> <pre><code>{dataset_root}/\n    tree/\n        feature-a/\n            _versions/\n                1.manifest\n                2.manifest\n        bugfix/\n            issue-123/\n                _versions/\n                    1.manifest\n</code></pre>"},{"location":"format/table/branch_tag/#tagging","title":"Tagging","text":""},{"location":"format/table/branch_tag/#tag-name","title":"Tag Name","text":"<p>Tag names must follow these validation rules:</p> <ol> <li>Cannot be empty</li> <li>Must contain only alphanumeric characters, <code>.</code>, <code>-</code>, <code>_</code></li> <li>Cannot start or end with <code>.</code></li> <li>Cannot end with <code>.lock</code></li> <li>Cannot contain consecutive <code>..</code></li> </ol> <p>Note that tag names do not support <code>/</code> characters, unlike branch names.</p>"},{"location":"format/table/branch_tag/#tag-storage","title":"Tag Storage","text":"<p>Tags are stored as JSON files under <code>_refs/tags/</code> at the dataset root:</p> <pre><code>{dataset_root}/\n    _refs/\n        tags/\n            v1.0.0.json\n            v1.1.0.json\n            production.json\n</code></pre> <p>Tags are always stored at the root dataset level, regardless of which branch they reference.</p>"},{"location":"format/table/branch_tag/#tag-file-format","title":"Tag File Format","text":"<p>Each tag file is a JSON file with the following fields:</p> JSON Key Type Optional Description <code>branch</code> string Yes Branch name being tagged. <code>null</code> or absent indicates main branch. <code>version</code> number Version number being tagged within that branch. <code>manifest_size</code> number Size of the manifest file in bytes. Used for efficient manifest loading."},{"location":"format/table/layout/","title":"Storage Layout Specification","text":""},{"location":"format/table/layout/#overview","title":"Overview","text":"<p>This specification defines how Lance datasets are organized on object storage. The layout design emphasizes portability, allowing datasets to be relocated or referenced across multiple storage systems with minimal metadata changes.</p>"},{"location":"format/table/layout/#dataset-root","title":"Dataset Root","text":"<p>The dataset root is the location where the dataset was initially created. Every Lance dataset has exactly one dataset root, which serves as the primary storage location for the dataset's files. The dataset root contains the standard subdirectory structure (<code>data/</code>, <code>_versions/</code>, <code>_deletions/</code>, <code>_indices/</code>, <code>_refs/</code>, <code>tree/</code>) that organizes the dataset's files.</p>"},{"location":"format/table/layout/#basic-layout","title":"Basic Layout","text":"<p>A Lance dataset in its basic form stores all files within the dataset root directory structure:</p> <pre><code>{dataset_root}/\n    data/\n        *.lance           -- Data files containing column data\n    _versions/\n        *.manifest        -- Manifest files (one per version)\n    _transactions/\n        *.txn             -- Transaction files for commit coordination\n    _deletions/\n        *.arrow           -- Deletion vector files (arrow format)\n        *.bin             -- Deletion vector files (bitmap format)\n    _indices/\n        {UUID}/\n            ...           -- Index content (different for each index type)\n    _refs/\n        tags/\n            *.json        -- Tag metadata\n        branches/\n            *.json        -- Branch metadata\n    tree/\n        {branch_name}/\n            ...           -- Branch dataset\n</code></pre>"},{"location":"format/table/layout/#base-path-system","title":"Base Path System","text":""},{"location":"format/table/layout/#basepath-message","title":"BasePath Message","text":"<p>The manifest's <code>base_paths</code> field contains an array of <code>BasePath</code> entries that define alternative storage locations for dataset files. Each base path entry has a unique numeric identifier that file metadata can reference to indicate where files are located. The <code>path</code> field specifies an absolute path interpretable by the object store. The <code>is_dataset_root</code> field determines how the path is interpreted: when true, the path points to a dataset root with standard subdirectories (<code>data/</code>, <code>_deletions/</code>, <code>_indices/</code>); when false, the path points directly to a file directory without subdirectories. An optional <code>name</code> field provides a human-readable alias, which is particularly useful for referencing tags in shallow clones.</p> BasePath protobuf message <pre><code>message BasePath {\n  uint32 id = 1;\n  optional string name = 2;\n  bool is_dataset_root = 3;\n  string path = 4;\n}\n</code></pre>"},{"location":"format/table/layout/#file-metadata-base-references","title":"File Metadata Base References","text":"<p>Three types of files can specify alternative base paths: data files, deletion files, and index metadata. Each of these file types includes an optional <code>base_id</code> field in their metadata that references a base path entry by its numeric identifier. When a file's <code>base_id</code> is absent, the file is located relative to the dataset root. When a file's <code>base_id</code> is present, readers must look up the corresponding base path entry in the manifest's <code>base_paths</code> array to determine where the file is stored.</p> <p>At read time, path resolution follows a two-step process. First, the reader determines the base path: if <code>base_id</code> is absent, the base path is the dataset root; otherwise, the reader looks up the base path entry using the <code>base_id</code> to obtain the path and its <code>is_dataset_root</code> flag. Second, the reader constructs the full file path based on whether the base path represents a dataset root. For dataset roots (when <code>is_dataset_root</code> is true), the full path includes standard subdirectories: data files are located under <code>data/</code>, deletion files under <code>_deletions/</code>, and indices under <code>_indices/</code>. For non-root base paths (when <code>is_dataset_root</code> is false), the base path points directly to the file directory, and the file path is appended directly without subdirectory prefixes.</p>"},{"location":"format/table/layout/#example-complex-layout-scenarios","title":"Example Complex Layout Scenarios","text":""},{"location":"format/table/layout/#hotcold-tiering","title":"Hot/Cold Tiering","text":"<pre><code>Manifest base_paths:\n[\n  { id: 0, is_dataset_root: true, path: \"s3://hot-bucket/dataset\" },\n  { id: 1, is_dataset_root: true, path: \"s3://cold-bucket/dataset-archive\" }\n]\n\nFragment 0 (recent data):\n  DataFile { path: \"fragment-0.lance\", base_id: 0 }\n  \u2192 resolves to: s3://hot-bucket/dataset/data/fragment-0.lance\n\nFragment 100 (historical data):\n  DataFile { path: \"fragment-100.lance\", base_id: 1 }\n  \u2192 resolves to: s3://cold-bucket/dataset-archive/data/fragment-100.lance\n</code></pre> <p>This allows seamless querying across storage tiers without data movement.</p>"},{"location":"format/table/layout/#multi-region-distribution","title":"Multi-Region Distribution","text":"<pre><code>Manifest base_paths:\n[\n  { id: 0, is_dataset_root: true, path: \"s3://us-east-bucket/dataset\" },\n  { id: 1, is_dataset_root: true, path: \"s3://eu-west-bucket/dataset\" },\n  { id: 2, is_dataset_root: true, path: \"s3://ap-south-bucket/dataset\" }\n]\n\nFragments distributed by data locality:\n  Fragment 0 (US users): base_id: 0\n  Fragment 1 (EU users): base_id: 1\n  Fragment 2 (Asia users): base_id: 2\n</code></pre> <p>Compute jobs can read data from the nearest region without data transfer.</p>"},{"location":"format/table/layout/#shallow-clone","title":"Shallow Clone","text":"<p>Shallow clones create a new dataset that references data files from a source dataset without copying:</p> <p>Example: Shallow Clone</p> <pre><code>Source dataset: s3://production/main-dataset\nClone dataset:  s3://experiments/test-variant\n\nClone manifest base_paths:\n[\n  { id: 0, is_dataset_root: true, path: \"s3://experiments/test-variant\" },\n  { id: 1, is_dataset_root: true, path: \"s3://production/main-dataset\",\n    name: \"v1.0\" }\n]\n\nOriginal fragments (inherited):\n  DataFile { path: \"fragment-0.lance\", base_id: 1 }\n  \u2192 resolves to: s3://production/main-dataset/data/fragment-0.lance\n\nNew fragments (clone-specific):\n  DataFile { path: \"fragment-new.lance\", base_id: 0 }\n  \u2192 resolves to: s3://experiments/test-variant/data/fragment-new.lance\n</code></pre> <p>The clone can append new data, modify schemas, or delete rows without affecting the source dataset. Only the manifest and new data files are stored in the clone location.</p> <p>Workflow:</p> <ol> <li>Clone transaction creates new manifest in target location</li> <li>Manifest includes base path pointing to source dataset</li> <li>Original fragments reference source via <code>base_id: 1</code></li> <li>Subsequent writes reference clone location via <code>base_id: 0</code></li> <li>Source dataset remains immutable and can be garbage collected independently</li> </ol>"},{"location":"format/table/layout/#dataset-portability","title":"Dataset Portability","text":"<p>The base path system combined with relative file references provides strong portability guarantees for Lance datasets. All file paths within Lance files are stored relative to their containing directory, enabling datasets to be relocated without file modifications.</p> <p>To port a dataset to a new location, simply copy all contents from the dataset root directory. The copied dataset will function immediately at the new location without any manifest updates, as all file references within the dataset root resolve through relative paths.</p> <p>When a dataset uses multiple base paths (such as in shallow clones or multi-bucket configurations), users have flexibility in how to port the dataset. The simplest approach is to copy only the dataset root, which preserves references to the original base path locations. Alternatively, users can copy additional base paths to the new location and update the manifest's <code>base_paths</code> array to reflect the new base paths. Since only the <code>base_paths</code> field in the manifest requires modification, this remains a lightweight metadata operation that does not require rewriting additional metadata or data files.</p>"},{"location":"format/table/layout/#file-naming-conventions","title":"File Naming Conventions","text":""},{"location":"format/table/layout/#data-files","title":"Data Files","text":"<p>Pattern: <code>data/{uuid-based-filename}.lance</code></p> <p>Data files use UUID-based filenames optimized for S3 throughput. The filename is generated from a UUID (16 bytes) by converting the first 3 bytes to a 24-character binary string and the remaining 13 bytes to a 26-character hex string, resulting in a 50-character filename. The binary prefix (rather than hex) provides maximum entropy per character, allowing S3's internal partitioning to quickly recognize access patterns and scale appropriately, minimizing throttling.</p> <p>Example: <code>data/101100101101010011010110a1b2c3d4e5f6g7h8i9j0.lance</code></p>"},{"location":"format/table/layout/#deletion-files","title":"Deletion Files","text":"<p>Pattern: <code>_deletions/{fragment_id}-{read_version}-{id}.{extension}</code></p> <p>Deletion files use two extensions: <code>.arrow</code> for Arrow IPC format (sparse deletions) and <code>.bin</code> for Roaring bitmap format (dense deletions).</p> <p>Example: <code>_deletions/42-10-a1b2c3d4.arrow</code></p>"},{"location":"format/table/layout/#transaction-files","title":"Transaction Files","text":"<p>Pattern: <code>_transactions/{read_version}-{uuid}.txn</code></p> <p>Where <code>read_version</code> is the table version the transaction was built from.</p> <p>Example: <code>_transactions/5-550e8400-e29b-41d4-a716-446655440000.txn</code></p>"},{"location":"format/table/layout/#manifest-files","title":"Manifest Files","text":"<p>Manifest files are stored in the <code>_versions/</code> directory with naming schemes that support atomic commits.</p> <p>See Manifest Naming Schemes for details on the V1 and V2 patterns and their implications for version discovery.</p>"},{"location":"format/table/mem_wal/","title":"MemTable &amp; WAL Specification (Experimental)","text":"<p>Lance MemTable &amp; WAL (MemWAL) specification describes a Log-Structured-Merge (LSM) tree architecture for Lance tables, enabling high-performance streaming write workloads while maintaining indexed read performance for key workloads including scan, point lookup, vector search and full-text search.</p> <p>Note</p> <p>MemWAL requires the table to have an unenforced primary key defined.</p>"},{"location":"format/table/mem_wal/#overall-architecture","title":"Overall Architecture","text":""},{"location":"format/table/mem_wal/#base-table","title":"Base Table","text":"<p>Under the MemWAL setup, the Lance table is called the base table.</p>"},{"location":"format/table/mem_wal/#region","title":"Region","text":"<p>A Region is the main unit to horizontally scale out writes.</p> <p>Each region has exactly one active writer at any time, using epoch-based fencing to guarantee single-writer semantics without distributed coordination. Writers claim a region by incrementing the writer epoch, then write data to that region. Data in each region is merged into the base table gradually in the background.</p> <p>Regions must contain rows that are mutually exclusive. Two regions contain rows with the same primary key, the following scenario can cause data corruption:</p> <ol> <li>Region A receives a write with primary key <code>pk=1</code> at time T1</li> <li>Region B receives a write with primary key <code>pk=1</code> at time T2 (T2 &gt; T1)</li> <li>The row in region B is merged into the base table first</li> <li>The row in region A is merged into the base table second</li> <li>The row from Region A (older) now overwrites the row from Region B (newer)</li> </ol> <p>This violates the expected \"last write wins\" semantics. By ensuring each primary key is assigned to exactly one region via the region spec, merge order between regions becomes irrelevant for correctness.</p>"},{"location":"format/table/mem_wal/#memwal-index","title":"MemWAL Index","text":"<p>A MemWAL Index is the centralized structure for all MemWAL metadata for a base table. A table has at most one MemWAL index. It stores:</p> <ul> <li>Configuration: Region specs defining how rows map to regions, and which indexes to maintain</li> <li>Merge progress: Last generation merged to base table for each region</li> <li>Index catchup progress: Which merged generation each base table index has been rebuilt to cover</li> <li>Region snapshots: Point-in-time snapshot of all region states for read optimization</li> </ul> <p>The index is the source of truth for configuration and merge progress, but region state snapshots are for read-only optimization (each region's manifest is authoritative for its own state).</p> <p>Writers read the MemWAL index to get configuration (region specs, maintained indexes) before writing. Readers use the index to get a snapshot of all region states, then query each region's data alongside the base table and merge results at runtime.</p> <p>A background process periodically updates region snapshots by listing regions and loading their manifests. See MemWAL Index Details for the complete structure.</p>"},{"location":"format/table/mem_wal/#region-architecture","title":"Region Architecture","text":"<p>Within a region, writes enter its MemTable and are flushed to the regional WAL for durability. The MemTable is flushed to storage as a Flushed MemTable based on memory pressure and other conditions. Flushed MemTables are then asynchronously merged into the base table.</p>"},{"location":"format/table/mem_wal/#memtable","title":"MemTable","text":"<p>An in-memory Lance table that buffers incoming writes. Each write inserts a fragment in the MemTable, making data immediately queryable without waiting for persistence.</p> <p>In addition to the data fragments, a MemTable maintains:</p> <ul> <li>Primary key bloom filter: For efficient existence checks during staleness detection</li> <li>In-memory index builders: Incremental index structures that mirror base table indexes, enabling indexed queries on unflushed data</li> <li>WAL fragment mapping: Tracks correspondence between MemTable fragment IDs and WAL entry IDs for index remapping during flush</li> </ul>"},{"location":"format/table/mem_wal/#wal","title":"WAL","text":"<p>Write-Ahead Log (WAL) serves as the durable storage of MemTable. A write to MemTable must be persisted also to the WAL to become fully durable. Every time we write to the WAL, we call it a WAL Flush.</p> <p>The whole LSM tree's durability is determined by the durability of the WAL. For example, if WAL is stored in Amazon S3, it has the 99.999999999% durability. If it is stored in local disk, the data will be lost if the local disk is damaged.</p> <p>A WAL consists of an ordered sequence of WAL entries starting from 1. Each entry is a Lance format file. The writer epoch is stored in the Lance file's schema metadata with key <code>writer_epoch</code> for fencing validation during replay.</p> <p>Each WAL entry is stored within the WAL directory of the region located at <code>_mem_wal/{region_id}/wal</code>.</p> <p>WAL files use bit-reversed 64-bit binary naming to distribute files evenly across the directory keyspace. This optimizes S3 throughput by spreading sequential writes across S3's internal partitions, minimizing throttling. The filename is the bit-reversed binary representation of the entry ID with suffix <code>.lance</code>. For example, entry ID 5 (binary <code>000...101</code>) becomes <code>1010000000000000000000000000000000000000000000000000000000000000.lance</code>.</p>"},{"location":"format/table/mem_wal/#flushed-memtable","title":"Flushed MemTable","text":"<p>A flushed MemTable is a complete Lance table created by flushing the MemTable to storage.</p> <p>Note</p> <p>This is called Sorted String Table (SSTable) or Sorted Run in many LSM-tree literatures and implementations. However, since our MemTable is not sorted, we just use the term flushed MemTable to avoid confusion.</p> <p>Each flushed MemTable has a generation number starting from 1 that identifies its relative position among all flushed MemTables in the region. When MemTable with generation <code>i</code> is flushed, the next MemTable gets generation number <code>i+1</code>.</p> <p>The MemTable of generation <code>i</code> is flushed to <code>_mem_wal/{region_uuid}/{random_hash}_gen_{i}/</code> directory, where <code>{random_hash}</code> is an 8-character hex value generated at flush time. The directory content follows Lance table layout.</p> <p>The actual directory path for each generation is recorded in the region manifest's <code>flushed_generations</code> list (see Region Manifest).</p> <p>Generation numbers determine merge order: lower numbers represent older data and must be merged to the base table first to preserve correct upsert semantics.</p>"},{"location":"format/table/mem_wal/#region-manifest","title":"Region Manifest","text":"<p>Each region has a manifest file containing epoch-based fencing tokens, WAL pointers, and flushed MemTable generation trackers. This is the source of truth for region state.</p> <p>The manifest is serialized as a protobuf binary file using the <code>RegionManifest</code> message.</p> <p>The manifest contains:</p> <ul> <li>Fencing state: <code>writer_epoch</code> (writer fencing token)</li> <li>WAL pointers: <code>replay_after_wal_id</code> (last entry flushed to MemTable), <code>wal_id_last_seen</code> (last entry seen at manifest update)</li> <li>Generation trackers: <code>current_generation</code> (next generation to flush)</li> <li>Flushed generations: <code>flushed_generations</code> list of generation number and directory path pairs (e.g., generation 1 at <code>a1b2c3d4_gen_1</code>)</li> </ul> <p>Note: <code>wal_id_last_seen</code> is a hint that may be stale since it's not updated on WAL write. The manifest itself is atomically written, but recovery must try to get newer WAL files to find the actual state beyond this hint.</p> RegionManifest protobuf message <pre><code>message RegionManifest {\n  // Region identifier (UUID v4).\n  UUID region_id = 11;\n\n  // Manifest version number.\n  // Matches the version encoded in the filename.\n  uint64 version = 1;\n\n  // Region spec ID this region was created with.\n  // Set at region creation and immutable thereafter.\n  // A value of 0 indicates a manually-created region not governed by any spec.\n  uint32 region_spec_id = 10;\n\n  // Writer fencing token - monotonically increasing.\n  // A writer must increment this when claiming the region.\n  uint64 writer_epoch = 2;\n\n  // The most recent WAL entry ID that has been flushed to a MemTable.\n  // During recovery, replay starts from replay_after_wal_id + 1.\n  uint64 replay_after_wal_id = 3;\n\n  // The most recent WAL entry ID at the time manifest was updated.\n  // This is a hint, not authoritative - recovery must list files to find actual state.\n  uint64 wal_id_last_seen = 4;\n\n  // Next generation ID to create (incremented after each MemTable flush).\n  uint64 current_generation = 6;\n\n  // Field 7 removed: merged_generation moved to MemWalIndexDetails.merged_generations\n  // which is the authoritative source for merge progress.\n\n  // List of flushed MemTable generations and their directory paths.\n  repeated FlushedGeneration flushed_generations = 8;\n\n}\n</code></pre> <p>Manifests are versioned starting from 1 and immutable. Each update creates a new manifest file at the next version number. Updates use put-if-not-exists or file rename to ensure atomicity depending on the storage system. If two processes compete, one wins and the other retries.</p> <p>To commit a manifest version:</p> <ol> <li>Compute the next version number</li> <li>Write the manifest to <code>{bit_reversed_version}.binpb</code> using put-if-not-exists</li> <li>In parallel best-effort write to <code>version_hint.json</code> with <code>{\"version\": &lt;new_version&gt;}</code> (failure is acceptable)</li> </ol> <p>To read the latest manifest version:</p> <ol> <li>Read <code>version_hint.json</code> to get the latest version hint. If not found, start from version 1</li> <li>Check existence for subsequent versions from the starting version</li> <li>Continue until a version is not found</li> <li>The latest version is the last found version</li> </ol> <p>This approach uses HEAD requests instead of LIST operations in cloud storage, which is generally faster and is friendly to systems like S3 Express that do not support lexicographically sorted listing.</p> <p>Note</p> <p>This works because the write rate to region manifests is significantly lower than read rates. Region manifests are only updated when region metadata changes (MemTable flush), not on every write. This ensures HEAD requests will eventually terminate and find the latest version.</p> <p>All region manifest versions are stored in <code>_mem_wal/{region_id}/manifest</code> directory.</p> <p>Each region manifest version file uses bit-reversed 64-bit binary naming, the same scheme as WAL files. For example, version 5 becomes <code>1010000000000000000000000000000000000000000000000000000000000000.binpb</code>.</p> <p>The region manifest is updated atomically in the following cases:</p> Trigger Fields Updated Details Initialization &amp; Recovery <code>writer_epoch</code> Incremented when writer claims the region MemTable Flush <code>replay_after_wal_id</code>, <code>wal_id_last_seen</code>, <code>current_generation</code>, <code>flushed_generations</code> After flushing MemTable to storage MemWAL Index Builder <code>wal_id_last_seen</code> Periodically scans WAL entries and updates hint Garbage Collector <code>flushed_generations</code> Removes entries for deleted flushed MemTables <p>Note</p> <p>WAL flush does not update the manifest to keep the hot write path fast.</p> <p>Writers use epoch-based fencing (<code>writer_epoch</code>) to ensure single-writer semantics. See Writer Fencing for details.</p>"},{"location":"format/table/mem_wal/#memwal-index-details","title":"MemWAL Index Details","text":"<p>The MemWAL Index uses the standard index storage at <code>_indices/{UUID}/</code>.</p> <p>The index stores its data in two parts:</p> <ol> <li>Index details (<code>index_details</code> in <code>IndexMetadata</code>): Contains configuration, merge progress, and snapshot metadata</li> <li>Region snapshots: Stored as a Lance file or inline, depending on region count</li> </ol>"},{"location":"format/table/mem_wal/#index-details","title":"Index Details","text":"<p>The <code>index_details</code> field in <code>IndexMetadata</code> contains a <code>MemWalIndexDetails</code> protobuf message with the following key fields:</p> <p>Configuration fields (<code>region_specs</code>, <code>maintained_indexes</code>) are the source of truth for MemWAL configuration. Writers read these fields to determine how to partition data and which indexes to maintain.</p> <p>Merge progress (<code>merged_generations</code>) tracks the last generation merged to the base table for each region. This field is updated atomically with merge-insert data commits, enabling conflict resolution when multiple mergers operate concurrently. Each entry contains the region UUID and generation number.</p> <p>Index catchup progress (<code>index_catchup</code>) tracks which merged generation each base table index has been rebuilt to cover. When data is merged from a flushed MemTable to the base table, the base table's indexes are rebuilt asynchronously. During this window, queries should use the flushed MemTable's pre-built indexes instead of scanning unindexed data in the base table. See Index Catchup and Read Path for details.</p> <p>Region snapshot fields (<code>snapshot_ts_millis</code>, <code>num_regions</code>, <code>inline_snapshots</code>) provide a point-in-time snapshot of region states. The actual region manifests remain authoritative for region state. When <code>num_regions</code> is 0, the <code>inline_snapshots</code> field may be <code>None</code> or an empty Lance file with 0 rows but proper schema.</p> MemWalIndexDetails protobuf message <pre><code>message MemWalIndexDetails {\n  // Snapshot timestamp (Unix timestamp in milliseconds).\n  int64 snapshot_ts_millis = 1;\n\n  // Number of regions in the snapshot.\n  // Used to determine storage format without reading the snapshot data.\n  uint32 num_regions = 2;\n\n  // Inline region snapshots for small region counts.\n  // When num_regions &lt;= threshold (implementation-defined, e.g., 100),\n  // snapshots are stored inline as serialized bytes.\n  // Format: Lance file bytes with the region snapshot schema.\n  optional bytes inline_snapshots = 3;\n\n  // Region specs defining how to derive region identifiers.\n  // This configuration determines how rows are partitioned into regions.\n  repeated RegionSpec region_specs = 7;\n\n  // Indexes from the base table to maintain in MemTables.\n  // These are index names referencing indexes defined on the base table.\n  // The primary key btree index is always maintained implicitly and\n  // should not be listed here.\n  //\n  // For vector indexes, MemTables inherit quantization parameters (PQ codebook,\n  // SQ params) from the base table index to ensure distance comparability.\n  repeated string maintained_indexes = 8;\n\n  // Last generation merged to base table for each region.\n  // This is updated atomically with merge-insert data commits, enabling\n  // conflict resolution when multiple mergers operate concurrently.\n  //\n  // Note: This is separate from region snapshots because:\n  // 1. merged_generations is updated by mergers (atomic with data commit)\n  // 2. region snapshots are updated by background index builder\n  repeated MergedGeneration merged_generations = 9;\n\n  // Per-index catchup progress tracking.\n  // When data is merged to the base table, base table indexes are rebuilt\n  // asynchronously. This field tracks which generation each index covers.\n  //\n  // For indexed queries, if an index's caught_up_generation &lt; merged_generation,\n  // readers should use flushed MemTable indexes for the gap instead of\n  // scanning unindexed data in the base table.\n  //\n  // If an index is not present in this list, it is assumed to be fully caught up.\n  repeated IndexCatchupProgress index_catchup = 10;\n\n}\n</code></pre>"},{"location":"format/table/mem_wal/#region-identifier","title":"Region Identifier","text":"<p>Each region has a unique identifier across all regions following UUID v4 standard. When a new region is created, it is assigned a new identifier.</p>"},{"location":"format/table/mem_wal/#region-spec","title":"Region Spec","text":"<p>A Region Spec defines how all rows in a table are logically divided into different regions, enabling automatic region assignment and query-time region pruning.</p> <p>Each region spec has:</p> <ul> <li>Spec ID: A positive integer that uniquely identifies this spec within the MemWAL index. IDs are never reused.</li> <li>Region fields: An array of field definitions that determine how to compute region values.</li> </ul> <p>Each region is bound to a specific region spec ID, recorded in its manifest. Regions without a spec ID (<code>spec_id = 0</code>) are manually-created regions not governed by any spec.</p> <p>A region spec's field array consists of region field definitions. Each region field has the following properties:</p> Property Description <code>field_id</code> Unique string identifier for this region field <code>source_ids</code> Array of field IDs referencing source columns in the schema <code>transform</code> A well-known region expression, specify this or <code>expression</code> <code>expression</code> A DataFusion SQL expression for custom logic, specify this or <code>transform</code> <code>result_type</code> The output type of the region value"},{"location":"format/table/mem_wal/#region-expression","title":"Region Expression","text":"<p>A Region Expression is a DataFusion SQL expression that derives a region value from source column(s). Source columns are referenced as <code>col0</code>, <code>col1</code>, etc., corresponding to the order of field IDs in <code>source_ids</code>.</p> <p>Region expressions must satisfy the following requirements:</p> <ol> <li>Deterministic: The same input value must always produce the same output value.</li> <li>Stateless: The expression must not depend on external state (e.g., current time, random values, session variables).</li> <li>Type-promotion resistant: The expression must produce the same result for equivalent values regardless of their numeric type (e.g., <code>int32(5)</code> and <code>int64(5)</code> must yield the same region value).</li> <li>Column removal resistant: If a source field ID is not found in the schema, the column should be interpreted as NULL.</li> <li>NULL-safe: The expression should properly handle NULL inputs and have defined behavior (e.g., return NULL if input is NULL for single-column expressions).</li> <li>Consistent with result type: The expression's return type must be consistent with <code>result_type</code> in non-NULL cases.</li> </ol>"},{"location":"format/table/mem_wal/#region-transform","title":"Region Transform","text":"<p>A Region Transform is a well-known region expression with a predefined name. When a transform is specified, the expression is derived automatically.</p> Transform Parameters Region Expression Result Type <code>identity</code> (none) <code>col0</code> same as source <code>year</code> (none) <code>date_part('year', col0)</code> <code>int32</code> <code>month</code> (none) <code>date_part('month', col0)</code> <code>int32</code> <code>day</code> (none) <code>date_part('day', col0)</code> <code>int32</code> <code>hour</code> (none) <code>date_part('hour', col0)</code> <code>int32</code> <code>bucket</code> <code>num_buckets</code> <code>abs(murmur3(col0)) % N</code> <code>int32</code> <code>multi_bucket</code> <code>num_buckets</code> <code>abs(murmur3_multi(col0, col1, ...)) % N</code> <code>int32</code> <code>truncate</code> <code>width</code> <code>left(col0, W)</code> (string) or <code>col0 - (col0 % W)</code> (numeric) same as source <p>The <code>bucket</code> and <code>multi_bucket</code> transforms use Murmur3 hash functions:</p> <ul> <li><code>murmur3(col)</code>: Computes the 32-bit Murmur3 hash (x86 variant, seed 0) of a single column. Returns a signed 32-bit integer. Returns NULL if input is NULL.</li> <li><code>murmur3_multi(col0, col1, ...)</code>: Computes the Murmur3 hash across multiple columns. Returns a signed 32-bit integer. NULL fields are ignored during hashing; returns NULL only if all inputs are NULL.</li> </ul> <p>The hash result is wrapped with <code>abs()</code> and modulo <code>N</code> to produce a non-negative bucket number in the range <code>[0, N)</code>.</p>"},{"location":"format/table/mem_wal/#region-snapshot-storage","title":"Region Snapshot Storage","text":"<p>Region snapshots are stored using one of two strategies based on the number of regions:</p> Region Count Storage Strategy Location &lt;= 100 (threshold) Inline <code>inline_snapshots</code> field in index details &gt; 100 External Lance file <code>_indices/{UUID}/index.lance</code> <p>The threshold (100 regions) is implementation-defined and may vary.</p> <p>Inline storage: For small region counts, snapshots are serialized as a Lance file and stored in the <code>inline_snapshots</code> field. This keeps the index metadata compact while avoiding an additional file read for common cases.</p> <p>External Lance file: For large region counts, snapshots are stored as a Lance file at <code>_indices/{UUID}/index.lance</code>. This file uses standard Lance format with the region snapshot schema, enabling efficient columnar access and compression.</p>"},{"location":"format/table/mem_wal/#region-snapshot-arrow-schema","title":"Region Snapshot Arrow Schema","text":"<p>Region snapshots are stored as a Lance file with one row per region. The schema has one column per <code>RegionManifest</code> field plus region spec columns:</p> Column Type Description <code>region_id</code> <code>fixed_size_binary(16)</code> Region UUID bytes <code>version</code> <code>uint64</code> Region manifest version <code>region_spec_id</code> <code>uint32</code> Region spec ID (0 if manual) <code>writer_epoch</code> <code>uint64</code> Writer fencing token <code>replay_after_wal_id</code> <code>uint64</code> Last WAL entry flushed to MemTable <code>wal_id_last_seen</code> <code>uint64</code> Last WAL entry seen (hint) <code>current_generation</code> <code>uint64</code> Next generation to flush <code>flushed_generations</code> <code>list&lt;struct&lt;generation: uint64, path: string&gt;&gt;</code> Flushed MemTable paths <code>region_field_{field_id}</code> varies Region field value (one column per field in region spec) <p>For example, with a region spec containing a field <code>user_bucket</code> of type <code>int32</code>:</p> Column Type Description ... ... (base columns above) <code>region_field_user_bucket</code> <code>int32</code> Bucket value for this region <p>This schema directly corresponds to the fields in the <code>RegionManifest</code> protobuf message plus the computed region field values.</p>"},{"location":"format/table/mem_wal/#vector-index-configuration","title":"Vector Index Configuration","text":"<p>If the main use case is IVF family vector index, it is recommended to have these indexes on the Lance table before enabling MemWAL. This is because IVF index needs to remain the same quantization codebook (e.g. PQ codebook) across all the layers of the LSM tree for vector distance to be comparable.</p> <p>MemTables automatically inherit vector indexing from base table indexes. For each vector index on the base table, MemTable uses the same index type (IVF-PQ, IVF-SQ, etc.) with the same centroids and quantization parameters. This ensures distances are precise and comparable across generations.</p> <p>The base table vector index should not change the codebook once MemWAL is enabled. To switch codebooks, a migration is required: create another vector index with the new codebook, configure MemTable to maintain both indexes, and eventually drop the old index after all readers are using the new codebook and all MemTables have indexes using the new codebook.</p>"},{"location":"format/table/mem_wal/#storage-layout","title":"Storage Layout","text":"<p>Here is a recap of the storage layout with all the files and concepts defined so far:</p> <pre><code>{table_path}/\n\u251c\u2500\u2500 _indices/\n\u2502   \u2514\u2500\u2500 {index_uuid}/                    # MemWAL Index (uses standard index storage)\n\u2502       \u2514\u2500\u2500 index.lance                  # Serialized region snapshots (Lance file)\n\u2502\n\u2514\u2500\u2500 _mem_wal/\n    \u2514\u2500\u2500 {region_uuid}/                   # Region directory (UUID v4)\n        \u251c\u2500\u2500 manifest/\n        \u2502   \u251c\u2500\u2500 {bit_reversed_version}.binpb     # Serialized region manifest (bit-reversed naming)\n        \u2502   \u2514\u2500\u2500 version_hint.json                # Version hint file\n        \u251c\u2500\u2500 wal/\n        \u2502   \u251c\u2500\u2500 {bit_reversed_entry_id}.lance    # WAL data files (bit-reversed naming)\n        \u2502   \u2514\u2500\u2500 ...\n        \u2514\u2500\u2500 {random_hash}_gen_{i}/        # Flushed MemTable (generation i, random prefix)\n            \u251c\u2500\u2500 _versions/\n            \u2502   \u2514\u2500\u2500 {version}.manifest    # Table manifest (V2 naming scheme)\n            \u251c\u2500\u2500 _indices/                 # Indexes\n            \u2502   \u251c\u2500\u2500 {vector_index}/\n            \u2502   \u2514\u2500\u2500 {scalar_index}/\n            \u2514\u2500\u2500 bloom_filter.bin          # Primary key bloom filter\n</code></pre>"},{"location":"format/table/mem_wal/#writer-expectations","title":"Writer Expectations","text":"<p>A writer operates on a single region within a single process and may spawn asynchronous tasks for background operations like WAL flush and MemTable flush.</p>"},{"location":"format/table/mem_wal/#writer-configuration","title":"Writer Configuration","text":"<p>Writers can be configured with the following options that affect write behavior:</p> Option Description Durable write Each write is persisted to WAL before reporting success. Ensures no data loss on crash, but adds latency for object storage writes. Indexed write Each write refreshes MemTable indexes before reporting success. Ensures new data is immediately searchable via indexes, but adds indexing latency. <p>Both options can be enabled independently. When disabled:</p> <ul> <li>Non-durable writes buffer data in memory until a flush threshold is reached, accepting potential data loss on crash</li> <li>Non-indexed writes defer index updates, meaning newly written data may not appear in index-accelerated queries until the next index refresh</li> </ul>"},{"location":"format/table/mem_wal/#synchronous-vs-asynchronous-operations","title":"Synchronous vs Asynchronous Operations","text":"<p>Writer operations can be categorized by their synchronous or asynchronous nature:</p> Operation Mode Description Initialization &amp; Recovery Synchronous Claims region and replays WAL entries Write to MemTable Synchronous Data inserted into in-memory fragments WAL Flush Configurable Synchronous with durable writes, asynchronous otherwise Index Update Configurable Synchronous with indexed writes, asynchronous otherwise MemTable Flush Asynchronous Triggered by thresholds, runs in background"},{"location":"format/table/mem_wal/#initialization-recovery","title":"Initialization &amp; Recovery","text":"<p>A writer must claim a region before performing any write operations:</p> <ol> <li>Load the latest region manifest</li> <li>Increment <code>writer_epoch</code> by one</li> <li>Atomically write a new manifest</li> <li>If the write fails (another writer claimed the epoch), reload the manifest and retry with a higher epoch</li> <li>After initialization, read WAL entries sequentially from <code>replay_after_wal_id + 1</code> until not found</li> <li>Replay valid WAL entries (those with <code>writer_epoch</code> &lt;= current epoch) to reconstruct the MemTable with 1:1 WAL fragment mapping (each WAL entry becomes one MemTable fragment)</li> </ol> <p>After initialization, the writer updates the WAL fragment mapping as new WAL flushes occur.</p>"},{"location":"format/table/mem_wal/#write-operations","title":"Write Operations","text":"<p>Each write operation follows this sequence:</p> <ol> <li>Validate incoming records</li> <li>Insert records into the MemTable, creating an in-memory fragment (immediately queryable via full scan)</li> <li>Track the Lance data file in the new fragment for pending WAL flush</li> <li>Optionally trigger WAL flush based on size, count, or time thresholds</li> <li>For durable writes, wait for WAL flush to complete before returning</li> <li>For indexed writes, update MemTable indexes before returning:<ul> <li>Insert primary keys into the bloom filter</li> <li>For each vector column with a base table index: encode and insert using the same index type as base table</li> <li>For each index in <code>maintained_indexes</code>: update the corresponding index structure</li> </ul> </li> </ol>"},{"location":"format/table/mem_wal/#wal-flush","title":"WAL Flush","text":"<p>WAL flush batches pending MemTable fragments into a single Lance data file:</p> <ol> <li>Identify pending (unflushed) fragments in the MemTable</li> <li>Start writing the WAL entry to object storage</li> <li>Stream binary pages from each pending fragment's Lance data file directly to the WAL entry</li> <li>Write the footer containing batched data file metadata and <code>writer_epoch</code> in schema metadata</li> <li>Complete the WAL entry write atomically</li> <li>Mark fragments as flushed in the MemTable</li> <li>Update the WAL fragment mapping (MemTable fragment IDs in this batch -&gt; WAL entry ID and positions) for index remapping during MemTable Flush</li> </ol> <p>Note</p> <p>The region manifest is not updated on every WAL flush. The <code>wal_id_last_seen</code> field is a hint that can be updated:</p> <ol> <li>During MemTable flush - when the region manifest is updated anyway</li> <li>By a background index builder - which scans WAL entries and updates each region's <code>wal_id_last_seen</code></li> </ol> <p>This keeps the hot write path fast. On recovery, the writer reads WAL entries sequentially starting from <code>wal_id_last_seen + 1</code> to discover any WAL entries beyond what the manifest indicates.</p> <p>The WAL flush behavior depends on the durable write option:</p> Mode Behavior Result Durable write Flush immediately, wait for completion One or more Lance files per write Non-durable write Buffer until threshold, return immediately Batched Lance files (fewer S3 operations)"},{"location":"format/table/mem_wal/#memtable-indexing","title":"MemTable Indexing","text":"<p>MemTable indexing differs from base table indexing to balance write performance with query capability. Rather than maintaining all base table indexes, MemTables maintain a subset specified in the MemWAL Index.</p> <p>MemTables maintain a primary key bloom filter for efficiently checking whether a primary key exists in a generation. This enables staleness detection during search queries without requiring expensive point lookups.</p> <p>For vector indexes, MemTables use the same index type as the base table (e.g., IVF-PQ with the same centroids and PQ codebook). This ensures distances are precise and directly comparable across generations. The centroid assignment also impacts recall, so using the same centroids ensures consistent search quality.</p> <p>For full-text search indexes, MemTables inherit tokenizer configuration from base table indexes to ensure consistent tokenization across generations. Each generation maintains its own corpus statistics (document count, term frequencies) which are aggregated at query time for globally-comparable BM25 scores.</p> <p>When a MemTable is flushed to storage:</p> <ol> <li>Indexes are serialized to disk in the flushed MemTable's <code>_indices/</code> directory following the Lance table index format</li> <li>The primary key bloom filter is serialized to <code>bloom_filter.bin</code> in the generation directory</li> <li>The in-memory index structures may be retained as a cache for readers in the same process</li> </ol>"},{"location":"format/table/mem_wal/#wal-fragment-mapping-construction","title":"WAL Fragment Mapping Construction","text":"<p>The WAL fragment mapping tracks the correspondence between MemTable fragment IDs and WAL entry IDs. This mapping is essential for remapping indexes during MemTable flush, since indexes reference MemTable fragment IDs but the flushed MemTable references WAL entry IDs.</p> <p>The mapping is structured as: <code>MemTable fragment ID -&gt; (WAL entry ID, position within entry)</code></p> <p>Where:</p> <ul> <li>MemTable fragment ID: The fragment's position in the MemTable (0-indexed within the current generation)</li> <li>WAL entry ID: The WAL entry containing this fragment's data (relative to <code>replay_after_wal_id</code>)</li> <li>Position within entry: The fragment's position within the WAL entry (since multiple fragments may be batched)</li> </ul> <p>The mapping is updated in two scenarios:</p> <ol> <li>Initialization &amp; Recovery: During WAL replay, each replayed WAL entry creates MemTable fragments with 1:1 mapping (one fragment per WAL entry, position 0)</li> <li>WAL Flush: After flushing pending fragments to a new WAL entry, the mapping records which MemTable fragments were written to which WAL entry and their positions</li> </ol> <p>During MemTable flush, indexes are remapped by translating MemTable fragment IDs to the corresponding WAL entry references using this mapping.</p>"},{"location":"format/table/mem_wal/#memtable-flush","title":"MemTable Flush","text":"<p>Flushing the MemTable creates a new flushed MemTable (generation) with data and indexes:</p> <ol> <li>Generate a random 8-character hex prefix (e.g., <code>a1b2c3d4</code>)</li> <li>Create directory <code>_mem_wal/{region_uuid}/{random_hash}_gen_{current_generation}/</code></li> <li>Identify WAL entries to include (from <code>replay_after_wal_id + 1</code> to the last flushed entry)</li> <li>Create table manifest with <code>base_paths</code> pointing to the WAL directory</li> <li>Add fragment entries referencing WAL files via <code>base_id</code></li> <li>Remap indexes using the WAL fragment mapping:<ul> <li>Read index entries referencing MemTable fragment IDs</li> <li>Translate to flushed MemTable fragment IDs using the mapping</li> <li>Write remapped indexes to <code>_mem_wal/{region_uuid}/{random_hash}_gen_{current_generation}/_indices/</code></li> </ul> </li> <li>Write the manifest to <code>_mem_wal/{region_uuid}/{random_hash}_gen_{current_generation}/_versions/{version}.manifest</code> (using V2 naming scheme)</li> <li>Update the region manifest:<ul> <li>Advance <code>replay_after_wal_id</code> to the last flushed WAL entry</li> <li>Update <code>wal_id_last_seen</code></li> <li>Increment <code>current_generation</code></li> <li>Append <code>(current_generation, {random_hash}_gen_{current_generation})</code> to <code>flushed_generations</code></li> </ul> </li> </ol> <p>The random prefix ensures that flush retries write to a new directory, avoiding conflicts with partially written files from failed attempts. Only the directory recorded in <code>flushed_generations</code> is considered valid.</p> <p>If the writer crashes before completing MemTable flush, the new writer replays WAL entries into memory with 1:1 WAL fragment mapping, rebuilds the in-memory indexes, and can then perform a fresh MemTable flush with a new random prefix.</p>"},{"location":"format/table/mem_wal/#writer-fencing","title":"Writer Fencing","text":"<p>Before any manifest update (MemTable flush), a writer must verify its <code>writer_epoch</code> remains valid:</p> <ul> <li>If <code>local_writer_epoch == stored_writer_epoch</code>: The writer is still active and may proceed</li> <li>If <code>local_writer_epoch &lt; stored_writer_epoch</code>: The writer has been fenced and must abort</li> </ul> <p>Fenced writers must stop all operations immediately and notify pending writes of the failure.</p> <p>For a concrete example of fencing between two writers, see Appendix 1: Writer Fencing Example.</p>"},{"location":"format/table/mem_wal/#background-job-expectations","title":"Background Job Expectations","text":"<p>Background jobs run independently from writers and handle asynchronous maintenance tasks.</p>"},{"location":"format/table/mem_wal/#memtable-merger","title":"MemTable Merger","text":"<p>Flushed MemTables are merged to the base table in generation order using Lance's merge-insert operation.</p>"},{"location":"format/table/mem_wal/#merge-workflow","title":"Merge Workflow","text":"<ol> <li>Read <code>merged_generations[region_id]</code></li> <li>Load the region manifest and identify unmerged flushed MemTables from <code>flushed_generations</code>: those with generation numbers &gt; <code>merged_generations[region_id]</code></li> <li>For each flushed MemTable in ascending generation order:<ul> <li>Look up the directory path from <code>flushed_generations</code></li> <li>Open it as a Lance table</li> <li>Execute merge-insert into the base table, atomically updating the MemWAL Index:<ul> <li>Set <code>merged_generations[region_id]</code> to this generation</li> </ul> </li> <li>On commit conflict, apply conflict resolution rules</li> </ul> </li> <li>After merge, the flushed MemTable and its referenced WAL files may be garbage collected (see Garbage Collector)</li> </ol> <p>Ordered merge ensures correct upsert semantics: flushed MemTables with higher generation numbers overwrite those with lower numbers.</p>"},{"location":"format/table/mem_wal/#conflict-resolution-and-concurrency","title":"Conflict Resolution and Concurrency","text":"<p>Multiple mergers may operate on the same region concurrently. This is safe due to:</p> <ol> <li>Atomic update: <code>merged_generations</code> is updated atomically with the data commit</li> <li>Conflict resolution: When a merge-insert commit encounters a version conflict, the merger reads the conflicting commit's <code>merged_generations</code>. If <code>merged_generations[region_id] &gt;= my_generation</code>, abort without retry (data already merged or superseded). Otherwise, retry the commit as normal.</li> <li>Merge-insert idempotency: If two mergers merge the same generation before either commits, both write identical data (primary key upsert semantics)</li> </ol> <p>After aborting due to a conflict, reload the MemWAL Index and region manifest, then continue to the next unmerged generation.</p> <p><code>merged_generations</code> is the single source of truth for merge progress. If a merger crashes after committing, the next merger reads the MemWAL Index to determine which generations are already merged.</p> <p>For a concrete example, see Appendix 2: Concurrent Merger Example.</p>"},{"location":"format/table/mem_wal/#memwal-index-builder","title":"MemWAL Index Builder","text":"<p>A background process periodically builds a new region snapshot:</p> <ol> <li>Load the existing MemWAL Index to preserve configuration (<code>region_specs</code>, <code>maintained_indexes</code>) and merge progress (<code>merged_generations</code>)</li> <li>List all region directories under <code>_mem_wal/</code></li> <li>For each region:<ul> <li>Load the region manifest</li> <li>Scan WAL entries sequentially to find the actual last entry ID</li> <li>If the observed WAL ID is greater than <code>wal_id_last_seen</code>, update the region manifest (ignore errors since this is best-effort)</li> <li>Copy manifest fields (including <code>flushed_generations</code>) into a region snapshot row</li> </ul> </li> <li>Determine storage strategy based on region count:<ul> <li>If <code>num_regions &lt;= threshold</code>: Serialize as Lance file bytes to <code>inline_snapshots</code></li> <li>If <code>num_regions &gt; threshold</code>: Write as Lance file to <code>_indices/{UUID}/index.lance</code></li> </ul> </li> <li>Create new <code>MemWalIndexDetails</code> with preserved configuration, merge progress, and new region snapshots</li> <li>Update the table manifest with the new index metadata</li> </ol> <p>This process serves two purposes:</p> <ul> <li>Keeps <code>wal_id_last_seen</code> up-to-date in region manifests (since writers don't update it on every WAL flush)</li> <li>Provides readers with an efficient snapshot of all region states</li> </ul> <p>The build frequency is implementation-defined. More frequent builds reduce staleness but increase I/O overhead.</p>"},{"location":"format/table/mem_wal/#base-table-index-builder","title":"Base Table Index Builder","text":"<p>A background process rebuilds base table indexes to cover newly merged data and updates <code>index_catchup</code> progress in the MemWAL Index. Typically there is a dedicated builder for each index.</p> <p>The index builder workflow is expected to be: 1. Rebuild the base table index to the latest state, this automatically covers all merged generations 2. Read the current <code>merged_generations</code> 3. Update the MemWAL Index atomically:     - Set <code>index_catchup[index_name].caught_up_generations</code> to match <code>merged_generations</code> 4. On commit conflict, reload the MemWAL Index and retry</p>"},{"location":"format/table/mem_wal/#garbage-collector","title":"Garbage Collector","text":"<p>The garbage collector removes obsolete data from the region directory and updates the region manifest to remove entries from <code>flushed_generations</code> for deleted flushed MemTables.</p> <p>Eligible for deletion:</p> <ol> <li>Flushed MemTable directories: Generation directories where <code>generation &lt;= merged_generations[region_id]</code> AND <code>generation &lt;= min(index_catchup[I].caught_up_generation)</code> for all maintained indexes</li> <li>WAL data files: Files referenced only by deleted generations</li> <li>Old region manifest versions: Versions older than the current version minus a retention threshold</li> <li>Orphaned directories: Directories matching <code>*_gen_*</code> pattern but not in <code>flushed_generations</code> (from failed flush attempts)</li> </ol> <p>Index catchup consideration: Flushed MemTables must be retained until all base table indexes have caught up. Since flushed MemTables contain pre-built indexes, they are used for indexed queries when the base table index has not yet been rebuilt to cover the merged data. Only after all indexes in <code>maintained_indexes</code> have <code>caught_up_generation &gt;= generation</code> can a flushed MemTable be safely deleted.</p> <p>Time travel consideration: Garbage collection must not remove generations that are reachable by any retained base table version. When a reader opens an older table version, the MemWAL Index snapshot from that version references specific <code>merged_generations</code> values. Generations that satisfy <code>generation &gt; merged_generations[region_id]</code> for any retained table version must be preserved.</p> <p>Garbage collection must verify that no flushed MemTable still references a WAL file before deletion.</p>"},{"location":"format/table/mem_wal/#reader-expectations","title":"Reader Expectations","text":""},{"location":"format/table/mem_wal/#lsm-tree-merging-read","title":"LSM Tree Merging Read","text":"<p>Readers MUST merge results from multiple data sources (base table, flushed MemTables, in-memory MemTables) by primary key to ensure correctness.</p> <p>When the same primary key exists in multiple sources, the reader must keep only the newest version based on:</p> <ol> <li>Generation number (<code>_gen</code>): Higher generation wins. The base table has generation -1, MemTables have positive integers starting from 1.</li> <li>Row address (<code>_rowaddr</code>): Within the same generation, higher row address wins (later writes within a batch overwrite earlier ones).</li> </ol> <p>The ordering for \"newest\" is: highest <code>_gen</code> first, then highest <code>_rowaddr</code>.</p> <p>This deduplication is essential because:</p> <ul> <li>A row updated in a MemTable also exists (with older data) in the base table</li> <li>A flushed MemTable that has been merged to the base table may not yet be garbage collected, causing the same row to appear in both</li> <li>A single write batch may contain multiple updates to the same primary key</li> </ul> <p>Without proper merging, queries would return duplicate or stale rows.</p>"},{"location":"format/table/mem_wal/#reader-consistency","title":"Reader Consistency","text":"<p>Reader consistency depends on two factors: </p> <ol> <li>access to in-memory MemTables</li> <li>the source of region metadata (either through MemWAL index or region manifests)</li> </ol> <p>Strong consistency requires access to in-memory MemTables for all regions involved in the query and reading region manifests directly. Otherwise, the query is eventually consistent due to missing unflushed data or stale MemWAL Index snapshots.</p> <p>Note</p> <p>Reading a stale MemWAL Index does not impact correctness, only freshness:</p> <ul> <li>Merged MemTable still in index: If a flushed MemTable has been merged to the base table but still shows in the MemWAL index, readers query both. This results in some inefficiency for querying the same data twice, but LSM-tree merging ensures correct results since both contain the same data. The inefficiency is also compensated by the fact that the data is covered by index and we rarely end up scanning both data.</li> <li>Garbage collected MemTable still in index: If a flushed MemTable has been garbage collected, but is still in the MemWAL index, readers would fail to open it and skip it. This is also safe because if it is garbage collected, the data must already exist in the base table.</li> <li>Newly flushed MemTable not in index: If a newly flushed MemTable is added after the snapshot was built, it is not queried. The result is eventually consistent but correct for the snapshot's point in time.</li> </ul>"},{"location":"format/table/mem_wal/#query-planning","title":"Query Planning","text":""},{"location":"format/table/mem_wal/#memtable-collection","title":"MemTable Collection","text":"<p>The query planner collects datasets from multiple sources and assembles them for unified query execution. Datasets come from:</p> <ol> <li>base table (representing already-merged data)</li> <li>flushed MemTables (persisted but not yet merged)</li> <li>optionally in-memory MemTables (if accessible).</li> </ol> <p>Each dataset is tagged with a generation number: -1 for the base table, and positive integers for MemTable generations. Within a region, the generation number determines data freshness, with higher numbers representing newer data. Rows from different regions do not need deduplication since each primary key maps to exactly one region.</p> <p>The planner also collects bloom filters from each generation for staleness detection during search queries.</p>"},{"location":"format/table/mem_wal/#region-pruning","title":"Region Pruning","text":"<p>Before executing queries, if region spec is available, the planner evaluates filter predicates against region specs to determine which regions may contain matching data. This pruning step reduces the number of regions to scan.</p> <p>For each filter predicate:</p> <ol> <li>Extract predicates on columns used in region specs</li> <li>Evaluate which region values can satisfy the predicate</li> <li>Prune regions whose values cannot match</li> </ol> <p>For example, with a region spec using <code>bucket(user_id, 10)</code> and a filter <code>user_id = 123</code>:</p> <ol> <li>Compute <code>bucket(123, 10) = 3</code></li> <li>Only scan regions with bucket value 3</li> <li>Skip all other regions</li> </ol> <p>Region pruning applies to both scan queries and prefilters in search queries.</p>"},{"location":"format/table/mem_wal/#indexed-read-plan","title":"Indexed Read Plan","text":"<p>When data is merged from a flushed MemTable to the base table, the base table's indexes are rebuilt asynchronously by the base table index builders. During this window, the merged data exists in the base table but is not yet covered by the base table's indexes.</p> <p>Without special handling, indexed queries would fall back to expensive full scans for the unindexed part of the base table. To maintain indexed read performance, the query planner should use <code>index_catchup</code> progress to determine the optimal data source for each query.</p> <p>The key insight is that flushed MemTables serve as a bridge between the base table's index catchup and the current merged state. For a query that requires a specific index for acceleration, when <code>index_gen &lt; merged_gen</code>,  the generations in the gap <code>(index_gen, merged_gen]</code> have data already merged in the base table but are not covered by the base table's index. Since flushed MemTables contain pre-built indexes (created during MemTable flush), queries can use these indexes instead of scanning unindexed data in the base table. This ensures all reads remain indexed regardless of how far behind the async index builder is.</p> <p>See Appendix 4: Index Catchup Example for a detailed timeline showing how this works in practice.</p>"},{"location":"format/table/mem_wal/#query-execution","title":"Query Execution","text":"<p>Query execution unions datasets within each region and deduplicates by primary key according to LSM tree merge read.</p> <p>The next few subsections go through the query plan expectations using custom execution nodes optimized for MemWAL's data model.</p> <p>All query plans assume the following MemWAL setup:</p> <pre><code>base_table: shared across all regions (gen -1)\n\nregion_A:\n  gen 1: flushed_gen_1\n  gen 2: in_memory_memtable\n\nregion_B:\n  gen 1: flushed_gen_1\n  gen 2: flushed_gen_2\n  gen 3: in_memory_memtable\n</code></pre> <p>Existing Lance index optimizations (scalar indexes, fragment pruning, etc.) continue to apply within each scan and is omitted. See Appendix 3: Execution Nodes for uncommon execution nodes we use here for optimized performance.</p>"},{"location":"format/table/mem_wal/#scan-queries","title":"Scan Queries","text":"<p>For scan queries, the base table is scanned once and each region's MemTables are scanned separately. Deduplication happens per primary key across all generations.</p> <pre><code>DeduplicateExec: partition_by=[pk], order_by=[_gen DESC, _rowaddr DESC]\n  UnionExec\n    # Base table (shared)\n    ScanExec: base_table[gen=-1], filter=[pushed_down]\n    # Region A MemTables\n    ScanExec: region_A[gen=2], filter=[pushed_down]\n    ScanExec: region_A[gen=1], filter=[pushed_down]\n    # Region B MemTables\n    ScanExec: region_B[gen=3], filter=[pushed_down]\n    ScanExec: region_B[gen=2], filter=[pushed_down]\n    ScanExec: region_B[gen=1], filter=[pushed_down]\n</code></pre>"},{"location":"format/table/mem_wal/#point-lookups","title":"Point Lookups","text":"<p>Primary key-based point lookups first determine the target region using the region spec, then short-circuit by checking newest generations first within that region, falling back to the base table.</p> <p>Bloom filters optimize point lookups by skipping generations that definitely don't contain the key:</p> <ol> <li>Check the bloom filter for each MemTable generation (newest first)</li> <li>If the bloom filter returns negative, skip that generation (key definitely not present)</li> <li>If the bloom filter returns positive, try to take last matching row of that generation</li> <li>If the key is found, return immediately without checking older generations</li> </ol> <pre><code># After region pruning: only region_A needs to be checked\n# Bloom filters checked before each scan to skip unnecessary I/O\nCoalesceFirstExec: return_first_non_null\n  BloomFilterGuardExec: bf[region_A][gen=2]\n    TakeLastExec: region_A[gen=2], filter=[pk = target]\n  BloomFilterGuardExec: bf[region_A][gen=1]\n    TakeLastExec: region_A[gen=1], filter=[pk = target]\n  TakeLastExec: base_table[gen=-1], filter=[pk = target]\n</code></pre>"},{"location":"format/table/mem_wal/#vector-search-queries","title":"Vector Search Queries","text":"<p>Vector search uses bloom filters to detect stale results across all generations.</p> <pre><code>GlobalLimitExec: limit=k\n  SortExec: order_by=[_dist ASC]\n    FilterStaleExec: bloom_filters=[bf[region_A][gen=2], bf[region_A][gen=1], bf[region_B][gen=3], bf[region_B][gen=2], bf[region_B][gen=1]]\n      UnionExec\n        # Base table (shared)\n        KNNExec: base_table[gen=-1], k=k\n        # Region A MemTables\n        KNNExec: region_A[gen=2], k=k\n        KNNExec: region_A[gen=1], k=k\n        # Region B MemTables\n        KNNExec: region_B[gen=3], k=k\n        KNNExec: region_B[gen=2], k=k\n        KNNExec: region_B[gen=1], k=k\n</code></pre> <p>For each candidate from generation G, <code>FilterStaleExec</code> checks if the primary key exists in bloom filters of generations &gt; G. If found, the candidate is filtered out because a newer version exists that was not as relevant to the query.</p>"},{"location":"format/table/mem_wal/#full-text-search-queries","title":"Full-Text Search Queries","text":"<p>Full-text search aggregates corpus statistics across all generations for globally-comparable BM25 scores.</p> <pre><code>GlobalLimitExec: limit=k\n  SortExec: order_by=[_bm25 DESC]\n    FilterStaleExec: bloom_filters=[bf[region_A][gen=2], bf[region_A][gen=1], bf[region_B][gen=3], bf[region_B][gen=2], bf[region_B][gen=1]]\n      GlobalBM25Exec                       # Aggregates stats across all generations\n        UnionExec\n          # Base table (shared)\n          FTSExec: base_table[gen=-1], query=\"search terms\"\n          # Region A MemTables\n          FTSExec: region_A[gen=2], query=\"search terms\"\n          FTSExec: region_A[gen=1], query=\"search terms\"\n          # Region B MemTables\n          FTSExec: region_B[gen=3], query=\"search terms\"\n          FTSExec: region_B[gen=2], query=\"search terms\"\n          FTSExec: region_B[gen=1], query=\"search terms\"\n</code></pre> <p><code>GlobalBM25Exec</code> collects document counts and term frequencies from all FTS indexes, computes global BM25 parameters, and passes them to each <code>FTSExec</code> for comparable scoring.</p>"},{"location":"format/table/mem_wal/#appendices","title":"Appendices","text":""},{"location":"format/table/mem_wal/#appendix-1-writer-fencing-example","title":"Appendix 1: Writer Fencing Example","text":"<p>This example demonstrates how epoch-based fencing prevents data corruption when two writers compete for the same region.</p>"},{"location":"format/table/mem_wal/#initial-state","title":"Initial State","text":"<pre><code>Region manifest (version 1):\n  writer_epoch: 5\n  replay_after_wal_id: 10\n  wal_id_last_seen: 12\n</code></pre>"},{"location":"format/table/mem_wal/#scenario","title":"Scenario","text":"Step Writer A Writer B Manifest State 1 Loads manifest, sees epoch=5 epoch=5, version=1 2 Increments to epoch=6, writes manifest v2 epoch=6, version=2 3 Starts writing WAL entries 13, 14, 15 4 Loads manifest v2, sees epoch=6 epoch=6, version=2 5 Increments to epoch=7, writes manifest v3 epoch=7, version=3 6 Starts writing WAL entries 16, 17 7 Tries to flush MemTable, loads manifest 8 Sees epoch=7, but local epoch=6 9 Writer A is fenced! Aborts all operations 10 Continues writing normally epoch=7, version=3"},{"location":"format/table/mem_wal/#what-happens-to-writer-as-wal-entries","title":"What Happens to Writer A's WAL Entries?","text":"<p>Writer A wrote WAL entries 13, 14, 15 with <code>writer_epoch=6</code> in their schema metadata.</p> <p>When Writer B performs crash recovery or MemTable flush:</p> <ol> <li>Reads WAL entries sequentially starting from <code>replay_after_wal_id + 1</code> (entry 13)</li> <li>For each entry, checks existence using HEAD request on the bit-reversed filename</li> <li>Continues until an entry is not found (e.g., entry 18 doesn't exist)</li> <li>Finds entries 13, 14, 15, 16, 17</li> <li>Reads each file's <code>writer_epoch</code> from schema metadata</li> <li>Entries 13, 14, 15 have <code>writer_epoch=6</code> which is &lt;= current epoch (7) -&gt; valid, will be replayed</li> <li>Entries 16, 17 have <code>writer_epoch=7</code> -&gt; valid, will be replayed</li> </ol>"},{"location":"format/table/mem_wal/#key-points","title":"Key Points","text":"<ol> <li> <p>No data loss: Writer A's entries are not discarded. They were written with a valid epoch at the time and will be included in recovery.</p> </li> <li> <p>Consistency preserved: Writer A is prevented from making further writes that could conflict with Writer B.</p> </li> <li> <p>Orphaned files are safe: WAL files from fenced writers remain on storage and are replayed by the new writer. They are only garbage collected after being included in a flushed MemTable that has been merged.</p> </li> <li> <p>Epoch validation timing: Writers check their epoch before manifest updates (MemTable flush), not on every WAL write. This keeps the hot path fast while ensuring consistency at commit boundaries.</p> </li> </ol>"},{"location":"format/table/mem_wal/#appendix-2-concurrent-merger-example","title":"Appendix 2: Concurrent Merger Example","text":"<p>This example demonstrates how MemWAL Index and conflict resolution handle concurrent mergers safely.</p>"},{"location":"format/table/mem_wal/#initial-state_1","title":"Initial State","text":"<pre><code>MemWAL Index:\n  merged_generations: {region: 5}\n\nRegion manifest (version 1):\n  current_generation: 8\n  flushed_generations: [(6, \"abc123_gen_6\"), (7, \"def456_gen_7\")]\n</code></pre>"},{"location":"format/table/mem_wal/#scenario-1-racing-on-the-same-generation","title":"Scenario 1: Racing on the Same Generation","text":"<p>Two mergers both try to merge generation 6 concurrently.</p> Step Merger A Merger B MemWAL Index 1 Reads index: merged_gen=5 merged_gen=5 2 Reads region manifest 3 Starts merging gen 6 4 Reads index: merged_gen=5 merged_gen=5 5 Reads region manifest 6 Starts merging gen 6 7 Commits (merged_gen=6) merged_gen=6 8 Tries to commit 9 Conflict: reads new index 10 Sees merged_gen=6 &gt;= 6, aborts 11 Reloads, continues to gen 7 <p>Merger B's conflict resolution detected that generation 6 was already merged by checking the MemWAL Index in the conflicting commit.</p>"},{"location":"format/table/mem_wal/#scenario-2-crash-after-table-commit","title":"Scenario 2: Crash After Table Commit","text":"<p>Merger A crashes after committing to the table.</p> Step Merger A Merger B MemWAL Index 1 Reads index: merged_gen=5 merged_gen=5 2 Merges gen 6, commits merged_gen=6 3 CRASH merged_gen=6 4 Reads index: merged_gen=6 merged_gen=6 5 Reads region manifest 6 Skips gen 6 (already merged) 7 Merges gen 7, commits merged_gen=7 <p>The MemWAL Index is the single source of truth. Merger B correctly used it to determine that generation 6 was already merged.</p>"},{"location":"format/table/mem_wal/#key-points_1","title":"Key Points","text":"<ol> <li> <p>Single source of truth: <code>merged_generations</code> is the authoritative source for merge progress, updated atomically with data.</p> </li> <li> <p>Conflict resolution uses MemWAL Index: When a commit conflicts, the merger checks the conflicting commit's MemWAL Index.</p> </li> <li> <p>No progress regression: Because MemWAL Index is updated atomically with data, concurrent mergers cannot regress the merge progress.</p> </li> </ol>"},{"location":"format/table/mem_wal/#appendix-3-execution-nodes","title":"Appendix 3: Execution Nodes","text":"<p>This appendix describes custom execution nodes for MemWAL query execution.</p>"},{"location":"format/table/mem_wal/#deduplicateexec","title":"DeduplicateExec","text":"<p>Deduplicates rows by primary key, keeping the row with highest <code>(_gen, _rowaddr)</code>. Since each dataset has a fixed <code>_gen</code> and rows are naturally ordered by <code>_rowaddr</code>, this can be implemented as a streaming operator without full materialization.</p>"},{"location":"format/table/mem_wal/#takelastexec","title":"TakeLastExec","text":"<p>Efficiently finds the last matching row for a filter predicate without full scan. If the primary key has a btree index, directly queries the btree to get the result. Otherwise, scans fragments in reverse order and within each fragment takes the last matching row. Returns immediately upon finding a match, avoiding unnecessary I/O on earlier fragments.</p>"},{"location":"format/table/mem_wal/#coalescefirstexec","title":"CoalesceFirstExec","text":"<p>Returns the first non-empty result from multiple inputs with short-circuit evaluation. Inputs are evaluated lazily in order; on first non-empty result, remaining inputs are not evaluated.</p>"},{"location":"format/table/mem_wal/#filterstaleexec","title":"FilterStaleExec","text":"<p>Filters out rows that have a newer version in a higher generation. For each candidate with primary key <code>pk</code> from generation G, checks bloom filters of generations &gt; G. If the bloom filter indicates the key may exist in a newer generation, the candidate is filtered out. False positives from bloom filters may cause some valid results to be filtered, but this is acceptable for search workloads where approximate results are expected.</p>"},{"location":"format/table/mem_wal/#bloomfilterguardexec","title":"BloomFilterGuardExec","text":"<p>Guards a child execution node with a bloom filter check. Given a primary key, checks the bloom filter before executing the child node. If the bloom filter returns negative (key definitely not present), returns empty without executing the child. If the bloom filter returns positive (key may be present), executes the child node normally. Used in point lookups to skip unnecessary scans of generations that don't contain the target key.</p>"},{"location":"format/table/mem_wal/#appendix-4-index-catchup-example","title":"Appendix 4: Index Catchup Example","text":"<p>This example demonstrates how <code>index_catchup</code> enables indexed reads during async index rebuilding.</p>"},{"location":"format/table/mem_wal/#scenario-setup","title":"Scenario Setup","text":"<pre><code>Generation:         1       2       3       4       5       6\n                    |       |       |       |       |       |\nState:           merged  merged  merged  merged flushed  active\n                    |       |       |       |               |\nBase IVF index:  [--   covers 1-3   --]     |               |\n                            \u2191               \u2191               \u2191\n                        index_gen=3    merged_gen=4         |\n                                                      current_gen=6\n</code></pre> <p>In this example:</p> <ul> <li>Generations 1-4 have been merged to the base table (<code>merged_gen=4</code>)</li> <li>Base IVF index has only been rebuilt to cover generations 1-3 (<code>index_gen=3</code>)</li> <li>Generation 4 is in the base table but NOT covered by the base IVF index</li> <li>Generation 5 is flushed to disk (not yet merged to base table)</li> <li>Generation 6 is the active in-memory MemTable</li> </ul>"},{"location":"format/table/mem_wal/#example-read-strategy-for-vector-search","title":"Example Read Strategy for Vector Search","text":"<p>Without <code>index_catchup</code> tracking, the query planner would need to perform an expensive full scan on the base table for generation 4. With <code>index_catchup</code>, the planner knows exactly which data is indexed and can use flushed MemTable indexes for the gap:</p> Data Source Generations Strategy Base table with IVF index 1-3 Use base table's IVF index Flushed MemTable gen 4 4 Use flushed MemTable's IVF index Flushed MemTable gen 5 5 Use flushed MemTable's IVF index Active MemTable 6 Use in-memory IVF index <p>All data sources provide indexed access, maintaining query performance during async index rebuild.</p>"},{"location":"format/table/row_id_lineage/","title":"Row ID and Lineage Specification","text":""},{"location":"format/table/row_id_lineage/#overview","title":"Overview","text":"<p>Lance provides row identification and lineage tracking capabilities. Row addressing enables efficient random access to rows within the table through a physical location encoding. Stable row IDs provide persistent identifiers that remain constant throughout a row's lifetime, even as its physical location changes. Row version tracking records when rows were created and last modified, enabling incremental processing, change data capture, and time-travel queries.</p>"},{"location":"format/table/row_id_lineage/#row-identifier-forms","title":"Row Identifier Forms","text":"<p>A row in Lance has two forms of row identifiers:</p> <ul> <li>Row address - the current physical location of the row in the dataset.</li> <li>Row ID - a logical identifier of the row. When stable row IDs are enabled, this remains stable for the lifetime of a logical row. When disabled (default mode), it is exactly equal to the row address.</li> </ul>"},{"location":"format/table/row_id_lineage/#row-address","title":"Row Address","text":"<p>Row address is the physical location of a row in the table, represented as a 64-bit identifier composed of two 32-bit values:</p> <pre><code>row_address = (fragment_id &lt;&lt; 32) | local_row_offset\n</code></pre> <p>This addressing scheme enables efficient random access: given a row address, the fragment and offset are extracted with bit operations. Row addresses change when data is reorganized through compaction or updates.</p> <p>Row address is currently the primary form of identifier used for indexing purposes. Secondary indices (vector indices, scalar indices, full-text search indices) reference rows by their row addresses.</p> <p>Note</p> <p>Work to support stable row IDs in indices is in progress.</p>"},{"location":"format/table/row_id_lineage/#row-id","title":"Row ID","text":"<p>Row ID is a logical identifier for a row.</p>"},{"location":"format/table/row_id_lineage/#stable-row-id","title":"Stable Row ID","text":"<p>When a dataset is created with stable row IDs enabled, each row is assigned a unique auto-incrementing <code>u64</code> identifier that remains constant throughout the row's lifetime, even when the row's physical location (row address) changes. The <code>_rowid</code> system column exposes this logical identifier to users. See the next section for more details on assignment and update semantics.</p>"},{"location":"format/table/row_id_lineage/#historicalunstable-usage","title":"Historical/unstable usage","text":"<p>Historically, the term \"row id\" was often used to refer to the physical row address (<code>_rowaddr</code>), which is not stable across compaction or updates.</p> <p>Warning</p> <p>With the introduction of stable row IDs, there may still be places in code and documentation that mix the terms \"row ID\" and \"row address\" or \"row ID\" and \"stable row ID\".   Please raise a PR if you find any place incorrect or confusing.</p>"},{"location":"format/table/row_id_lineage/#stable-row-id_1","title":"Stable Row ID","text":""},{"location":"format/table/row_id_lineage/#row-id-assignment","title":"Row ID Assignment","text":"<p>Row IDs are assigned using a monotonically increasing <code>next_row_id</code> counter stored in the manifest.</p> <p>Assignment Protocol:</p> <ol> <li>Writer reads the current <code>next_row_id</code> from the manifest at the read version</li> <li>Writer assigns row IDs sequentially starting from <code>next_row_id</code> for new rows</li> <li>Writer updates <code>next_row_id</code> in the new manifest to <code>next_row_id + num_new_rows</code></li> <li>If commit fails due to conflict, writer rebases:</li> <li>Re-reads the new <code>next_row_id</code> from the latest version</li> <li>Reassigns row IDs to new rows using the updated counter</li> <li>Retries commit</li> </ol> <p>This protocol mirrors fragment ID assignment and ensures row IDs are unique across all table versions.</p>"},{"location":"format/table/row_id_lineage/#enabling-stable-row-ids","title":"Enabling Stable Row IDs","text":"<p>Stable row IDs are a dataset-level feature recorded in the table manifest.</p> <ul> <li>Stable row IDs must be enabled when the dataset is first created.</li> <li>Currently, they cannot be turned on later for an existing dataset. Attempts to write with <code>enable_stable_row_ids = true</code> against a dataset that was created without stable row IDs will not change the dataset's configuration.</li> <li>When stable row IDs are disabled, the <code>_rowid</code> column (if requested) is not stable and should not be used as a persistent identifier.</li> </ul> <p>Row-level version tracking (<code>_row_created_at_version</code>, <code>_row_last_updated_at_version</code>) and the row ID index described below are only available when stable row IDs are enabled.</p>"},{"location":"format/table/row_id_lineage/#row-id-behavior-on-updates","title":"Row ID Behavior on Updates","text":"<p>When stable row IDs are enabled, updates preserve the logical row ID and remap it to a new physical address instead of assigning a new ID.</p> <p>Update Workflow:</p> <ol> <li>Original row with <code>_rowid = R</code> exists at address <code>(F1, O1)</code>.</li> <li>An update operation writes a new physical row with the updated values at address <code>(F2, O2)</code>.</li> <li>The new physical row is assigned the same <code>_rowid = R</code>, so the logical identifier is preserved.</li> <li>The original physical row at <code>(F1, O1)</code> is marked deleted using the deletion vector for fragment <code>F1</code>.</li> <li>The row ID index for the new dataset version maps <code>_rowid = R</code> to <code>(F2, O2)</code>, and uses deletion vectors and fragment bitmaps to avoid returning the tombstoned row at <code>(F1, O1)</code>.</li> </ol> <p>This design keeps <code>_rowid</code> stable for the lifetime of a logical row while allowing physical storage and secondary indices to be maintained independently.</p>"},{"location":"format/table/row_id_lineage/#row-id-sequences","title":"Row ID Sequences","text":""},{"location":"format/table/row_id_lineage/#storage-format","title":"Storage Format","text":"<p>Row ID sequences are stored using the <code>RowIdSequence</code> protobuf message. The sequence is partitioned into segments, each encoded optimally based on the data pattern.</p> RowIdSequence protobuf message <pre><code>message RowIdSequence {\n    repeated U64Segment segments = 1;\n\n}\n</code></pre>"},{"location":"format/table/row_id_lineage/#segment-encodings","title":"Segment Encodings","text":"<p>Each segment uses one of five encodings optimized for different data patterns:</p>"},{"location":"format/table/row_id_lineage/#range-contiguous-values","title":"Range (Contiguous Values)","text":"<p>For sorted, contiguous values with no gaps. Example: Row IDs <code>[100, 101, 102, 103, 104]</code> \u2192 <code>Range{start: 100, end: 105}</code>. Used for new fragments where row IDs are assigned sequentially.</p> Range protobuf message <pre><code>message Range {\n    /// The start of the range, inclusive.\n    uint64 start = 1;\n    /// The end of the range, exclusive.\n    uint64 end = 2;\n}\n</code></pre>"},{"location":"format/table/row_id_lineage/#range-with-holes-sparse-deletions","title":"Range with Holes (Sparse Deletions)","text":"<p>For sorted values with few gaps. Example: Row IDs <code>[100, 101, 103, 104]</code> (missing 102) \u2192 <code>RangeWithHoles{start: 100, end: 105, holes: [102]}</code>. Used for fragments with sparse deletions where maintaining the range is efficient.</p> RangeWithHoles protobuf message <pre><code>message RangeWithHoles {\n    /// The start of the range, inclusive.\n    uint64 start = 1;\n    /// The end of the range, exclusive.\n    uint64 end = 2;\n    /// The holes in the range, as a sorted array of values;\n    /// Binary search can be used to check whether a value is a hole and should\n    /// be skipped. This can also be used to count the number of holes before a\n    /// given value, if you need to find the logical offset of a value in the\n    /// segment.\n    EncodedU64Array holes = 3;\n}\n</code></pre>"},{"location":"format/table/row_id_lineage/#range-with-bitmap-dense-deletions","title":"Range with Bitmap (Dense Deletions)","text":"<p>For sorted values with many gaps. The bitmap encodes 8 values per byte, with the most significant bit representing the first value. Used for fragments with dense deletion patterns.</p> RangeWithBitmap protobuf message <pre><code>message RangeWithBitmap {\n    /// The start of the range, inclusive.\n    uint64 start = 1;\n    /// The end of the range, exclusive.\n    uint64 end = 2;\n    /// A bitmap of the values in the range. The bitmap is a sequence of bytes,\n    /// where each byte represents 8 values. The first byte represents values\n    /// start to start + 7, the second byte represents values start + 8 to\n    /// start + 15, and so on. The most significant bit of each byte represents\n    /// the first value in the range, and the least significant bit represents\n    /// the last value in the range. If the bit is set, the value is in the\n    /// range; if it is not set, the value is not in the range.\n    bytes bitmap = 3;\n}\n</code></pre>"},{"location":"format/table/row_id_lineage/#sorted-array-sparse-values","title":"Sorted Array (Sparse Values)","text":"<p>For sorted but non-contiguous values, stored as an <code>EncodedU64Array</code>. Used for merged fragments or fragments after compaction.</p>"},{"location":"format/table/row_id_lineage/#unsorted-array-general-case","title":"Unsorted Array (General Case)","text":"<p>For unsorted values, stored as an <code>EncodedU64Array</code>. Rare; most operations maintain sorted order.</p>"},{"location":"format/table/row_id_lineage/#encoded-u64-arrays","title":"Encoded U64 Arrays","text":"<p>The <code>EncodedU64Array</code> message supports bitpacked encoding to minimize storage. The implementation selects the most compact encoding based on the value range, choosing between base + 16-bit offsets, base + 32-bit offsets, or full 64-bit values.</p> EncodedU64Array protobuf message <pre><code>message EncodedU64Array {\n    message U16Array {\n        uint64 base = 1;\n        /// The deltas are stored as 16-bit unsigned integers.\n        /// (protobuf doesn't support 16-bit integers, so we use bytes instead)\n        bytes offsets = 2;\n    }\n\n    message U32Array {\n        uint64 base = 1;\n        /// The deltas are stored as 32-bit unsigned integers.\n        /// (we use bytes instead of uint32 to avoid overhead of varint encoding)\n        bytes offsets = 2;\n    }\n\n    message U64Array {\n        /// (We use bytes instead of uint64 to avoid overhead of varint encoding)\n        bytes values = 2;\n    }\n\n    oneof array {\n        U16Array u16_array = 1;\n        U32Array u32_array = 2;\n        U64Array u64_array = 3;\n    }\n\n}\n</code></pre>"},{"location":"format/table/row_id_lineage/#inline-vs-external-storage","title":"Inline vs External Storage","text":"<p>Row ID sequences are stored either inline in the fragment metadata or in external files. Sequences smaller than ~200KB are stored inline to avoid additional I/O, while larger sequences are written to external files referenced by path and offset. This threshold balances manifest size against the overhead of separate file reads.</p> DataFragment row_id_sequence field <pre><code>message DataFragment {\n  oneof row_id_sequence {\n    bytes inline_row_ids = 5;\n    ExternalFile external_row_ids = 6;\n  }\n}\n</code></pre>"},{"location":"format/table/row_id_lineage/#row-id-index","title":"Row ID Index","text":""},{"location":"format/table/row_id_lineage/#construction","title":"Construction","text":"<p>The row ID index is built at table load time by aggregating row ID sequences from all fragments:</p> <pre><code>For each fragment F with ID f:\n  For each (position p, row_id r) in F.row_id_sequence:\n    index[r] = (f, p)\n</code></pre> <p>This creates a mapping from row ID to current row address.</p>"},{"location":"format/table/row_id_lineage/#index-invalidation-with-updates","title":"Index Invalidation with Updates","text":"<p>When rows are updated and stable row IDs are enabled, the row ID index for a given dataset version only contains mappings for live physical rows. Tombstoned rows are excluded using deletion vectors, and logical row IDs whose contents have changed simply map to new row addresses.</p> <p>Example Scenario:</p> <ol> <li>Initial state (version V): Fragment 1 contains rows with IDs <code>[1, 2, 3]</code> at offsets <code>[0, 1, 2]</code>.</li> <li>An update operation modifies the row with <code>_rowid = 2</code>:<ul> <li>A new fragment 2 is created with a row for <code>_rowid = 2</code> at offset <code>0</code>.</li> <li>In fragment 1, the original physical row at offset <code>1</code> is marked deleted in the deletion vector.</li> </ul> </li> <li>Row ID index in version V+1:<ul> <li><code>1 \u2192 (1, 0)</code> \u2713 Valid</li> <li><code>2 \u2192 (2, 0)</code> \u2713 Valid (updated row in fragment 2)</li> <li><code>3 \u2192 (1, 2)</code> \u2713 Valid</li> </ul> </li> </ol> <p>The address <code>(1, 1)</code> is no longer reachable via the row ID index because it is filtered out by the deletion vector when the index is constructed.</p>"},{"location":"format/table/row_id_lineage/#fragment-bitmaps-for-index-masking","title":"Fragment Bitmaps for Index Masking","text":"<p>Secondary indices use fragment bitmaps to track which row IDs remain valid:</p> <p>Without Row Updates:</p> <pre><code>String Index on column \"str\":\n  Fragment Bitmap: {1, 2}  (covers fragments 1 and 2)\n  All indexed row addresses are valid\n</code></pre> <p>With Row Updates:</p> <pre><code>Vector Index on column \"vec\":\n  Fragment Bitmap: {1}  (only fragment 1)\n  The row with _rowid = 2 was updated, so the index entry that points to its old physical address is stale\n  Index queries filter out the stale address using deletion vectors while returning the row at its new address\n</code></pre> <p>This bitmap-based approach allows indices to remain immutable while accounting for row modifications.</p>"},{"location":"format/table/row_id_lineage/#row-version-tracking","title":"Row Version Tracking","text":"<p>Row version tracking is available for datasets that use stable row IDs. Version sequences are aligned with the stable <code>_rowid</code> ordering within each fragment.</p>"},{"location":"format/table/row_id_lineage/#created-at-version","title":"Created At Version","text":"<p>Each row tracks the version at which it was created. For rows that are later updated, this creation version remains the version in which the row first appeared; updates do not change it. The sequence uses run-length encoding for efficient storage, where each run specifies a span of consecutive rows and the version they were created in.</p> <p>Example: Fragment with 1000 rows created in version 5: <pre><code>RowDatasetVersionSequence {\n  runs: [\n    RowDatasetVersionRun { span: Range{start: 0, end: 1000}, version: 5 }\n  ]\n}\n</code></pre></p> DataFragment created_at_version_sequence field <pre><code>message DataFragment {\n  oneof created_at_version_sequence {\n    bytes inline_created_at_versions = 9;\n    ExternalFile external_created_at_versions = 10;\n  }\n}\n</code></pre> RowDatasetVersionSequence protobuf messages <pre><code>message RowDatasetVersionSequence {\n    repeated RowDatasetVersionRun runs = 1;\n\n}\n</code></pre>"},{"location":"format/table/row_id_lineage/#last-updated-at-version","title":"Last Updated At Version","text":"<p>Each row tracks the version at which it was last modified. When a row is created, <code>last_updated_at_version</code> equals <code>created_at_version</code>.</p> <p>When stable row IDs are enabled and a row is updated, Lance writes a new physical row for the same logical <code>_rowid</code> while tombstoning the old physical row. The <code>created_at_version</code> for that logical row is preserved from the original row, and <code>last_updated_at_version</code> is set to the current dataset version at the time of the update.</p> <p>Example: Row created in version 3, updated in version 7: <pre><code>Old physical row (tombstoned):\n  _rowid: R\n  created_at_version: 3\n  last_updated_at_version: 3\n\nNew physical row (current):\n  _rowid: R\n  created_at_version: 3\n  last_updated_at_version: 7\n</code></pre></p> DataFragment last_updated_at_version_sequence field <pre><code>message DataFragment {\n  oneof last_updated_at_version_sequence {\n    bytes inline_last_updated_at_versions = 7;\n    ExternalFile external_last_updated_at_versions = 8;\n  }\n}\n</code></pre>"},{"location":"format/table/row_id_lineage/#change-data-feed","title":"Change Data Feed","text":"<p>Lance supports querying rows that changed between versions through version tracking columns. These queries can be expressed as standard SQL predicates on the <code>_row_created_at_version</code> and <code>_row_last_updated_at_version</code> columns.</p>"},{"location":"format/table/row_id_lineage/#inserted-rows","title":"Inserted Rows","text":"<p>Rows created between two versions can be retrieved by filtering on <code>_row_created_at_version</code>:</p> <pre><code>SELECT * FROM dataset\nWHERE _row_created_at_version &gt; {begin_version}\n  AND _row_created_at_version &lt;= {end_version}\n</code></pre> <p>This query returns all rows inserted in the specified version range, including the version metadata columns <code>_row_created_at_version</code>, <code>_row_last_updated_at_version</code>, and <code>_rowid</code>.</p>"},{"location":"format/table/row_id_lineage/#updated-rows","title":"Updated Rows","text":"<p>Rows modified (but not newly created) between two versions can be retrieved by combining filters on both version columns:</p> <pre><code>SELECT * FROM dataset\nWHERE _row_created_at_version &lt;= {begin_version}\n  AND _row_last_updated_at_version &gt; {begin_version}\n  AND _row_last_updated_at_version &lt;= {end_version}\n</code></pre> <p>This query excludes newly inserted rows by requiring <code>_row_created_at_version &lt;= {begin_version}</code>, ensuring only pre-existing rows that were subsequently updated are returned.</p>"},{"location":"format/table/transaction/","title":"Transaction Specification","text":""},{"location":"format/table/transaction/#transaction-overview","title":"Transaction Overview","text":"<p>Lance implements Multi-Version Concurrency Control (MVCC) to provide ACID transaction guarantees for concurrent readers and writers. Each commit creates a new immutable table version through atomic storage operations. All table versions form a serializable history, enabling features such as time travel and schema evolution.</p> <p>Transactions are the fundamental unit of change in Lance. A transaction describes a set of modifications to be applied atomically to create a new table version. The transaction model supports concurrent writes through optimistic concurrency control with automatic conflict resolution.</p>"},{"location":"format/table/transaction/#commit-protocol","title":"Commit Protocol","text":""},{"location":"format/table/transaction/#storage-primitives","title":"Storage Primitives","text":"<p>Lance commits rely on atomic write operations provided by the underlying object store:</p> <ul> <li>rename-if-not-exists: Atomically rename a file only if the target does not exist</li> <li>put-if-not-exists: Atomically write a file only if it does not already exist (also known as PUT-IF-NONE-MATCH or conditional PUT)</li> </ul> <p>These primitives guarantee that exactly one writer succeeds when multiple writers attempt to create the same manifest file concurrently.</p>"},{"location":"format/table/transaction/#manifest-naming-schemes","title":"Manifest Naming Schemes","text":"<p>Lance supports two manifest naming schemes:</p> <ul> <li>V1: <code>{version}.manifest</code> - Monotonically increasing version numbers (e.g., <code>1.manifest</code>, <code>2.manifest</code>)</li> <li>V2: <code>{u64::MAX - version:020}.manifest</code> - Reverse-sorted lexicographic ordering (e.g., <code>18446744073709551614.manifest</code> for version 1)</li> </ul> <p>The V2 scheme enables efficient discovery of the latest version through lexicographic object listing.</p>"},{"location":"format/table/transaction/#transaction-files","title":"Transaction Files","text":"<p>Transaction files store the serialized transaction protobuf message for each commit attempt. These files serve two purposes:</p> <ol> <li>Enable manifest reconstruction during commit retries when concurrent transactions have been committed</li> <li>Support conflict detection by describing the operation performed</li> </ol>"},{"location":"format/table/transaction/#commit-algorithm","title":"Commit Algorithm","text":"<p>The commit process attempts to atomically write a new manifest file using the storage primitives described above. When concurrent writers conflict, the system loads transaction files to detect conflicts and attempts to rebase the transaction if possible. If the atomic commit fails, the process retries with updated transaction state. For detailed conflict detection and resolution mechanisms, see the Conflict Resolution section.</p>"},{"location":"format/table/transaction/#transaction-types","title":"Transaction Types","text":"<p>The authoritative specification for transaction types is defined in <code>protos/transaction.proto</code>.</p> <p>Each transaction contains a <code>read_version</code> field indicating the table version from which the transaction was built, a <code>uuid</code> field uniquely identifying the transaction, and an <code>operation</code> field specifying one of the following transaction types:</p>"},{"location":"format/table/transaction/#append","title":"Append","text":"<p>Adds new fragments to the table without modifying existing data. Fragment IDs are not assigned at transaction creation time; they are assigned during manifest construction.</p> Append protobuf message <pre><code>message Append {\n  // The new fragments to append.\n  //\n  // Fragment IDs are not yet assigned.\n  repeated DataFragment fragments = 1;\n}\n</code></pre>"},{"location":"format/table/transaction/#delete","title":"Delete","text":"<p>Marks rows as deleted using deletion vectors. May update fragments (adding deletion vectors) or delete entire fragments. The <code>predicate</code> field stores the deletion condition, enabling conflict detection with concurrent transactions.</p> Delete protobuf message <pre><code>message Delete {\n  // The fragments to update\n  //\n  // The fragment IDs will match existing fragments in the dataset.\n  repeated DataFragment updated_fragments = 1;\n  // The fragments to delete entirely.\n  repeated uint64 deleted_fragment_ids = 2;\n  // The predicate that was evaluated\n  //\n  // This may be used to determine whether the delete would have affected \n  // files written by a concurrent transaction.\n  string predicate = 3;\n}\n</code></pre>"},{"location":"format/table/transaction/#overwrite","title":"Overwrite","text":"<p>Creates or completely overwrites the table with new data, schema, and configuration.</p> Overwrite protobuf message <pre><code>message Overwrite {\n  // The new fragments\n  //\n  // Fragment IDs are not yet assigned.\n  repeated DataFragment fragments = 1;\n  // The new schema\n  repeated lance.file.Field schema = 2;\n  // Schema metadata.\n  map&lt;string, bytes&gt; schema_metadata = 3;\n  // Key-value pairs to merge with existing config.\n  map&lt;string, string&gt; config_upsert_values = 4;\n  // The base paths to be added for the initial dataset creation\n  repeated BasePath initial_bases = 5;\n}\n</code></pre>"},{"location":"format/table/transaction/#createindex","title":"CreateIndex","text":"<p>Adds, replaces, or removes secondary indices (vector indices, scalar indices, full-text search indices).</p> CreateIndex protobuf message <pre><code>message CreateIndex {\n  repeated IndexMetadata new_indices = 1;\n  repeated IndexMetadata removed_indices = 2;\n}\n</code></pre>"},{"location":"format/table/transaction/#rewrite","title":"Rewrite","text":"<p>Reorganizes data without semantic modification. This includes operations such as compaction, defragmentation, and re-ordering. Rewrite operations change row addresses, requiring index updates. New fragment IDs must be reserved via <code>ReserveFragments</code> before executing a <code>Rewrite</code> transaction.</p> Rewrite protobuf message <pre><code>message Rewrite {\n  // The old fragments that are being replaced\n  //\n  // DEPRECATED: use groups instead.\n  //\n  // These should all have existing fragment IDs.\n  repeated DataFragment old_fragments = 1;\n  // The new fragments\n  //\n  // DEPRECATED: use groups instead.\n  //\n  // These fragments IDs are not yet assigned.\n  repeated DataFragment new_fragments = 2;\n\n  // During a rewrite an index may be rewritten.  We only serialize the UUID\n  // since a rewrite should not change the other index parameters.\n  message RewrittenIndex {\n    // The id of the index that will be replaced\n    UUID old_id = 1;\n    // the id of the new index\n    UUID new_id = 2;\n    // the new index details\n    google.protobuf.Any new_index_details = 3;\n    // the version of the new index\n    uint32 new_index_version = 4;\n  }\n\n  // A group of rewrite files that are all part of the same rewrite.\n  message RewriteGroup {\n    // The old fragment that is being replaced\n    //\n    // This should have an existing fragment ID.\n    repeated DataFragment old_fragments = 1;\n    // The new fragment\n    //\n    // The ID should have been reserved by an earlier\n    // reserve operation\n    repeated DataFragment new_fragments = 2;\n  }\n\n  // Groups of files that have been rewritten\n  repeated RewriteGroup groups = 3;\n  // Indices that have been rewritten\n  repeated RewrittenIndex rewritten_indices = 4;\n}\n</code></pre>"},{"location":"format/table/transaction/#merge","title":"Merge","text":"<p>Adds new columns to the table, modifying the schema. All fragments must be updated to include the new columns.</p> Merge protobuf message <pre><code>message Merge {\n  // The updated fragments\n  //\n  // These should all have existing fragment IDs.\n  repeated DataFragment fragments = 1;\n  // The new schema\n  repeated lance.file.Field schema = 2;\n  // Schema metadata.\n  map&lt;string, bytes&gt; schema_metadata = 3;\n}\n</code></pre>"},{"location":"format/table/transaction/#project","title":"Project","text":"<p>Removes columns from the table, modifying the schema. This is a metadata-only operation; data files are not modified.</p> Project protobuf message <pre><code>message Project {\n  // The new schema\n  repeated lance.file.Field schema = 1;\n}\n</code></pre>"},{"location":"format/table/transaction/#restore","title":"Restore","text":"<p>Reverts the table to a previous version.</p> Restore protobuf message <pre><code>message Restore {\n  // The version to restore to\n  uint64 version = 1;\n}\n</code></pre>"},{"location":"format/table/transaction/#reservefragments","title":"ReserveFragments","text":"<p>Pre-allocates fragment IDs for use in future <code>Rewrite</code> operations. This allows rewrite operations to reference fragment IDs before the rewrite transaction is committed.</p> ReserveFragments protobuf message <pre><code>message ReserveFragments {\n  uint32 num_fragments = 1;\n}\n</code></pre>"},{"location":"format/table/transaction/#clone","title":"Clone","text":"<p>Creates a shallow or deep copy of the table. Shallow clones are metadata-only copies that reference original data files through <code>base_paths</code>. Deep clones are full copies using object storage native copy operations (e.g., S3 CopyObject).</p> Clone protobuf message <pre><code>message Clone {\n  // - true:  Performs a metadata-only clone (copies manifest without data files).\n  //          The cloned dataset references original data through `base_paths`,\n  //          suitable for experimental scenarios or rapid metadata migration.\n  // - false: Performs a full deep clone using the underlying object storage's native\n  //          copy API (e.g., S3 CopyObject, GCS rewrite). This leverages server-side\n  //          bulk copy operations to bypass download/upload bottlenecks, achieving\n  //          near-linear speedup for large datasets (typically 3-10x faster than\n  //          manual file transfers). The operation maintains atomicity and data\n  //          integrity guarantees provided by the storage backend.\n  bool is_shallow = 1;\n  // the reference name in the source dataset\n  // in most cases it should be the branch or tag name in the source dataset\n  optional string ref_name = 2;\n  // the version of the source dataset for cloning\n  uint64 ref_version = 3;\n  // the absolute base path of the source dataset for cloning\n  string ref_path = 4;\n  // if the target dataset is a branch, this is the branch name of the target dataset\n  optional string branch_name = 5;\n}\n</code></pre>"},{"location":"format/table/transaction/#update","title":"Update","text":"<p>Modifies row values without adding or removing rows. Supports two execution modes: REWRITE_ROWS deletes rows in current fragments and rewrites them in new fragments, which is optimal when the majority of columns are modified or only a small number of rows are affected; REWRITE_COLUMNS fully rewrites affected columns within fragments by tombstoning old column versions, which is optimal when most rows are affected but only a subset of columns are modified.</p> Update protobuf message <pre><code>message Update {\n  // The fragments that have been removed. These are fragments where all rows\n  // have been updated and moved to a new fragment.\n  repeated uint64 removed_fragment_ids = 1;\n  // The fragments that have been updated.\n  repeated DataFragment updated_fragments = 2;\n  // The new fragments where updated rows have been moved to.\n  repeated DataFragment new_fragments = 3;\n  // The ids of the fields that have been modified.\n  repeated uint32 fields_modified = 4;\n  /// List of MemWAL region generations to mark as merged after this transaction\n  repeated MergedGeneration merged_generations = 5;\n  /// The fields that used to judge whether to preserve the new frag's id into\n  /// the frag bitmap of the specified indices.\n  repeated uint32 fields_for_preserving_frag_bitmap = 6;\n  // The mode of update\n  UpdateMode update_mode = 7;\n  // Filter for checking existence of keys in newly inserted rows, used for conflict detection.\n  // Only tracks keys from INSERT operations during merge insert, not updates.\n  optional KeyExistenceFilter inserted_rows = 8;\n}\n</code></pre>"},{"location":"format/table/transaction/#updateconfig","title":"UpdateConfig","text":"<p>Modifies table configuration, table metadata, schema metadata, or field metadata without changing data.</p> UpdateConfig protobuf message <pre><code>message UpdateConfig {\n  UpdateMap config_updates = 6;\n  UpdateMap table_metadata_updates = 7;\n  UpdateMap schema_metadata_updates = 8;\n  map&lt;int32, UpdateMap&gt; field_metadata_updates = 9;\n\n  // Deprecated -------------------------------\n  map&lt;string, string&gt; upsert_values = 1;\n  repeated string delete_keys = 2;\n  map&lt;string, string&gt; schema_metadata = 3;\n  map&lt;uint32, FieldMetadataUpdate&gt; field_metadata = 4;\n\n  message FieldMetadataUpdate {\n    map&lt;string, string&gt; metadata = 5;\n  }\n}\n</code></pre>"},{"location":"format/table/transaction/#datareplacement","title":"DataReplacement","text":"<p>Replaces data in specific column regions with new data files.</p> DataReplacement protobuf message <pre><code>message DataReplacement {\n  repeated DataReplacementGroup replacements = 1;\n}\n</code></pre>"},{"location":"format/table/transaction/#updatememwalstate","title":"UpdateMemWalState","text":"<p>Updates the state of MemWal indices (write-ahead log based indices).</p> UpdateMemWalState protobuf message <pre><code>message UpdateMemWalState {\n  // Regions and generations being marked as merged.\n  repeated MergedGeneration merged_generations = 1;\n}\n</code></pre>"},{"location":"format/table/transaction/#updatebases","title":"UpdateBases","text":"<p>Adds new base paths to the table, enabling reference to data files in additional locations.</p> UpdateBases protobuf message <pre><code>message UpdateBases {\n  // The new base paths to add to the manifest.\n  repeated BasePath new_bases = 1;\n}\n</code></pre>"},{"location":"format/table/transaction/#conflict-resolution","title":"Conflict Resolution","text":""},{"location":"format/table/transaction/#terminology","title":"Terminology","text":"<p>When concurrent transactions attempt to commit against the same read version, Lance employs conflict resolution to determine whether the transactions can coexist. Three outcomes are possible:</p> <ul> <li> <p>Rebasable: The transaction can be modified to incorporate concurrent changes while preserving its semantic intent.   The transaction is transformed to account for the concurrent modification, then the commit is retried automatically within the commit layer.</p> </li> <li> <p>Retryable: The transaction cannot be rebased, but the operation can be re-executed at the application level with updated data.   The implementation returns a retryable conflict error, signaling that the application should re-read the data and retry the operation.   The retried operation is expected to produce semantically equivalent results.</p> </li> <li> <p>Incompatible: The transactions conflict in a fundamental way where retrying would violate the operation's assumptions or produce semantically different results than expected.   The commit fails with a non-retryable error.   Callers should proceed with extreme caution if they decide to retry, as the transaction may produce different output than originally intended.</p> </li> </ul>"},{"location":"format/table/transaction/#rebase-mechanism","title":"Rebase Mechanism","text":"<p>The <code>TransactionRebase</code> structure tracks the state necessary to rebase a transaction against concurrent commits:</p> <ol> <li>Fragment tracking: Maintains a map of fragments as they existed at the transaction's read version, marking which require rewriting</li> <li>Modification detection: Tracks the set of fragment IDs that have been modified or deleted</li> <li>Affected rows: For Delete and Update operations, stores the specific rows affected by the operation for fine-grained conflict detection</li> <li>Fragment reuse indices: Accumulates fragment reuse index metadata from concurrent Rewrite operations</li> </ol> <p>When a concurrent transaction is detected, the rebase process:</p> <ol> <li>Compares fragment modifications to determine if there is overlap</li> <li>For Delete/Update operations, compares <code>affected_rows</code> to detect whether the same rows were modified</li> <li>Merges deletion vectors when both transactions delete rows from the same fragment</li> <li>Accumulates fragment reuse index updates when concurrent Rewrites change fragment IDs</li> <li>Modifies the transaction if rebasable, or returns a retryable/incompatible conflict error</li> </ol>"},{"location":"format/table/transaction/#conflict-scenarios","title":"Conflict Scenarios","text":""},{"location":"format/table/transaction/#rebasable-conflict-example","title":"Rebasable Conflict Example","text":"<p>The following diagram illustrates a rebasable conflict where two Delete operations modify different rows in the same fragment:</p> <pre><code>gitGraph\n    commit id: \"v1\"\n    commit id: \"v2\"\n    branch writer-a\n    branch writer-b\n    checkout writer-a\n    commit id: \"Delete rows 100-199\" tag: \"read_version=2\"\n    checkout writer-b\n    commit id: \"Delete rows 500-599\" tag: \"read_version=2\"\n    checkout main\n    merge writer-a tag: \"v3\"\n    checkout writer-b\n    commit id: \"Rebase: merge deletion vectors\" type: HIGHLIGHT\n    checkout main\n    merge writer-b tag: \"v4\"</code></pre> <p>In this scenario:</p> <ul> <li>Writer A deletes rows 100-199 and successfully commits version 3</li> <li>Writer B attempts to commit but detects version 3 exists</li> <li>Writer B's transaction is rebasable because it only modified deletion vectors (not data files) and <code>affected_rows</code> do not overlap</li> <li>Writer B rebases by merging Writer A's deletion vector with its own, write it to storage</li> <li>Writer B successfully commits version 4</li> </ul>"},{"location":"format/table/transaction/#retryable-conflict-example","title":"Retryable Conflict Example","text":"<p>The following diagram illustrates a retryable conflict where an Update operation encounters a concurrent Rewrite (compaction) that prevents automatic rebasing:</p> <pre><code>gitGraph\n    commit id: \"v1\"\n    commit id: \"v2\"\n    branch writer-a\n    branch writer-b\n    checkout writer-a\n    commit id: \"Compact fragments 1-5\" tag: \"read_version=2\"\n    checkout writer-b\n    commit id: \"Update rows in fragment 3\" tag: \"read_version=2\"\n    checkout main\n    merge writer-a tag: \"v3: fragments compacted\"\n    checkout writer-b\n    commit id: \"Detect conflict: cannot rebase\" type: REVERSE</code></pre> <p>In this scenario:</p> <ul> <li>Writer A compacts fragments 1-5 into a single fragment and successfully commits version 3</li> <li>Writer B attempts to update rows in fragment 3 but detects version 3 exists</li> <li>Writer B's Update transaction is retryable but not rebasable: fragment 3 no longer exists after compaction</li> <li>The commit layer returns a retryable conflict error</li> <li>The application must re-execute the Update operation against version 3, locating the rows in the new compacted fragment</li> </ul>"},{"location":"format/table/transaction/#incompatible-conflict-example","title":"Incompatible Conflict Example","text":"<p>The following diagram illustrates an incompatible conflict where a Delete operation encounters a concurrent Restore that fundamentally invalidates the operation:</p> <pre><code>gitGraph\n    commit id: \"v1\"\n    commit id: \"v2\"\n    commit id: \"v3\"\n    branch writer-a\n    branch writer-b\n    checkout writer-a\n    commit id: \"Restore to v1\" tag: \"read_version=3\"\n    checkout writer-b\n    commit id: \"Delete rows added in v2-v3\" tag: \"read_version=3\"\n    checkout main\n    merge writer-a tag: \"v4: restored to v1\"\n    checkout writer-b\n    commit id: \"Detect conflict: incompatible\" type: REVERSE</code></pre> <p>In this scenario:</p> <ul> <li>Writer A restores the table to version 1 and successfully commits version 4</li> <li>Writer B attempts to delete rows that were added between versions 2 and 3</li> <li>Writer B's Delete transaction is incompatible: the table has been restored to version 1, and the rows it intended to delete no longer exist</li> <li>The commit fails with a non-retryable error</li> <li>If the caller retries the deletion operation against version 4, it would either delete nothing (if those rows don't exist in v1) or delete different rows (if similar row IDs exist in v1), producing semantically different results than originally intended</li> </ul>"},{"location":"format/table/transaction/#external-manifest-store","title":"External Manifest Store","text":"<p>If the backing object store does not support atomic operations (rename-if-not-exists or put-if-not-exists), an external manifest store can be used to enable concurrent writers.</p> <p>An external manifest store is a key-value store that supports put-if-not-exists operations. The external manifest store supplements but does not replace the manifests in object storage. A reader unaware of the external manifest store can still read the table, but may observe a version up to one commit behind the true latest version.</p>"},{"location":"format/table/transaction/#commit-process-with-external-store","title":"Commit Process with External Store","text":"<p>The commit process follows a four-step protocol:</p> <p></p> <ol> <li> <p>Stage manifest: <code>PUT_OBJECT_STORE {dataset}/_versions/{version}.manifest-{uuid}</code></p> <ul> <li>Write the new manifest to object storage under a unique path determined by a new UUID</li> <li>This staged manifest is not yet visible to readers</li> </ul> </li> <li> <p>Commit to external store: <code>PUT_EXTERNAL_STORE base_uri, version, {dataset}/_versions/{version}.manifest-{uuid}</code></p> <ul> <li>Atomically commit the path of the staged manifest to the external store using put-if-not-exists</li> <li>The commit is effectively complete after this step</li> <li>If this operation fails due to conflict, another writer has committed this version</li> </ul> </li> <li> <p>Finalize in object store: <code>COPY_OBJECT_STORE {dataset}/_versions/{version}.manifest-{uuid} \u2192 {dataset}/_versions/{version}.manifest</code></p> <ul> <li>Copy the staged manifest to the final path</li> <li>This makes the manifest discoverable by readers unaware of the external store</li> </ul> </li> <li> <p>Update external store pointer: <code>PUT_EXTERNAL_STORE base_uri, version, {dataset}/_versions/{version}.manifest</code></p> <ul> <li>Update the external store to point to the finalized manifest path</li> <li>Completes the synchronization between external store and object storage</li> </ul> </li> </ol> <p>Fault Tolerance:</p> <p>If the writer fails after step 2 but before step 4, the external store and object store are temporarily out of sync. Readers detect this condition and attempt to complete the synchronization. If synchronization fails, the reader refuses to load to ensure dataset portability.</p>"},{"location":"format/table/transaction/#reader-process-with-external-store","title":"Reader Process with External Store","text":"<p>The reader follows a validation and synchronization protocol:</p> <p></p> <ol> <li> <p>Query external store: <code>GET_EXTERNAL_STORE base_uri, version</code> \u2192 <code>path</code></p> <ul> <li>Retrieve the manifest path for the requested version</li> <li>If the path does not end with a UUID, return it directly (synchronization complete)</li> <li>If the path ends with a UUID, synchronization is required</li> </ul> </li> <li> <p>Synchronize to object store: <code>COPY_OBJECT_STORE {dataset}/_versions/{version}.manifest-{uuid} \u2192 {dataset}/_versions/{version}.manifest</code></p> <ul> <li>Attempt to finalize the staged manifest</li> <li>This operation is idempotent</li> </ul> </li> <li> <p>Update external store: <code>PUT_EXTERNAL_STORE base_uri, version, {dataset}/_versions/{version}.manifest</code></p> <ul> <li>Update the external store to reflect the finalized path</li> <li>Future readers will see the synchronized state</li> </ul> </li> <li> <p>Return finalized path: Return <code>{dataset}/_versions/{version}.manifest</code></p> <ul> <li>Always return the finalized path</li> <li>If synchronization fails, return an error to prevent reading inconsistent state</li> </ul> </li> </ol> <p>This protocol ensures that datasets using external manifest stores remain portable: copying the dataset directory preserves all data without requiring the external store.</p>"},{"location":"format/table/versioning/","title":"Format Versioning","text":""},{"location":"format/table/versioning/#feature-flags","title":"Feature Flags","text":"<p>As the table format evolves, new feature flags are added to the format. There are two separate fields for checking for feature flags, depending on whether you are trying to read or write the table. Readers should check the <code>reader_feature_flags</code> to see if there are any flag it is not aware of. Writers should check <code>writer_feature_flags</code>. If either sees a flag they don't know, they should return an \"unsupported\" error on any read or write operation.</p>"},{"location":"format/table/versioning/#current-feature-flags","title":"Current Feature Flags","text":"Flag Bit Flag Name Reader Required Writer Required Description 1 <code>FLAG_DELETION_FILES</code> Yes Yes Fragments may contain deletion files, which record the tombstones of soft-deleted rows. 2 <code>FLAG_STABLE_ROW_IDS</code> Yes Yes Row IDs are stable for both moves and updates. Fragments contain an index mapping row IDs to row addresses. 4 <code>FLAG_USE_V2_FORMAT_DEPRECATED</code> No No Files are written with the new v2 format. This flag is deprecated and no longer used. 8 <code>FLAG_TABLE_CONFIG</code> No Yes Table config is present in the manifest. 16 <code>FLAG_BASE_PATHS</code> Yes Yes Dataset uses multiple base paths (for shallow clones or multi-base datasets). <p>Flags with bit values 32 and above are unknown and will cause implementations to reject the dataset with an \"unsupported\" error.</p>"},{"location":"format/table/index/","title":"Indices in Lance","text":"<p>Lance supports three main categories of indices to accelerate data access: scalar indices, vector indices, and system indices.</p> <p>Scalar indices are traditional indices that speed up queries on scalar data types, such as  integers and strings. Examples include B-trees and full-text search indices. Typically, scalar indices receive a query predicate, such as equality or range conditions, and output a set of row addresses that satisfy the predicate.</p> <p>Vector indices are specialized for approximate nearest neighbor (ANN) search on high-dimensional vector data, such as embeddings from machine learning models. Examples includes IVF (Inverted File) indices and HNSW (Hierarchical Navigable Small World) indices. These are separate from scalar indices because they use meaningfully different query patterns. Instead of sargable predicates, vector indices receive a query vector and return the nearest neighbor row addresses based on some distance metric, such as Euclidean distance or cosine similarity. They return row addresses and the corresponding distances.</p> <p>System indices are auxiliary indices that help accelerate internal system operations. They are different from user-facing scalar and vector indices, as they are not directly used in user queries. Examples include the Fragment Reuse Index, which supports efficient row address remapping after compaction.</p>"},{"location":"format/table/index/#design","title":"Design","text":"<p>Lance indices are designed with the following design choices in mind:</p> <ol> <li>Indexes are loaded on demand: A dataset and be loaded and read without loading any indices.    Indices are only loaded when a query can benefit from them.    This design minimizes memory usage and speeds up dataset opening time.</li> <li>Indexes can be loaded progressively: indexes are designed so that only the necessary parts    are loaded into memory during query execution. For example, when querying a B-tree index,    it loads a small page table to figure out which pages of the index to load for the given query,    and then only loads those pages to perform the indexed search. This amortizes the cost of    cold index queries, since each query only needs to load a small portion of the index.</li> <li>Indexes can be coalesced to larger units than fragments. Indexes are much smaller than    data files, so it is efficient to coalesce index segments to cover multiple fragments.    This reduces the number of index files that need to be opened during query execution and    then number of unique index data structures that need to be queried.</li> <li>Index files are immutable once written, similar to data files. They can be modified only    by creating new files. This means they can be safely cached in memory or on disk without    worrying about consistency issues.</li> </ol>"},{"location":"format/table/index/#basic-concepts","title":"Basic Concepts","text":"<p>An index in Lance is defined over a specific column (or multiple columns) of a dataset. It is identified by its name.</p> <p>An index is made up of multiple index segments, identified by their unique UUIDs. Each segment is an independent, self-contained index covering a subset of the data.</p> <p>Each index segment covers a disjoint subset of fragments in the dataset. The segments must cover all rows in the fragments they cover, with one exception: if a fragment has delete markers at the time of index creation, the index segment is allowed to not contain the deleted rows. The fragments an index covers are those recorded in the <code>fragment_bitmap</code> field.</p> <p>Index segments together do not need to cover all fragments. This means an index isn't required to  be fully up-to-date. When this happens, engines can split their queries into indexed and unindexed subplans and merge the results.</p> Abstract layout of a typical dataset, with three fragments and two indices.    <p>Consider the example dataset in the figure above:</p> <ul> <li>The dataset contains three fragments with ids 0, 1, 2. Fragment 1 has 10 deleted rows, indicated   by the deletion file.</li> <li>There is an index called \"id_idx\", which has two segments: one covering fragments 0 and another covering   fragment 1. Fragment 2 is not covered by the index. Queries using this index will need to query both   segments and then scan fragment 2 directly. Additionally, when querying the segment covering fragment 1,   the engine will need to filter out the 10 deleted rows.</li> <li>There is another index called \"vec_idx\", which has a single segment covering all three fragments.   Because it covers all fragments, queries using this index do not need to scan any fragments directly.   They do, however, need to filter out the 10 deleted rows from fragment 1.</li> </ul>"},{"location":"format/table/index/#index-storage","title":"Index Storage","text":"<p>The content of each index is stored at the <code>_indices/{UUID}</code> directory under the base path. We call this location the index directory. The actual content stored in the index directory depends on the index type. These can be arbitrary files defined by the index implementation. However, often they are made up of Lance files containing the index data structures. This allows reuse of the existing Lance file format code for reading and writing index data.</p>"},{"location":"format/table/index/#creating-and-updating-index-segments","title":"Creating and Updating Index Segments","text":"<p>Index segments are created and updated through a transactional process:</p> <ol> <li> <p>Build the index data: Read the relevant column data from the fragments to be indexed    and construct the index data structures. Write these to files in a new <code>_indices/{UUID}</code>    directory, where <code>{UUID}</code> is a newly generated unique identifier.</p> </li> <li> <p>Prepare the metadata: Create an <code>IndexMetadata</code> message with:</p> <ul> <li><code>uuid</code>: The newly generated UUID</li> <li><code>name</code>: The index name (must match existing segments if adding to an existing index)</li> <li><code>fields</code>: The column(s) being indexed</li> <li><code>fragment_bitmap</code>: The set of fragment IDs covered by this segment</li> <li><code>index_details</code>: Index-specific configuration and parameters</li> <li><code>version</code>: The format version of this index type</li> <li>See the full protobuf definition in table.proto.</li> </ul> </li> <li> <p>Commit the transaction: Write a new manifest that includes the new index segment    in its <code>IndexSection</code>. This is done atomically using the same transaction mechanism    as data writes.</p> </li> </ol> <p>When updating an indexed column in place (without deleting the row), the engine must remove the affected fragment IDs from the <code>fragment_bitmap</code> field of any index segments that cover those fragments. This marks those fragments as needing re-indexing without invalidating the entire segment and prevents invalid data from being read from the index.</p>"},{"location":"format/table/index/#index-compatibility","title":"Index Compatibility","text":"<p>Before using an index segment, engines must verify they support it:</p> <ol> <li> <p>Check the index type: The <code>index_details</code> field contains a protobuf <code>Any</code> message    whose type URL identifies the index type (e.g., B-tree, IVF, HNSW). If the engine    does not recognize the type, it should skip this index segment.</p> </li> <li> <p>Check the version: The <code>version</code> field in <code>IndexMetadata</code> indicates the format    version of the index segment. If the engine does not support this version, it should    skip this index segment. This allows index formats to evolve over time while    maintaining backwards compatibility.</p> </li> </ol> <p>When an engine cannot use an index segment, it should fall back to scanning the fragments that would have been covered by that segment.</p>"},{"location":"format/table/index/#loading-an-index","title":"Loading an index","text":"<p>When loading an index:</p> <ol> <li>Get the offset to the index section from the <code>index_section</code> field in the manifest.</li> <li>Read the index section from the manifest file. This is a protobuf message of type <code>IndexSection</code>, which    contains a list of <code>IndexMetadata</code> messages, each describing an index segment.</li> <li>Read the index files from the <code>_indices/{UUID}</code> directory under the dataset directory,    where <code>{UUID}</code> is the UUID of the index segment.</li> </ol> <p>Optimizing manifest loading</p> <p>When the manifest file is small, you can read and cache the index section eagerly. This avoids an extra file read when loading indices.</p> <p>The <code>IndexMetadata</code> message contains important information about the index segment:</p> <ul> <li><code>uuid</code>: the unique identifier of the index segment.</li> <li><code>fields</code>: the column(s) the index is built on.</li> <li><code>fragment_bitmap</code>: the set of fragment IDs covered by this index segment.</li> <li><code>index_details</code>: a protobuf <code>Any</code> message that contains index-specific details, such as index type,   parameters, and storage format. This allows different index types to store their own metadata.</li> </ul> Full protobuf definitions  There are both part of the `table.proto` file in the Lance source code.  <pre><code>message IndexSection {\n  repeated IndexMetadata indices = 1;\n\n}\n\nmessage IndexMetadata {\n  // Unique ID of an index. It is unique across all the dataset versions.\n  UUID uuid = 1;\n\n  // The columns to build the index. These refer to file.Field.id.\n  repeated int32 fields = 2;\n\n  // Index name. Must be unique within one dataset version.\n  string name = 3;\n\n  // The version of the dataset this index was built from.\n  uint64 dataset_version = 4;\n\n  // A bitmap of the included fragment ids.\n  //\n  // This may by used to determine how much of the dataset is covered by the\n  // index. This information can be retrieved from the dataset by looking at\n  // the dataset at `dataset_version`. However, since the old version may be\n  // deleted while the index is still in use, this information is also stored\n  // in the index.\n  //\n  // The bitmap is stored as a 32-bit Roaring bitmap.\n  bytes fragment_bitmap = 5;\n\n  // Details, specific to the index type, which are needed to load / interpret the index\n  //\n  // Indices should avoid putting large amounts of information in this field, as it will\n  // bloat the manifest.\n  //\n  // Indexes are plugins, and so the format of the details message is flexible and not fully\n  // defined by the table format.  However, there are some conventions that should be followed:\n  //\n  // - When Lance APIs refer to indexes they will use the type URL of the index details as the\n  //   identifier for the index type.  If a user provides a simple string identifier like\n  //   \"btree\" then it will be converted to \"/lance.table.BTreeIndexDetails\"\n  // - Type URLs comparisons are case-insensitive.  Thereform an index must have a unique type\n  //   URL ignoring case.\n  google.protobuf.Any index_details = 6;\n\n  // The minimum lance version that this index is compatible with.\n  optional int32 index_version = 7;\n\n  // Timestamp when the index was created (UTC timestamp in milliseconds since epoch)\n  //\n  // This field is optional for backward compatibility. For existing indices created before\n  // this field was added, this will be None/null.\n  optional uint64 created_at = 8;\n\n  // The base path index of the data file. Used when the file is imported or referred from another dataset.\n  // Lance use it as key of the base_paths field in Manifest to determine the actual base path of the data file.\n  optional uint32 base_id = 9;\n\n}\n</code></pre>"},{"location":"format/table/index/#handling-deleted-and-invalidated-rows","title":"Handling deleted and invalidated rows","text":"<p>Since index segments are immutable, they may contain references to rows that have been deleted or updated. These should be filtered out during query execution.</p> Representation of index segment covering fragments that have deleted rows,   completely deleted fragments, and updated fragments.    <p>There are three situations to consider:</p> <ol> <li>A fragment has some deleted rows. A few of the rows in the fragment have been marked    as deleted, but some of the rows are still present. The row addresses from the deletion    file should be used to filter out results from the index.</li> <li>A fragment has been completely deleted. This can be detected by checking if a    fragment ID present in the fragment bitmap is missing from the dataset.    Any row addresses from this fragment should be filtered out.</li> <li>A fragment has had the indexed column updated in place. This cannot be detected just    by examining metadata. To prevent reading invalid data, the engine should filter out any    row addresses that are not in the index's current <code>fragment_bitmap</code>.</li> </ol>"},{"location":"format/table/index/#compaction-and-remapping","title":"Compaction and remapping","text":"<p>When fragments are compacted, the row addresses of the rows in the fragments change. This means that any index segments referencing those fragments will no longer point to existing row addresses. There are three ways to handle this:</p> <ol> <li> <p>Do nothing and let the index segment not cover those fragments anymore. This approach is    simple and valid, but it means compaction can immediately make an index out-of-date. This    is the worst options for query performance.</p> </li> <li> <p>Immediately rewrite the index segments with the row addresses remapped. This approach    ensures the index is kept up-to-date, but it incurs significant write amplification    during compaction.</p> </li> <li> <p>Create a Fragment Reuse Index that maps old row addresses to new    row addresses. This allows readers to remap the row addresses in memory upon reading    the index segments. This approach adds some IO and computation overhead during query    execution, but avoids write amplification during compaction.</p> </li> </ol>"},{"location":"format/table/index/#stable-row-id-for-index","title":"Stable Row ID for Index","text":"<p>Indices can optionally use stable row IDs instead of row addresses. A stable row ID is a logical identifier that remains constant even when rows are moved during compaction.</p> <p>Benefits:</p> <ul> <li>No remapping needed after compaction</li> <li>Updates only invalidate the index if the indexed column data changes</li> </ul> <p>Tradeoffs:</p> <ul> <li>Requires an additional lookup to translate stable row IDs to physical row addresses   at query time</li> </ul> <p>This feature is currently experimental. Performance evaluation is ongoing to determine when the tradeoff is worthwhile.</p>"},{"location":"format/table/index/scalar/bitmap/","title":"Bitmap Index","text":"<p>Bitmap indices use bit arrays to represent the presence or absence of values, providing extremely fast query performance for low-cardinality columns.</p>"},{"location":"format/table/index/scalar/bitmap/#index-details","title":"Index Details","text":"<pre><code>message BitmapIndexDetails {\n\n}\n</code></pre>"},{"location":"format/table/index/scalar/bitmap/#storage-layout","title":"Storage Layout","text":"<p>The bitmap index consists of a single file <code>bitmap_page_lookup.lance</code> that stores the mapping from values to their bitmaps.</p>"},{"location":"format/table/index/scalar/bitmap/#file-schema","title":"File Schema","text":"Column Type Nullable Description <code>keys</code> {DataType} true The unique value from the indexed column <code>bitmaps</code> Binary true Serialized RowAddrTreeMap containing row addrs where this value appears"},{"location":"format/table/index/scalar/bitmap/#accelerated-queries","title":"Accelerated Queries","text":"Query Type Description Operation Equals <code>column = value</code> Returns the bitmap for the specific value Range <code>column BETWEEN a AND b</code> Unions all bitmaps for values in the range IsIn <code>column IN (v1, v2, ...)</code> Unions bitmaps for all specified values IsNull <code>column IS NULL</code> Returns the pre-computed null bitmap"},{"location":"format/table/index/scalar/bloom_filter/","title":"Bloom Filter Index","text":"<p>Bloom filters are probabilistic data structures that allow for fast membership testing. They are space-efficient and can test whether an element is a member of a set. It's an inexact filter - they may include false positives but never false negatives.</p>"},{"location":"format/table/index/scalar/bloom_filter/#index-details","title":"Index Details","text":"<pre><code>message BloomFilterIndexDetails {\n\n}\n</code></pre>"},{"location":"format/table/index/scalar/bloom_filter/#storage-layout","title":"Storage Layout","text":"<p>The bloom filter index stores zone-based bloom filters in a single file:</p> <ol> <li><code>bloomfilter.lance</code> - Bloom filter statistics and data for each zone</li> </ol>"},{"location":"format/table/index/scalar/bloom_filter/#bloom-filter-file-schema","title":"Bloom Filter File Schema","text":"Column Type Nullable Description <code>fragment_id</code> UInt64 false Fragment containing this zone <code>zone_start</code> UInt64 false Starting row offset within the fragment <code>zone_length</code> UInt64 false Number of rows in this zone <code>has_null</code> Boolean false Whether this zone contains any null values <code>bloom_filter_data</code> Binary false Serialized SBBF (Split Block Bloom Filter) data"},{"location":"format/table/index/scalar/bloom_filter/#schema-metadata","title":"Schema Metadata","text":"Key Type Description <code>bloomfilter_item</code> String Expected number of items per zone (default: \"8192\") <code>bloomfilter_probability</code> String False positive probability (default: \"0.00057\", ~1 in 1754)"},{"location":"format/table/index/scalar/bloom_filter/#bloom-filter-spec","title":"Bloom Filter Spec","text":"<p>The bloom filter index uses a Split Block Bloom Filter (SBBF) implementation, which is optimized for SIMD operations.</p>"},{"location":"format/table/index/scalar/bloom_filter/#sbbf-structure","title":"SBBF Structure","text":"<p>The SBBF divides the bit array into blocks of 256 bits, where each block consists of 8 contiguous 32-bit words. This structure enables efficient SIMD operations and cache-friendly memory access patterns. The block layout is the following:</p> <ul> <li>Block size: 256 bits (32 bytes)</li> <li>Words per block: 8 \u00d7 32-bit integers</li> <li>Minimum filter size: 32 bytes (1 block)</li> <li>Maximum filter size: 128 MiB</li> </ul>"},{"location":"format/table/index/scalar/bloom_filter/#hashing-mechanism","title":"Hashing Mechanism","text":"<p>The SBBF uses xxHash64 with seed=0 for primary hashing, combined with a salt-based secondary hashing scheme:</p> <ol> <li>Primary hash: xxHash64(value) \u2192 64-bit hash</li> <li>Block selection: Upper 32 bits determine which block to use</li> <li>Bit selection: Lower 32 bits combined with 8 salt values set 8 bits in the block</li> </ol>"},{"location":"format/table/index/scalar/bloom_filter/#salt-values","title":"Salt Values","text":"<pre><code>0x47b6137b\n0x44974d91\n0x8824ad5b\n0xa2b7289d\n0x705495c7\n0x2df1424b\n0x9efc4947\n0x5c6bfb31\n</code></pre> <p>Each salt value generates one bit position within the block, ensuring uniform distribution.</p>"},{"location":"format/table/index/scalar/bloom_filter/#filter-sizing-algorithm","title":"Filter Sizing Algorithm","text":"<p>The SBBF automatically determines optimal filter size based on: - NDV (Number of Distinct Values): Expected unique items - FPP (False Positive Probability): Target error rate</p> <p>The implementation uses binary search to find the minimum log\u2082(bytes) that achieves the desired FPP, using Putze et al.'s cache-efficient bloom filter formula.</p>"},{"location":"format/table/index/scalar/bloom_filter/#fpp-convergence","title":"FPP Convergence","text":"<p>The implementation uses up to 750 iterations of Poisson distribution calculations to ensure accurate FPP estimation, particularly for dense filters where NDV approaches filter capacity.</p>"},{"location":"format/table/index/scalar/bloom_filter/#serialization","title":"Serialization","text":"<p>The SBBF is serialized as a contiguous byte array stored in the <code>bloom_filter_data</code> column:</p> <pre><code>[Block 0][Block 1]...[Block N-1]\n</code></pre> <p>Where each block is 32 bytes:</p> <pre><code>[Word 0][Word 1][Word 2][Word 3][Word 4][Word 5][Word 6][Word 7]\n</code></pre> <p>Each word is a 32-bit little-endian integer (4 bytes), with:</p> <ul> <li>Total size: Must be a multiple of 32 bytes</li> <li>Byte order: Little-endian for all 32-bit words</li> <li>Block alignment: Each block starts at offset <code>i * 32</code></li> <li>Word offset: Word <code>j</code> in block <code>i</code> is at byte offset <code>i * 32 + j * 4</code></li> </ul>"},{"location":"format/table/index/scalar/bloom_filter/#example","title":"Example","text":"<p>For a filter with 2 blocks (64 bytes total): <pre><code>Offset  0-3:   Block 0, Word 0 (32-bit LE)\nOffset  4-7:   Block 0, Word 1 (32-bit LE)\n...\nOffset 28-31:  Block 0, Word 7 (32-bit LE)\nOffset 32-35:  Block 1, Word 0 (32-bit LE)\n...\nOffset 60-63:  Block 1, Word 7 (32-bit LE)\n</code></pre></p>"},{"location":"format/table/index/scalar/bloom_filter/#accelerated-queries","title":"Accelerated Queries","text":"<p>The bloom filter index provides inexact results for the following query types:</p> Query Type Description Operation Result Type Equals <code>column = value</code> Tests if value exists in bloom filter AtMost IsIn <code>column IN (v1, v2, ...)</code> Tests if any value exists in bloom filter AtMost IsNull <code>column IS NULL</code> Returns zones where has_null is true AtMost"},{"location":"format/table/index/scalar/btree/","title":"BTree Index","text":"<p>The BTree index is a two-level structure that provides efficient range queries and sorted access.  It strikes a balance between an expensive memory structure containing all values  and an expensive disk structure that can't be efficiently searched.</p> <p>The upper layers of the BTree are designed to be cached in memory and stored in a  BTree structure (<code>page_lookup.lance</code>), while the leaves are searched using sub-indices  (<code>page_data.lance</code>, currently just a flat file).  This design enables efficient memory usage - for example, with 1 billion values,  the index can store 256K leaves of size 4K each, requiring only a few MiB of memory  (depending on data type) for the BTree metadata while narrowing any search to just 4K values.</p>"},{"location":"format/table/index/scalar/btree/#index-details","title":"Index Details","text":"<pre><code>message BTreeIndexDetails {\n\n}\n</code></pre>"},{"location":"format/table/index/scalar/btree/#storage-layout","title":"Storage Layout","text":"<p>The BTree index consists of two files:</p> <ol> <li><code>page_lookup.lance</code> - The BTree structure mapping value ranges to page numbers</li> <li><code>page_data.lance</code> - The actual sub-indices (flat file) containing sorted values and row IDs</li> </ol>"},{"location":"format/table/index/scalar/btree/#page-lookup-file-schema-btree-structure","title":"Page Lookup File Schema (BTree Structure)","text":"Column Type Nullable Description <code>min</code> {DataType} true Minimum value in the page (forms BTree keys) <code>max</code> {DataType} true Maximum value in the page (for range pruning) <code>null_count</code> UInt32 false Number of null values in the page <code>page_idx</code> UInt32 false Page number pointing to the sub-index in page_data.lance"},{"location":"format/table/index/scalar/btree/#schema-metadata","title":"Schema Metadata","text":"Key Type Description <code>batch_size</code> String Number of rows per page (default: \"4096\")"},{"location":"format/table/index/scalar/btree/#page-data-file-schema-sub-indices","title":"Page Data File Schema (Sub-indices)","text":"Column Type Nullable Description <code>values</code> {DataType} true Sorted values from the indexed column (flat file) <code>ids</code> UInt64 false Row IDs corresponding to each value"},{"location":"format/table/index/scalar/btree/#accelerated-queries","title":"Accelerated Queries","text":"<p>The BTree index provides exact results for the following query types:</p> Query Type Description Operation Equals <code>column = value</code> BTree lookup to find relevant pages, then search within sub-indices Range <code>column BETWEEN a AND b</code> BTree traversal for pages overlapping the range, then search each sub-index IsIn <code>column IN (v1, v2, ...)</code> Multiple BTree lookups, union results from all matching sub-indices IsNull <code>column IS NULL</code> Returns rows from all pages where null_count &gt; 0"},{"location":"format/table/index/scalar/fts/","title":"Full Text Search Index","text":"<p>The full text search (FTS) index (a.k.a. inverted index) provides efficient text search by mapping terms to the documents containing them. It's designed for high-performance text search with support for various scoring algorithms and phrase queries.</p>"},{"location":"format/table/index/scalar/fts/#index-details","title":"Index Details","text":"<pre><code>message InvertedIndexDetails {\n  // Marking this field as optional as old versions of the index store blank details and we\n  // need to make sure we have a proper optional field to detect this.\n  optional string base_tokenizer = 1;\n  string language = 2;\n  bool with_position = 3;\n  optional uint32 max_token_length = 4;\n  bool lower_case = 5;\n  bool stem = 6;\n  bool remove_stop_words = 7;\n  bool ascii_folding = 8;\n  uint32 min_ngram_length = 9;\n  uint32 max_ngram_length = 10;\n  bool prefix_only = 11;\n\n}\n</code></pre>"},{"location":"format/table/index/scalar/fts/#storage-layout","title":"Storage Layout","text":"<p>The FTS index consists of multiple files storing the token dictionary, document information, and posting lists:</p> <ol> <li><code>tokens.lance</code> - Token dictionary mapping tokens to token IDs</li> <li><code>docs.lance</code> - Document metadata including token counts</li> <li><code>invert.lance</code> - Compressed posting lists for each token</li> <li><code>metadata.lance</code> - Index metadata and configuration</li> </ol>"},{"location":"format/table/index/scalar/fts/#token-dictionary-file-schema","title":"Token Dictionary File Schema","text":"Column Type Nullable Description <code>_token</code> Utf8 false The token string <code>_token_id</code> UInt32 false Unique identifier for the token"},{"location":"format/table/index/scalar/fts/#document-file-schema","title":"Document File Schema","text":"Column Type Nullable Description <code>_rowid</code> UInt64 false Document row ID <code>_num_tokens</code> UInt32 false Number of tokens in the document"},{"location":"format/table/index/scalar/fts/#fts-list-file-schema","title":"FTS List File Schema","text":"Column Type Nullable Description <code>_posting</code> List false Compressed posting lists (delta-encoded row IDs and frequencies) <code>_max_score</code> Float32 false Maximum score for the token (for query optimization) <code>_length</code> UInt32 false Number of documents containing the token <code>_compressed_position</code> List&gt; true Optional compressed position lists for phrase queries"},{"location":"format/table/index/scalar/fts/#metadata-file-schema","title":"Metadata File Schema","text":"<p>The metadata file contains JSON-serialized configuration and partition information:</p> Key Type Description <code>partitions</code> Array List of partition IDs for distributed index organization <code>params</code> JSON Object Serialized InvertedIndexParams with tokenizer config"},{"location":"format/table/index/scalar/fts/#invertedindexparams-structure","title":"InvertedIndexParams Structure","text":"Field Type Default Description <code>base_tokenizer</code> String \"simple\" Base tokenizer type (see Tokenizers section) <code>language</code> String \"English\" Language for stemming and stop words <code>with_position</code> Boolean false Store term positions for phrase queries (increases index size) <code>max_token_length</code> UInt32? None Maximum token length (tokens longer than this are removed) <code>lower_case</code> Boolean true Convert tokens to lowercase <code>stem</code> Boolean false Apply language-specific stemming <code>remove_stop_words</code> Boolean false Remove common stop words for the specified language <code>ascii_folding</code> Boolean true Convert accented characters to ASCII equivalents <code>min_gram</code> UInt32 2 Minimum n-gram length (only for ngram tokenizer) <code>max_gram</code> UInt32 15 Maximum n-gram length (only for ngram tokenizer) <code>prefix_only</code> Boolean false Generate only prefix n-grams (only for ngram tokenizer)"},{"location":"format/table/index/scalar/fts/#tokenizers","title":"Tokenizers","text":"<p>The full text search index supports multiple tokenizer types for different text processing needs:</p>"},{"location":"format/table/index/scalar/fts/#base-tokenizers","title":"Base Tokenizers","text":"Tokenizer Description Use Case simple Splits on whitespace and punctuation, removes non-alphanumeric characters General text (default) whitespace Splits only on whitespace characters Preserve punctuation raw No tokenization, treats entire text as single token Exact matching ngram Breaks text into overlapping character sequences Substring/fuzzy search jieba/* Chinese text tokenizer with word segmentation Chinese text lindera/* Japanese text tokenizer with morphological analysis Japanese text"},{"location":"format/table/index/scalar/fts/#jieba-tokenizer-chinese","title":"Jieba Tokenizer (Chinese)","text":"<p>Jieba is a popular Chinese text segmentation library that uses a dictionary-based approach with statistical methods for word segmentation.</p> <ul> <li>Configuration: Uses a <code>config.json</code> file in the model directory</li> <li>Models: Must be downloaded and placed in the Lance home directory under <code>jieba/</code></li> <li>Usage: Specify as <code>jieba/&lt;model_name&gt;</code> or just <code>jieba</code> for the default model</li> <li>Config Structure:   <pre><code>{\n  \"main\": \"path/to/main/dictionary\",\n  \"users\": [\"path/to/user/dict1\", \"path/to/user/dict2\"]\n}\n</code></pre></li> <li>Features:</li> <li>Accurate word segmentation for Simplified and Traditional Chinese</li> <li>Support for custom user dictionaries</li> <li>Multiple segmentation modes (precise, full, search engine)</li> </ul>"},{"location":"format/table/index/scalar/fts/#lindera-tokenizer-japanese","title":"Lindera Tokenizer (Japanese)","text":"<p>Lindera is a morphological analysis tokenizer specifically designed for Japanese text. It provides proper word segmentation for Japanese, which doesn't use spaces between words.</p> <ul> <li>Configuration: Uses a <code>config.yml</code> file in the model directory</li> <li>Models: Must be downloaded and placed in the Lance home directory under <code>lindera/</code></li> <li>Usage: Specify as <code>lindera/&lt;model_name&gt;</code> where <code>&lt;model_name&gt;</code> is the subdirectory containing the model files</li> <li>Features:</li> <li>Morphological analysis with part-of-speech tagging</li> <li>Dictionary-based tokenization</li> <li>Support for custom user dictionaries</li> </ul>"},{"location":"format/table/index/scalar/fts/#token-filters","title":"Token Filters","text":"<p>Token filters are applied in sequence after the base tokenizer:</p> Filter Description Configuration RemoveLong Removes tokens exceeding max_token_length <code>max_token_length</code> LowerCase Converts tokens to lowercase <code>lower_case</code> (default: true) Stemmer Reduces words to their root form <code>stem</code>, <code>language</code> StopWords Removes common words like \"the\", \"is\", \"at\" <code>remove_stop_words</code>, <code>language</code> AsciiFolding Converts accented characters to ASCII <code>ascii_folding</code> (default: true)"},{"location":"format/table/index/scalar/fts/#supported-languages","title":"Supported Languages","text":"<p>For stemming and stop word removal, the following languages are supported: Arabic, Danish, Dutch, English, Finnish, French, German, Greek, Hungarian, Italian, Norwegian, Portuguese, Romanian, Russian, Spanish, Swedish, Tamil, Turkish</p>"},{"location":"format/table/index/scalar/fts/#document-type","title":"Document Type","text":"<p>Lance supports 2 kinds of documents: text and json. Different document types have different tokenization rules, and parse tokens in different format.</p>"},{"location":"format/table/index/scalar/fts/#text-type","title":"Text Type","text":"<p>Text type includes text and list of text. Tokens are generated by base_tokenizer.</p> <p>The example below shows how text document is parsed into tokens.  <pre><code>Tom lives in San Francisco.\n</code></pre></p> <p>The tokens are below. <pre><code>Tom\nlives\nin\nSan\nFrancisco\n</code></pre></p>"},{"location":"format/table/index/scalar/fts/#json-type","title":"Json Type","text":"<p>Json is a nested structure, lance breaks down json document into tokens in triplet format <code>path,type,value</code>. The valid types are: str, number, bool, null.</p> <p>In scenarios where the triplet value is a str, the text value will be further tokenized using the base_tokenizer, resulting in multiple triplet tokens.</p> <p>During querying, the Json Tokenizer uses the triplet format instead of the json format, which simplifies the query syntax.</p> <p>The example below shows how the json document is tokenized. Assume we have the following json document: <pre><code>{\n  \"name\": \"Lance\",\n  \"legal.age\": 30,\n  \"address\": {\n    \"city\": \"San Francisco\",\n    \"zip:us\": 94102\n  }\n}\n</code></pre></p> <p>After parsing, the document will be tokenized into the following tokens: <pre><code>name,str,Lance\nlegal.age,number,30\naddress.city,str,San\naddress.city,str,Francisco\naddress.zip:us,number,94102\n</code></pre></p> <p>Then we do full text search in triplet format. To search for \"San Francisco,\" we can search with one of the triplets below: <pre><code>address.city:San Francisco\naddress.city:San\naddress.city:Francisco\n</code></pre></p>"},{"location":"format/table/index/scalar/fts/#accelerated-queries","title":"Accelerated Queries","text":"<p>Lance SDKs provide dedicated full text search APIs to leverage the FTS index capabilities.  These APIs support complex query types beyond simple token matching,  enabling sophisticated text search operations. Here are the query types enabled by the FTS index:</p> Query Type Description Example Usage Result Type contains_tokens Basic token-based search (UDF) with BM25 scoring and automatic result ranking SQL: <code>contains_tokens(column, 'search terms')</code> AtMost match Match query with configurable AND/OR operators and relevance scoring <code>{\"match\": {\"query\": \"text\", \"operator\": \"and/or\"}}</code> AtMost phrase Exact phrase matching with position information (requires <code>with_position: true</code>) <code>{\"phrase\": {\"query\": \"exact phrase\"}}</code> AtMost boolean Complex boolean queries with must/should/must_not clauses for sophisticated search logic <code>{\"boolean\": {\"must\": [...], \"should\": [...]}}</code> AtMost multi_match Search across multiple fields simultaneously with unified scoring <code>{\"multi_match\": [{\"field1\": \"query\"}, ...]}</code> AtMost boost Boost relevance scores for specific terms or queries by a configurable factor <code>{\"boost\": {\"query\": {...}, \"factor\": 2.0}}</code> AtMost"},{"location":"format/table/index/scalar/label_list/","title":"Label List Index","text":"<p>Label list indices are optimized for columns containing multiple labels or tags per row. They provide efficient set-based queries on multi-value columns using an underlying bitmap index.</p>"},{"location":"format/table/index/scalar/label_list/#index-details","title":"Index Details","text":"<pre><code>message LabelListIndexDetails {\n\n}\n</code></pre>"},{"location":"format/table/index/scalar/label_list/#storage-layout","title":"Storage Layout","text":"<p>The label list index uses a bitmap index internally and stores its data in:</p> <ol> <li><code>bitmap_page_lookup.lance</code> - Bitmap index mapping unique labels to row IDs</li> </ol>"},{"location":"format/table/index/scalar/label_list/#file-schema","title":"File Schema","text":"Column Type Nullable Description <code>keys</code> {DataType} true The unique label value from the indexed column <code>bitmaps</code> Binary true Serialized RowAddrTreeMap containing row addr where this label appears"},{"location":"format/table/index/scalar/label_list/#accelerated-queries","title":"Accelerated Queries","text":"<p>The label list index provides exact results for the following query types:</p> Query Type Description Operation Result Type array_has / array_contains Array contains the specified value Bitmap lookup for a single label Exact array_has_all Array contains all specified values Intersects bitmaps for all specified labels Exact array_has_any Array contains any of specified values Unions bitmaps for all specified labels Exact"},{"location":"format/table/index/scalar/ngram/","title":"N-gram Index","text":"<p>N-gram indices break text into overlapping sequences (trigrams) for efficient substring matching. They provide fast text search by indexing all 3-character sequences in the text after applying ASCII folding and lowercasing.</p>"},{"location":"format/table/index/scalar/ngram/#index-details","title":"Index Details","text":"<pre><code>message NGramIndexDetails {\n\n}\n</code></pre>"},{"location":"format/table/index/scalar/ngram/#storage-layout","title":"Storage Layout","text":"<p>The N-gram index stores tokenized text as trigrams with their posting lists:</p> <ol> <li><code>ngram_postings.lance</code> - Trigram tokens and their posting lists</li> </ol>"},{"location":"format/table/index/scalar/ngram/#file-schema","title":"File Schema","text":"Column Type Nullable Description <code>tokens</code> UInt32 true Hashed trigram token <code>posting_list</code> Binary false Compressed bitmap of row IDs containing the token"},{"location":"format/table/index/scalar/ngram/#accelerated-queries","title":"Accelerated Queries","text":"<p>The N-gram index provides inexact results for the following query types:</p> Query Type Description Operation Result Type contains Substring search in text Finds all trigrams in query, intersects posting lists AtMost"},{"location":"format/table/index/scalar/rtree/","title":"R-Tree Index","text":"<p>The R-Tree index is a static, immutable 2D spatial index. It is built on bounding boxes to organize the data. This index is intended to accelerate rectangle-based pruning.</p> <p>It is designed as a multi-level hierarchical structure: leaf pages store tuples <code>(bbox, id=rowid)</code> for indexed geometries; branch pages aggregate child bounding boxes and store <code>id=pageid</code> pointing to child pages; a single root page encloses the entire tree. Conceptually, it can be thought of as an extension of the B+-tree to multidimensional objects, where bounding boxes act as keys for spatial pruning.</p> <p>The index uses a packed-build strategy where items are first sorted and then grouped into fixed-size leaf pages.</p> <p>This packed-build flow is: - Sort items (bboxes) according to the sorting algorithm. - Pack consecutive items into leaf pages of <code>page_size</code> entries; then build parent pages bottom-up by aggregating child page bboxes.</p>"},{"location":"format/table/index/scalar/rtree/#sorting","title":"Sorting","text":"<p>Sorting does not change the R-Tree data structure, but it is critical to performance. Currently, Hilbert sorting is implemented, but the design is extensible to other spatial sorting algorithms.</p>"},{"location":"format/table/index/scalar/rtree/#hilbert-curve-sorting","title":"Hilbert Curve Sorting","text":"<p>Hilbert sorting imposes a linear order on 2D items using a space-filling Hilbert curve to maximize locality in both axes. This improves leaf clustering, which benefits query pruning.</p> <p>Hilbert sorting is performed in three steps:</p> <ol> <li>Global bounding box: compute the global bbox <code>[xmin_g, ymin_g, xmax_g, ymax_g]</code> over all items for training index.</li> <li>Normalize and compute Hilbert value:<ul> <li>For each item bbox <code>[xmin_i, ymin_i, xmax_i, ymax_i]</code>, compute its center:<ul> <li><code>cx = (xmin_i + xmax_i) / 2</code></li> <li><code>cy = (ymin_i + ymax_i) / 2</code></li> </ul> </li> <li>Map the center to a 16\u2011bit grid per axis using the global bbox. Let <code>W = xmax_g - xmin-g</code> and <code>H = ymax_g - ymin_g</code>. The normalized integer coordinates are:<ul> <li><code>xi = round(((cx - xmin_g) / W) * (2^16 - 1))</code></li> <li><code>yi = round(((cy - ymin_g) / H) * (2^16 - 1))</code></li> </ul> </li> <li>If the global width or height is effectively zero, the corresponding axis is treated as degenerate and set to <code>0</code> for all items (the ordering then degenerates to 1D on the other axis).</li> <li>For each <code>(xi, yi)</code> in <code>[0 .. 2^16-1] \u00d7 [0 .. 2^16-1]</code>, compute a 32\u2011bit Hilbert value using a standard 2D Hilbert algorithm. In pseudocode (with <code>bits = 16</code>):   <pre><code>fn hilbert_value(x, y, bits):\n    # x, y: integers in [0 .. 2^bits - 1]\n    h = 0\n    mask = (1 &lt;&lt; bits) - 1\n\n    for s from bits-1 down to 0:\n        rx = (x &gt;&gt; s) &amp; 1\n        ry = (y &gt;&gt; s) &amp; 1\n        d  = ((3 * rx) XOR ry) &lt;&lt; (2 * s)\n        h  = h | d\n\n        if ry == 0:\n            if rx == 1:\n                x = (~x) &amp; mask\n                y = (~y) &amp; mask\n            swap(x, y)\n\n    return h\n</code></pre></li> <li>The resulting <code>h</code> is stored as the item\u2019s Hilbert value (type <code>u32</code> with <code>bits = 16</code>).</li> </ul> </li> <li>Sort: sort items by Hilbert value.</li> </ol>"},{"location":"format/table/index/scalar/rtree/#index-details","title":"Index Details","text":"<pre><code>message RTreeIndexDetails {\n\n}\n</code></pre>"},{"location":"format/table/index/scalar/rtree/#storage-layout","title":"Storage Layout","text":"<p>The R-Tree index consists of two files:</p> <ol> <li><code>page_data.lance</code> - Stores all pages (leaf, branch) as repeated <code>(bbox, id)</code> tuples, written bottom-up (leaves first, then branch levels)</li> <li><code>nulls.lance</code> - Stores a serialized RowAddrTreeMap of rows with null</li> </ol>"},{"location":"format/table/index/scalar/rtree/#page-file-schema","title":"Page File Schema","text":"Column Type Nullable Description <code>bbox</code> RectType false Type is Rect defined by geoarrow-rs RectType; physical storage is Struct. Represents the node bounding box (leaf: item bbox; branch: child aggregation). <code>id</code> UInt64 false Reuse the <code>id</code> column to store <code>rowid</code> in leaf pages and <code>pageid</code> in branch pages"},{"location":"format/table/index/scalar/rtree/#nulls-file-schema","title":"Nulls File Schema","text":"Column Type Nullable Description <code>nulls</code> Binary false Serialized RowAddrTreeMap of rows with null/invalid geometry"},{"location":"format/table/index/scalar/rtree/#schema-metadata","title":"Schema Metadata","text":"<p>The following optional keys can be used by implementations and are stored in the schema metadata:</p> Key Type Description <code>page_size</code> String Page size per page <code>num_pages</code> String Total number of pages written <code>num_items</code> String Number of non-null leaf items in the index <code>bbox</code> String JSON-serialized global BoundingBox of the dataset"},{"location":"format/table/index/scalar/rtree/#query-traversal","title":"Query Traversal","text":"<p>This index serializes the multi-level hierarchical RTree structure into a single page file following the schema above. At lookup time, the reader computes each page offset using the algorithm below and reconstructs the hierarchy for traversal.</p> <p>Offsets are derived from <code>num_items</code> and <code>page_size</code> of metadata as follows:</p> <ul> <li>Leaf: <code>leaf_pages = ceil(num_items / page_size)</code>; leaf <code>i</code> has <code>page_offset = i * page_size</code>.</li> <li>Branch: let <code>level_offset</code> be the starting offset for current level, which actually represents total items from all lower levels; let <code>prev_pages</code> be pages in the level below; <code>level_pages = ceil(prev_pages / page_size)</code>. For branch <code>j</code>, <code>page_offset = j * page_size + level_offset</code>.</li> <li>Iterate levels until one page remains; the root is the last page and has <code>pageid = num_pages - 1</code>.</li> <li>Page lengths: once all page offsets are collected, compute each <code>page_len</code> by the next offset difference; for the final page (root), <code>page_len = page_file_total_rows - page_offset</code> (where <code>page_file_total_rows</code> is total rows in <code>page_data.lance</code>).</li> </ul> <p>Traversal starts from the root (<code>pageid = num_pages - 1</code>):</p> <ul> <li>If <code>page_offset &lt; num_items</code> (leaf), read items <code>[page_offset .. page_offset + page_len)</code> and emit candidate <code>rowid</code>s matching the query bbox.</li> <li>Otherwise (branch), descend into children whose bounding boxes match the query bbox.</li> <li>Continue until there are no more pages to visit; the union of emitted <code>rowid</code>s forms the candidate set for evaluation.</li> </ul>"},{"location":"format/table/index/scalar/rtree/#accelerated-queries","title":"Accelerated Queries","text":"<p>The R-Tree index accelerates the following query types by returning a candidate set of matching bounding boxes. Exact geometry verification must be performed by the execution engine.</p> Query Type Description Operation Result Type Intersects <code>St_Intersects(col, geom)</code> Prunes candidates by bbox intersection AtMost Contains <code>St_Contains(col, geom)</code> Prunes candidates by bbox containment AtMost Within <code>St_Within(col, geom)</code> Prunes candidates by bbox within relation AtMost Touches <code>St_Touches(col, geom)</code> Prunes candidates by bbox touch relation AtMost Crosses <code>St_Crosses(col, geom)</code> Prunes candidates by bbox crossing relation AtMost Overlaps <code>St_Overlaps(col, geom)</code> Prunes candidates by bbox overlap relation AtMost Covers <code>St_Covers(col, geom)</code> Prunes candidates by bbox cover relation AtMost CoveredBy <code>St_Coveredby(col, geom)</code> Prunes candidates by bbox covered-by relation AtMost IsNull <code>col IS NULL</code> Returns rows recorded in the nulls file Exact"},{"location":"format/table/index/scalar/zonemap/","title":"Zone Map Index","text":"<p>Zone maps are a columnar database technique for predicate pushdown and scan pruning. They break data into fixed-size chunks called \"zones\" and maintain summary statistics (min, max, null count) for each zone, enabling efficient filtering by eliminating zones that cannot contain matching values.</p> <p>Zone maps are \"inexact\" filters - they can definitively exclude zones but may include false positives that require rechecking.</p>"},{"location":"format/table/index/scalar/zonemap/#index-details","title":"Index Details","text":"<pre><code>message ZoneMapIndexDetails {\n\n}\n</code></pre>"},{"location":"format/table/index/scalar/zonemap/#storage-layout","title":"Storage Layout","text":"<p>The zone map index stores zone statistics in a single file:</p> <ol> <li><code>zonemap.lance</code> - Zone statistics for query pruning</li> </ol>"},{"location":"format/table/index/scalar/zonemap/#zone-statistics-file-schema","title":"Zone Statistics File Schema","text":"Column Type Nullable Description <code>min</code> {DataType} true Minimum value in the zone <code>max</code> {DataType} true Maximum value in the zone <code>null_count</code> UInt32 false Number of null values in the zone <code>nan_count</code> UInt32 false Number of NaN values (for float types) <code>fragment_id</code> UInt64 false Fragment containing this zone <code>zone_start</code> UInt64 false Starting row offset within the fragment <code>zone_length</code> UInt32 false Number of rows in this zone"},{"location":"format/table/index/scalar/zonemap/#schema-metadata","title":"Schema Metadata","text":"Key Type Description <code>rows_per_zone</code> String Number of rows per zone (default: \"8192\")"},{"location":"format/table/index/scalar/zonemap/#accelerated-queries","title":"Accelerated Queries","text":"<p>The zone map index provides inexact results for the following query types:</p> Query Type Description Operation Result Type Equals <code>column = value</code> Includes zones where min \u2264 value \u2264 max AtMost Range <code>column BETWEEN a AND b</code> Includes zones where ranges overlap AtMost IsIn <code>column IN (v1, v2, ...)</code> Includes zones that could contain any value AtMost IsNull <code>column IS NULL</code> Includes zones where null_count &gt; 0 AtMost"},{"location":"format/table/index/system/frag_reuse/","title":"Fragment Reuse Index","text":"<p>The Fragment Reuse Index is an internal index used to optimize fragment operations  during compaction and dataset updates.</p> <p>When data modifications happen against a Lance table, it could trigger compaction and index optimization at the same time to improve data layout and index coverage. By default, compaction will remap all indices at the same time to prevent read regression. This means both compaction and index optimization could modify the same index and cause one process to fail. Typically, the compaction would fail because it has to modify all indices and takes longer, resulting in table layout degrading over time.</p> <p>Fragment Reuse Index allows a compaction to defer the index remap process. Suppose a compaction removes fragments A and B and produces C. At query runtime, it reuses the old fragments A and B by  updating the row addresses related to A and B in the index to the latest ones in C. Because indices are typically cached in memory after initial load, the in-memory index is up to date after the fragment reuse application process.</p>"},{"location":"format/table/index/system/frag_reuse/#index-details","title":"Index Details","text":"<pre><code>message FragmentReuseIndexDetails {\n\n  oneof content {\n    // if &lt; 200KB, store the content inline, otherwise store the InlineContent bytes in external file\n    InlineContent inline = 1;\n    ExternalFile external = 2;\n  }\n\n  message InlineContent {\n    repeated Version versions = 1;\n  }\n\n  message FragmentDigest {\n    uint64 id = 1;\n\n    uint64 physical_rows = 2;\n\n    uint64 num_deleted_rows = 3;\n  }\n\n  // A summarized version of the RewriteGroup information in a Rewrite transaction\n  message Group {\n    // A roaring treemap of the changed row addresses.\n    // When combined with the old fragment IDs and new fragment IDs,\n    // it can recover the full mapping of old row addresses to either new row addresses or deleted.\n    // this mapping can then be used to remap indexes or satisfy index queries for the new unindexed fragments.\n    bytes changed_row_addrs = 1;\n\n    repeated FragmentDigest old_fragments = 2;\n\n    repeated FragmentDigest new_fragments = 3;\n  }\n\n  message Version {\n    // The dataset_version at the time the index adds this version entry\n    uint64 dataset_version = 1;\n\n    repeated Group groups = 3;\n  }\n\n}\n</code></pre>"},{"location":"format/table/index/system/frag_reuse/#expected-use-pattern","title":"Expected Use Pattern","text":"<p>Fragment Reuse Index should be created if the user defers index remap in compaction. The index accumulates a new reuse version every time a compaction is executed.</p> <p>As long as all the scalar and vector indices are created after the specific reuse version, the indices are all caught up and the specific reuse version can be trimmed.</p> <p>It is expected that the user schedules an additional process to trim the index periodically to keep the list of reuse versions in control.</p>"},{"location":"format/table/index/system/mem_wal/","title":"MemWAL Index","text":"<p>The MemWAL Index is a system index that serves as the centralized structure for all MemWAL metadata. It stores configuration (region specs, indexes to maintain), merge progress, and region state snapshots.</p> <p>A table has at most one MemWAL index.</p> <p>For the complete specification, see:</p> <ul> <li>MemWAL Index Overview - Purpose and high-level description</li> <li>MemWAL Index Details - Storage format, schemas, and staleness handling</li> <li>MemWAL Index Builder - Background process and configuration updates</li> </ul>"},{"location":"format/table/index/vector/","title":"Vector Indices","text":"<p>Lance provides a powerful and extensible secondary index system for efficient vector similarity search.  All vector indices are stored as regular Lance files, making them portable and easy to manage. It is designed for efficient similarity search across large-scale vector datasets.</p>"},{"location":"format/table/index/vector/#concepts","title":"Concepts","text":"<p>Lance splits each vector index into 3 parts - clustering, sub-index and quantization.</p>"},{"location":"format/table/index/vector/#clustering","title":"Clustering","text":"<p>Clustering divides all the vectors into different disjoint clusters (a.k.a. partitions). Lance currently supports using Inverted File (IVF) as the primary clustering mechanism. IVF partitions the vectors into clusters using the k-means clustering algorithm.  Each cluster contains vectors that are similar to the cluster centroid. During search, only the most relevant clusters are examined, dramatically reducing search time. IVF can be combined with any sub-index type and quantization method.</p>"},{"location":"format/table/index/vector/#sub-index","title":"Sub-Index","text":"<p>The sub-index determines how vectors are organized for search. Lance currently supports:</p> <ul> <li>FLAT: Exact search with no approximation - scans all vectors</li> <li>HNSW: Hierarchical Navigable Small World graphs for fast approximate search</li> </ul>"},{"location":"format/table/index/vector/#quantization","title":"Quantization","text":"<p>The quantization method determines how vectors are stored and compressed. Lance currently supports:</p> <ul> <li>Product Quantization (PQ): Compresses vectors by splitting them into smaller sub-vectors and quantizing each independently</li> <li>Scalar Quantization (SQ): Applies scalar quantization to each dimension of the vector independently</li> <li>RabitQ (RQ): Uses random rotation and binary quantization for extreme compression</li> <li>FLAT: No quantization, keeps original vectors for exact search</li> </ul>"},{"location":"format/table/index/vector/#common-combinations","title":"Common Combinations","text":"<p>When we refer to an index type, it is typically <code>{clustering}_{sub_index}_{quantization}</code>. If sub-index is just <code>FLAT</code>, we usually omit it and just refer to it by <code>{clustering}_{quantization}</code>. Here are the commonly used combinations:</p> Index Type Name Description IVF_PQ Inverted File with Product Quantization Combines IVF clustering with PQ compression for efficient storage and search IVF_HNSW_SQ Inverted File with HNSW and Scalar Quantization Uses IVF for coarse clustering and HNSW for fine-grained search with scalar quantization IVF_SQ Inverted File with Scalar Quantization Combines IVF clustering with scalar quantization for balanced compression IVF_RQ Inverted File with RabitQ Combines IVF clustering with RabitQ for extreme compression using binary quantization IVF_FLAT Inverted File without quantization Uses IVF clustering with exact vector storage for precise search within clusters"},{"location":"format/table/index/vector/#versioning","title":"Versioning","text":"<p>The Lance vector index format has gone through 3 versions so far. This document currently only records version 3 which is the latest version. The specific version of the vector index is recorded in the <code>index_version</code> field of the generic index metadata.</p>"},{"location":"format/table/index/vector/#storage-layout-v3","title":"Storage Layout (V3)","text":"<p>Each vector index is stored as 2 regular Lance files - index file and auxiliary file.</p>"},{"location":"format/table/index/vector/#index-file","title":"Index File","text":"<p>The index structure file containing the search graph/structure with index-specific schema. It is stored as a Lance file with name <code>index.idx</code> within the index directory.</p>"},{"location":"format/table/index/vector/#arrow-schema","title":"Arrow Schema","text":"<p>The index file stores the search structure with graph or flat organization. The Arrow schema of the Lance file varies depending on the sub-index type used.</p> <p>Note</p> <p>All partitions are stored in the same file, and partitions must be written in order.</p>"},{"location":"format/table/index/vector/#flat","title":"FLAT","text":"<p>FLAT indices perform exact search with no approximation. This is essentially an empty file with a minimal schema:</p> Column Type Nullable Description <code>__flat_marker</code> uint64 false Marker field for FLAT index (no actual data)"},{"location":"format/table/index/vector/#hnsw","title":"HNSW","text":"<p>HNSW (Hierarchical Navigable Small World) indices provide fast approximate search through a multi-level graph structure. This stores the HNSW graph with the following schema:</p> Column Type Nullable Description <code>__vector_id</code> uint64 false Vector identifier <code>__neighbors</code> list false Neighbor node IDs <code>_distance</code> list false Distances to neighbors <p>Note</p> <p>HNSW consists of multiple levels, and all levels must be written in order starting from level 0.</p>"},{"location":"format/table/index/vector/#arrow-schema-metadata","title":"Arrow Schema Metadata","text":"<p>The index file contains metadata in its Arrow schema metadata to describe the index configuration and structure. Here are the metadata keys and their corresponding values:</p>"},{"location":"format/table/index/vector/#lanceindex","title":"\"lance:index\"","text":"<p>Contains basic index configuration information in JSON:</p> JSON Key Type Expected Values <code>type</code> String Index type (e.g., \"IVF_PQ\", \"IVF_RQ\", \"IVF_HNSW\", \"FLAT\") <code>distance_type</code> String Distance metric (e.g., \"l2\", \"cosine\", \"dot\")"},{"location":"format/table/index/vector/#lanceivf","title":"\"lance:ivf\"","text":"<p>References the IVF metadata stored in the Lance file global buffer. This value records the global buffer index, currently this is always \"1\".</p> <p>Note</p> <p>Global buffer indices in Lance files are 1-based,  so you need to subtract 1 when accessing them through code.</p>"},{"location":"format/table/index/vector/#lanceflat","title":"\"lance:flat\"","text":"<p>Contains partition-specific metadata for the <code>FLAT</code> sub-index structure. This is an empty string since FLAT indices don't require additional metadata at this moment.</p>"},{"location":"format/table/index/vector/#lancehnsw","title":"\"lance:hnsw\"","text":"<p>Contains the HNSW-specific JSON metadata for each partition, including graph structure information:</p> JSON Key Type Expected Values <code>entry_point</code> u32 Starting node for graph traversal <code>params</code> Object HNSW construction parameters (see below) <code>level_offsets</code> Array Offset for each level in the graph <p>The <code>params</code> object contains the following HNSW construction parameters:</p> JSON Key Type Description Default <code>max_level</code> u16 Maximum level of the HNSW graph 7 <code>m</code> usize Number of connections to establish while inserting new element 20 <code>ef_construction</code> usize Size of the dynamic list for candidates 150 <code>prefetch_distance</code> Option Number of vectors ahead to prefetch while building Some(2)"},{"location":"format/table/index/vector/#lance-file-global-buffer","title":"Lance File Global Buffer","text":""},{"location":"format/table/index/vector/#ivf-metadata","title":"IVF Metadata","text":"<p>For efficiency, Lance serializes IVF metadata to protobuf format and stores it in the Lance file global buffer:</p> <pre><code>message IVF {\n  // Centroids of partitions. `dimension * num_partitions` of float32s.\n  //\n  // Deprecated, use centroids_tensor instead.\n  repeated float centroids = 1;  // [deprecated = true];\n\n  // File offset of each partition.\n  repeated uint64 offsets = 2;\n\n  // Number of records in the partition.\n  repeated uint32 lengths = 3;\n\n  // Tensor of centroids. `num_partitions * dimension` of float32s.\n  Tensor centroids_tensor = 4;\n\n  // KMeans loss.\n  optional double loss = 5;\n\n}\n</code></pre>"},{"location":"format/table/index/vector/#auxiliary-file","title":"Auxiliary File","text":"<p>The auxiliary file is a vector storage for quantized vectors. It is stored as a Lance file named <code>auxiliary.idx</code> within the index directory.</p>"},{"location":"format/table/index/vector/#arrow-schema_1","title":"Arrow Schema","text":"<p>Since the auxiliary file stores the actual (quantized) vectors, the Arrow schema of the Lance file varies depending on the quantization method used.</p> <p>Note</p> <p>All partitions are stored in the same file, and partitions must be written in order.</p>"},{"location":"format/table/index/vector/#flat_1","title":"FLAT","text":"<p>No quantization applied - stores original vectors in their full precision:</p> Column Type Nullable Description <code>_rowid</code> uint64 false Row identifier <code>flat</code> list[dimension] false Original vector values (list_size = vector dimension)"},{"location":"format/table/index/vector/#pq","title":"PQ","text":"<p>Compresses vectors using product quantization for significant memory savings:</p> Column Type Nullable Description <code>_rowid</code> uint64 false Row identifier <code>__pq_code</code> list[m] false PQ codes (list_size = number of subvectors)"},{"location":"format/table/index/vector/#sq","title":"SQ","text":"<p>Compresses vectors using scalar quantization for moderate memory savings:</p> Column Type Nullable Description <code>_rowid</code> uint64 false Row identifier <code>__sq_code</code> list[dimension] false SQ codes (list_size = vector dimension)"},{"location":"format/table/index/vector/#rq","title":"RQ","text":"<p>Compresses vectors using RabitQ with random rotation and binary quantization for extreme compression:</p> Column Type Nullable Description <code>_rowid</code> uint64 false Row identifier <code>_rabit_codes</code> list[dimension / 8] false Binary quantized codes (1 bit per dimension, packed into bytes) <code>__add_factors</code> float32 false Additive correction factors for distance computation <code>__scale_factors</code> float32 false Scale correction factors for distance computation"},{"location":"format/table/index/vector/#arrow-schema-metadata_1","title":"Arrow Schema Metadata","text":"<p>The auxiliary file also contains metadata in its Arrow schema metadata for vector storage configuration. Here are the metadata keys and their corresponding values:</p>"},{"location":"format/table/index/vector/#distance_type","title":"\"distance_type\"","text":"<p>The distance metric used to compute similarity between vectors (e.g., \"l2\", \"cosine\", \"dot\").</p>"},{"location":"format/table/index/vector/#lanceivf_1","title":"\"lance:ivf\"","text":"<p>Similar to the index file's \"lance:ivf\" but focused on vector storage layout.  This doesn't contain the partitions' centroids. It's only used for tracking each partition's offset and length in the auxiliary file.</p>"},{"location":"format/table/index/vector/#lancerabit","title":"\"lance:rabit\"","text":"<p>Contains RabitQ-specific metadata in JSON format (only present for RQ quantization). This includes the rotation matrix position, number of bits, and packing information. See the RQ metadata specification in the \"storage_metadata\" section below.</p>"},{"location":"format/table/index/vector/#storage_metadata","title":"\"storage_metadata\"","text":"<p>Contains quantizer-specific metadata as a list of JSON strings. Currently, the list always contains exactly 1 element with the quantizer metadata.</p> <p>For Product Quantization (PQ):</p> JSON Key Type Description <code>codebook_position</code> usize Position of the codebook in the global buffer <code>nbits</code> u32 Number of bits per subvector code (e.g., 8 bits = 256 codewords) <code>num_sub_vectors</code> usize Number of subvectors (m) <code>dimension</code> usize Original vector dimension <code>transposed</code> bool Whether the codebook is stored in transposed layout <p>For Scalar Quantization (SQ):</p> JSON Key Type Description <code>dim</code> usize Vector dimension <code>num_bits</code> u16 Number of bits for quantization <code>bounds</code> Range Min/max bounds for scalar quantization <p>For RabitQ (RQ):</p> JSON Key Type Description <code>rotate_mat_position</code> u32 Position of the rotation matrix in the global buffer <code>num_bits</code> u8 Number of bits per dimension (currently always 1) <code>packed</code> bool Whether codes are packed for optimized computation"},{"location":"format/table/index/vector/#lance-file-global-buffer_1","title":"Lance File Global Buffer","text":""},{"location":"format/table/index/vector/#quantization-codebook","title":"Quantization Codebook","text":"<p>For product quantization, the codebook is stored in <code>Tensor</code> format  in the auxiliary file's global buffer for efficient access:</p> <pre><code>message Tensor {\n  enum DataType {\n    BFLOAT16 = 0;\n    FLOAT16 = 1;\n    FLOAT32 = 2;\n    FLOAT64 = 3;\n    UINT8 = 4;\n    UINT16 = 5;\n    UINT32 = 6;\n    UINT64 = 7;\n  }\n\n  DataType data_type = 1;\n\n  // Data shape, [dim1, dim2, ...]\n  repeated uint32 shape = 2;\n\n  // Data buffer\n  bytes data = 3;\n\n}\n</code></pre>"},{"location":"format/table/index/vector/#rotation-matrix","title":"Rotation Matrix","text":"<p>For RabitQ, the rotation matrix is stored in <code>Tensor</code> format in the auxiliary file's global buffer. The rotation matrix is an orthogonal matrix used  to rotate vectors before binary quantization:</p> <pre><code>message Tensor {\n  enum DataType {\n    BFLOAT16 = 0;\n    FLOAT16 = 1;\n    FLOAT32 = 2;\n    FLOAT64 = 3;\n    UINT8 = 4;\n    UINT16 = 5;\n    UINT32 = 6;\n    UINT64 = 7;\n  }\n\n  DataType data_type = 1;\n\n  // Data shape, [dim1, dim2, ...]\n  repeated uint32 shape = 2;\n\n  // Data buffer\n  bytes data = 3;\n\n}\n</code></pre> <p>The rotation matrix has shape <code>[code_dim, code_dim]</code> where <code>code_dim = dimension * num_bits</code>.</p>"},{"location":"format/table/index/vector/#appendices","title":"Appendices","text":""},{"location":"format/table/index/vector/#appendix-1-example-ivf_pq-format","title":"Appendix 1: Example IVF_PQ Format","text":"<p>This example shows how an <code>IVF_PQ</code> index is physically laid out. Assume vectors have dimension 128, PQ uses 16 num_sub_vectors (m=16) with 8 num_bits per subvector, and distance type is \"l2\".</p>"},{"location":"format/table/index/vector/#index-file_1","title":"Index File","text":"<ul> <li> <p>Arrow Schema Metadata:</p> <ul> <li><code>\"lance:index\"</code> \u2192 <code>{ \"type\": \"IVF_PQ\", \"distance_type\": \"l2\" }</code></li> <li><code>\"lance:ivf\"</code> \u2192 \"1\" (references IVF metadata in the global buffer)</li> <li><code>\"lance:flat\"</code> \u2192 <code>[\"\", \"\", ...]</code> (one empty string per partition; IVF_PQ uses a FLAT sub-index inside each partition)</li> </ul> </li> <li> <p>Lance File Global buffer (Protobuf):</p> <ul> <li><code>Ivf</code> message containing:<ul> <li><code>centroids_tensor</code>: shape <code>[num_partitions, 128]</code> (float32)</li> <li><code>offsets</code>: start offset (row) of each partition in <code>auxiliary.idx</code></li> <li><code>lengths</code>: number of vectors in each partition</li> <li><code>loss</code>: k-means loss (optional)</li> </ul> </li> </ul> </li> </ul>"},{"location":"format/table/index/vector/#auxiliary-file_1","title":"Auxiliary File","text":"<ul> <li>Arrow Schema Metadata:<ul> <li><code>\"distance_type\"</code> \u2192 <code>\"l2\"</code></li> <li><code>\"lance:ivf\"</code> \u2192 tracks per-partition <code>offsets</code> and <code>lengths</code> (no centroids here)</li> <li><code>\"storage_metadata\"</code> \u2192 <code>[ \"{\"pq\":{\"num_sub_vectors\":16,\"nbits\":8,\"dimension\":128,\"transposed\":true}}\" ]</code></li> </ul> </li> <li>Lance File Global buffer:<ul> <li><code>Tensor</code> codebook with shape <code>[256, num_sub_vectors, dim/num_sub_vectors]</code> = <code>[256, 16, 8]</code> (float32)</li> </ul> </li> <li>Rows with Arrow schema: </li> </ul> <pre><code>pa.schema([\n    pa.field(\"_rowid\", pa.uint64()),\n    pa.field(\"__pq_code\", pa.list(pa.uint8(), list_size=16)), # m subvector codes\n])\n</code></pre>"},{"location":"format/table/index/vector/#appendix-2-example-ivf_rq-format","title":"Appendix 2: Example IVF_RQ Format","text":"<p>This example shows how an <code>IVF_RQ</code> index is physically laid out. Assume vectors have dimension 128, RQ uses 1 bit per dimension (num_bits=1), and distance type is \"l2\".</p>"},{"location":"format/table/index/vector/#index-file_2","title":"Index File","text":"<ul> <li> <p>Arrow Schema Metadata:</p> <ul> <li><code>\"lance:index\"</code> \u2192 <code>{ \"type\": \"IVF_RQ\", \"distance_type\": \"l2\" }</code></li> <li><code>\"lance:ivf\"</code> \u2192 \"1\" (references IVF metadata in the global buffer)</li> <li><code>\"lance:flat\"</code> \u2192 <code>[\"\", \"\", ...]</code> (one empty string per partition; IVF_RQ uses a FLAT sub-index inside each partition)</li> </ul> </li> <li> <p>Lance File Global buffer (Protobuf):</p> <ul> <li><code>Ivf</code> message containing:<ul> <li><code>centroids_tensor</code>: shape <code>[num_partitions, 128]</code> (float32)</li> <li><code>offsets</code>: start offset (row) of each partition in <code>auxiliary.idx</code></li> <li><code>lengths</code>: number of vectors in each partition</li> <li><code>loss</code>: k-means loss (optional)</li> </ul> </li> </ul> </li> </ul>"},{"location":"format/table/index/vector/#auxiliary-file_2","title":"Auxiliary File","text":"<ul> <li>Arrow Schema Metadata:<ul> <li><code>\"distance_type\"</code> \u2192 <code>\"l2\"</code></li> <li><code>\"lance:ivf\"</code> \u2192 tracks per-partition <code>offsets</code> and <code>lengths</code> (no centroids here)</li> <li><code>\"lance:rabit\"</code> \u2192 <code>\"{\"rotate_mat_position\":1,\"num_bits\":1,\"packed\":true}\"</code></li> </ul> </li> <li>Lance File Global buffer:<ul> <li><code>Tensor</code> rotation matrix with shape <code>[code_dim, code_dim]</code> = <code>[128, 128]</code> (float32)</li> </ul> </li> <li>Rows with Arrow schema: </li> </ul> <pre><code>pa.schema([\n    pa.field(\"_rowid\", pa.uint64()),\n    pa.field(\"_rabit_codes\", pa.list(pa.uint8(), list_size=16)), # dimension/8 = 128/8 = 16 bytes\n    pa.field(\"__add_factors\", pa.float32()),\n    pa.field(\"__scale_factors\", pa.float32()),\n])\n</code></pre>"},{"location":"format/table/index/vector/#appendix-3-accessing-index-file-with-python","title":"Appendix 3: Accessing Index File with Python","text":"<p>The following example demonstrates how to read and parse different components in the Lance index files using Python:</p> <pre><code>import pyarrow as pa\nimport lance\n\n# Open the index file\nindex_reader = lance.LanceFileReader.read_file(\"path/to/index.idx\")\n\n# Access schema metadata\nschema_metadata = index_reader.metadata().schema.metadata\n\n# Get the IVF metadata reference from schema\nivf_ref = schema_metadata.get(b\"lance:ivf\")  # Returns b\"1\" for global buffer index\n\n# Read the global buffer containing IVF metadata\nif ivf_ref:\n    buffer_index = int(ivf_ref) - 1  # Global buffer indices are 1-based\n    ivf_buffer = index_reader.global_buffer(buffer_index)\n\n    # Parse the protobuf message (requires lance protobuf definitions)\n    # ivf_metadata = parse_ivf_protobuf(ivf_buffer)\n\n# For auxiliary file with PQ codebook\naux_reader = lance.LanceFileReader.read_file(\"path/to/auxiliary.idx\")\n\n# Get storage metadata\nstorage_metadata = aux_reader.metadata().schema.metadata.get(b\"storage_metadata\")\nif storage_metadata:\n    import json\n    pq_metadata = json.loads(storage_metadata.decode())[0]  # First element of the list\n    pq_params = json.loads(pq_metadata)\n\n    # Access the codebook from global buffer\n    codebook_position = pq_params.get(\"codebook_position\", 1)\n    if codebook_position &gt; 0:\n        codebook_buffer = aux_reader.global_buffer(codebook_position - 1)\n        # Parse the tensor protobuf\n        # codebook_tensor = parse_tensor_protobuf(codebook_buffer)\n</code></pre>"},{"location":"guide/arrays/","title":"Extension Arrays","text":"<p>Lance provides extensions for Arrow arrays and Pandas Series to represent data types for machine learning applications.</p>"},{"location":"guide/arrays/#bfloat16","title":"BFloat16","text":"<p>BFloat16 is a 16-bit floating point number that is designed for machine learning use cases. Intuitively, it only has 2-3 digits of precision, but it has the same range as a 32-bit float: ~1e-38 to ~1e38. By comparison, a 16-bit float has a range of ~5.96e-8 to 65504.</p> <p>Lance provides an Arrow extension array (<code>lance.arrow.BFloat16Array</code>) and a Pandas extension array (<code>lance._arrow.PandasBFloat16Type</code>) for BFloat16. These are compatible with the ml_dtypes bfloat16 NumPy extension array.</p> <p>If you are using Pandas, you can use the <code>lance.bfloat16</code> dtype string to create the array:</p> <pre><code>import lance.arrow\n\npd.Series([1.1, 2.1, 3.4], dtype=\"lance.bfloat16\")\n# 0    1.1015625\n# 1      2.09375\n# 2      3.40625\n# dtype: lance.bfloat16\n</code></pre> <p>To create an Arrow array, use the <code>lance.arrow.bfloat16_array</code> function:</p> <pre><code>from lance.arrow import bfloat16_array\n\nbfloat16_array([1.1, 2.1, 3.4])\n# &lt;lance.arrow.BFloat16Array object at 0x000000016feb94e0&gt;\n# [\n#   1.1015625,\n#   2.09375,\n#   3.40625\n# ]\n</code></pre> <p>Finally, if you have a pre-existing NumPy array, you can convert it into either:</p> <pre><code>import numpy as np\nfrom ml_dtypes import bfloat16\nfrom lance.arrow import PandasBFloat16Array, BFloat16Array\n\nnp_array = np.array([1.1, 2.1, 3.4], dtype=bfloat16)\nPandasBFloat16Array.from_numpy(np_array)\n# &lt;PandasBFloat16Array&gt;\n# [1.1015625, 2.09375, 3.40625]\n# Length: 3, dtype: lance.bfloat16\nBFloat16Array.from_numpy(np_array)\n# &lt;lance.arrow.BFloat16Array object at 0x...&gt;\n# [\n#   1.1015625,\n#   2.09375,\n#   3.40625\n# ]\n</code></pre> <p>When reading, these can be converted back to to the NumPy bfloat16 dtype using each array class's <code>to_numpy</code> method.</p>"},{"location":"guide/arrays/#imageuri","title":"ImageURI","text":"<p><code>lance.arrow.ImageURIArray</code> is an array that stores the URI location of images in some other storage system. For example, <code>file:///path/to/image.png</code> for a local filesystem or <code>s3://bucket/path/image.jpeg</code> for an image on AWS S3. Use this array type when you want to lazily load images from an existing storage medium.</p> <p>It can be created by calling <code>lance.arrow.ImageURIArray.from_uris</code> with a list of URIs represented by either <code>pyarrow.StringArray</code> or an iterable that yields strings. Note that the URIs are not strongly validated and images are not read into memory automatically.</p> <pre><code>from lance.arrow import ImageURIArray\n\nImageURIArray.from_uris([\n   \"/tmp/image1.jpg\",\n   \"file:///tmp/image2.jpg\",\n   \"s3://example/image3.jpg\"\n])\n# &lt;lance.arrow.ImageURIArray object at 0x...&gt;\n# ['/tmp/image1.jpg', 'file:///tmp/image2.jpg', 's3://example/image3.jpg']\n</code></pre> <p><code>lance.arrow.ImageURIArray.read_uris</code> will read images into memory and return them as a new <code>lance.arrow.EncodedImageArray</code> object.</p> <pre><code>from lance.arrow import ImageURIArray\n\nrelative_path = \"images/1.png\"\nuris = [os.path.join(os.path.dirname(__file__), relative_path)]\nImageURIArray.from_uris(uris).read_uris()\n# &lt;lance.arrow.EncodedImageArray object at 0x...&gt;\n# [b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00...']\n</code></pre>"},{"location":"guide/arrays/#encodedimage","title":"EncodedImage","text":"<p><code>lance.arrow.EncodedImageArray</code> is an array that stores jpeg and png images in their encoded and compressed representation as they would appear written on disk. Use this array when you want to manipulate images in their compressed format such as when you're reading them from disk or embedding them into HTML.</p> <p>It can be created by calling <code>lance.arrow.ImageURIArray.read_uris</code> on an existing <code>lance.arrow.ImageURIArray</code>. This will read the referenced images into memory. It can also be created by calling <code>lance.arrow.ImageArray.from_array</code> and passing it an array of encoded images already read into <code>pyarrow.BinaryArray</code> or by calling <code>lance.arrow.ImageTensorArray.to_encoded</code>.</p> <p>A <code>lance.arrow.EncodedImageArray.to_tensor</code> method is provided to decode encoded images and return them as <code>lance.arrow.FixedShapeImageTensorArray</code>, from which they can be converted to numpy arrays or TensorFlow tensors. For decoding images, it will first attempt to use a decoder provided via the optional function parameter. If decoder is not provided it will attempt to use Pillow and tensorflow in that order. If neither library or custom decoder is available an exception will be raised.</p> <pre><code>from lance.arrow import ImageURIArray\n\nuris = [os.path.join(os.path.dirname(__file__), \"images/1.png\")]\nencoded_images = ImageURIArray.from_uris(uris).read_uris()\nprint(encoded_images.to_tensor())\n\ndef tensorflow_decoder(images):\n    import tensorflow as tf\n    import numpy as np\n\n    return np.stack(tf.io.decode_png(img.as_py(), channels=3) for img in images.storage)\n\nprint(encoded_images.to_tensor(tensorflow_decoder))\n# &lt;lance.arrow.FixedShapeImageTensorArray object at 0x...&gt;\n# [[42, 42, 42, 255]]\n# &lt;lance.arrow.FixedShapeImageTensorArray object at 0x...&gt;\n# [[42, 42, 42, 255]]\n</code></pre>"},{"location":"guide/arrays/#fixedshapeimagetensor","title":"FixedShapeImageTensor","text":"<p><code>lance.arrow.FixedShapeImageTensorArray</code> is an array that stores images as tensors where each individual pixel is represented as a numeric value. Typically images are stored as 3 dimensional tensors shaped (height, width, channels). In color images each pixel is represented by three values (channels) as per RGB color model. Images from this array can be read out as numpy arrays individually or stacked together into a single 4 dimensional numpy array shaped (batch_size, height, width, channels).</p> <p>It can be created by calling <code>lance.arrow.EncodedImageArray.to_tensor</code> on a previously existing <code>lance.arrow.EncodedImageArray</code>. This will decode encoded images and return them as a <code>lance.arrow.FixedShapeImageTensorArray</code>. It can also be created by calling <code>lance.arrow.ImageArray.from_array</code> and passing in a <code>pyarrow.FixedShapeTensorArray</code>.</p> <p>It can be encoded into to <code>lance.arrow.EncodedImageArray</code> by calling <code>lance.arrow.FixedShapeImageTensorArray.to_encoded</code> and passing custom encoder If encoder is not provided it will attempt to use tensorflow and Pillow in that order. Default encoders will encode to PNG. If neither library is available it will raise an exception.</p> <pre><code>from lance.arrow import ImageURIArray\n\nuris = [image_uri]\ntensor_images = ImageURIArray.from_uris(uris).read_uris().to_tensor()\ntensor_images.to_encoded()\n# &lt;lance.arrow.EncodedImageArray object at 0x...&gt;\n# [...\n# b'\\x89PNG\\r\\n\\x1a...'\n</code></pre>"},{"location":"guide/blob/","title":"Blob As Files","text":"<p>Unlike other data formats, large multimodal data is a first-class citizen in the Lance columnar format. Lance provides a high-level API to store and retrieve large binary objects (blobs) in Lance datasets.</p> <p></p> <p>Lance serves large binary data using <code>lance.BlobFile</code>, which is a file-like object that lazily reads large binary objects.</p> <p>To create a Lance dataset with large blob data, you can mark a large binary column as a blob column by adding the metadata <code>lance-encoding:blob</code> to <code>true</code>.</p> <pre><code>import pyarrow as pa\n\nschema = pa.schema(\n    [\n        pa.field(\"id\", pa.int64()),\n        pa.field(\"video\",\n            pa.large_binary(),\n            metadata={\"lance-encoding:blob\": \"true\"}\n        ),\n    ]\n)\n</code></pre> <p>To write blob data to a Lance dataset, create a PyArrow table with the blob schema and use <code>lance.write_dataset</code>:</p> <pre><code>import lance\n\n# First, download a sample video file for testing\n# wget https://www.learningcontainer.com/wp-content/uploads/2020/05/sample-mp4-file.mp4\nimport urllib.request\nurllib.request.urlretrieve(\n    \"https://www.learningcontainer.com/wp-content/uploads/2020/05/sample-mp4-file.mp4\",\n    \"sample_video.mp4\"\n)\n\n# Then read the video file content\nwith open(\"sample_video.mp4\", 'rb') as f:\n    video_data = f.read()\n\n# Create table with blob data\ntable = pa.table({\n    \"id\": [1],\n    \"video\": [video_data],\n}, schema=schema)\n\n# Write to Lance dataset\nds = lance.write_dataset(\n    table,\n    \"./youtube.lance\",\n    schema=schema\n)\n</code></pre> <p>To fetch blobs from a Lance dataset, you can use <code>lance.dataset.LanceDataset.take_blobs</code>.</p> <p>For example, it's easy to use <code>BlobFile</code> to extract frames from a video file without loading the entire video into memory.</p> <pre><code>import av # pip install av\nimport lance\n\nds = lance.dataset(\"./youtube.lance\")\nstart_time, end_time = 500, 1000\n# Get blob data from the first row (id=0)\nblobs = ds.take_blobs(\"video\", ids=[0])\nwith av.open(blobs[0]) as container:\n    stream = container.streams.video[0]\n    stream.codec_context.skip_frame = \"NONKEY\"\n\n    start_time = start_time / stream.time_base\n    start_time = start_time.as_integer_ratio()[0]\n    end_time = end_time / stream.time_base\n    container.seek(start_time, stream=stream)\n\n    for frame in container.decode(stream):\n        if frame.time &gt; end_time:\n            break\n        display(frame.to_image())\n        clear_output(wait=True) \n</code></pre>"},{"location":"guide/data_evolution/","title":"Data Evolution","text":"<p>Lance supports traditional schema evolution: adding, removing, and altering columns in a dataset. Most of these operations can be performed without rewriting the data files in the dataset, making them very efficient operations. In addition, Lance supports data evolution, which allows you to also backfill existing rows with the new column data without rewriting the data files in the dataset, making it highly suitable for use cases like ML feature engineering.</p> <p>In general, schema changes will conflict with most other concurrent write operations. For example, if you change the schema of the dataset while someone else is appending data to it, either your schema change or the append will fail, depending on the order of the operations. Thus, it's recommended to perform schema changes when no other writes are happening.</p>"},{"location":"guide/data_evolution/#adding-new-columns","title":"Adding new columns","text":""},{"location":"guide/data_evolution/#schema-only","title":"Schema only","text":"<p>A common use case we've seen in production is to add a new column to a dataset without populating it. This is useful to later run a large distributed job to populate the column lazily. To do this, you can use the <code>lance.LanceDataset.add_columns</code> method to add columns with <code>pyarrow.Field</code> or <code>pyarrow.Schema</code>.</p> <pre><code>table = pa.table({\"id\": pa.array([1, 2, 3])})\ndataset = lance.write_dataset(table, \"null_columns\")\n\n# With pyarrow Field\ndataset.add_columns(pa.field(\"embedding\", pa.list_(pa.float32(), 128)))\nassert dataset.schema == pa.schema([\n    (\"id\", pa.int64()),\n    (\"embedding\", pa.list_(pa.float32(), 128)),\n])\n\n# With pyarrow Schema\ndataset.add_columns(pa.schema([\n    (\"label\", pa.string()),\n    (\"score\", pa.float32()),\n]))\nassert dataset.schema == pa.schema([\n    (\"id\", pa.int64()),\n    (\"embedding\", pa.list_(pa.float32(), 128)),\n    (\"label\", pa.string()),\n    (\"score\", pa.float32()),\n])\n</code></pre> <p>This operation is very fast, as it only updates the metadata of the dataset.</p>"},{"location":"guide/data_evolution/#with-data-backfill","title":"With data backfill","text":"<p>New columns can be added and populated within a single operation using the <code>lance.LanceDataset.add_columns</code> method. There are two ways to specify how to populate the new columns: first, by providing a SQL expression for each new column, or second, by providing a function to generate the new column data.</p> <p>SQL expressions can either be independent expressions or reference existing columns. SQL literal values can be used to set a single value for all existing rows.</p> <pre><code>table = pa.table({\"name\": pa.array([\"Alice\", \"Bob\", \"Carla\"])})\ndataset = lance.write_dataset(table, \"names\")\ndataset.add_columns({\n    \"hash\": \"sha256(name)\",\n    \"status\": \"'active'\",\n})\nprint(dataset.to_table().to_pandas())\n#     name                                               hash  status\n# 0  Alice  b';\\xc5\\x10b\\x97&lt;E\\x8dZo-\\x8dd\\xa0#$cT\\xad~\\x0...  active\n# 1    Bob  b'\\xcd\\x9f\\xb1\\xe1H\\xcc\\xd8D.Z\\xa7I\\x04\\xccs\\x...  active\n# 2  Carla  b'\\xad\\x8d\\x83\\xff\\xd8+Z\\x8e\\xd4)\\xe8Y+\\\\\\xb3\\...  active\n</code></pre> <p>You can also provide a Python function to generate the new column data. This can be used, for example, to compute a new embedding column. This function should take a PyArrow RecordBatch and return either a PyArrow RecordBatch or a Pandas DataFrame. The function will be called once for each batch in the dataset.</p> <p>If the function is expensive to compute and can fail, it is recommended to set a checkpoint file in the UDF. This checkpoint file saves the state of the UDF after each invocation, so that if the UDF fails, it can be restarted from the last checkpoint. Note that this file can get quite large, since it needs to store unsaved results for up to an entire data file.</p> <pre><code>import lance\nimport pyarrow as pa\nimport numpy as np\n\ntable = pa.table({\"id\": pa.array([1, 2, 3])})\ndataset = lance.write_dataset(table, \"ids\")\n\n@lance.batch_udf(checkpoint_file=\"embedding_checkpoint.sqlite\")\ndef add_random_vector(batch):\n    embeddings = np.random.rand(batch.num_rows, 128).astype(\"float32\")\n    return pa.RecordBatch.from_arrays(\n        [pa.FixedSizeListArray.from_arrays(embeddings.flatten(), 128)],\n        names=[\"embedding\"]\n    )\ndataset.add_columns(add_random_vector)\n</code></pre>"},{"location":"guide/data_evolution/#using-merge","title":"Using merge","text":"<p>If you have pre-computed one or more new columns, you can add them to an existing dataset using the <code>lance.LanceDataset.merge</code> method. This allows filling in additional columns without having to rewrite the whole dataset.</p> <p>To use the <code>merge</code> method, provide a new dataset that includes the columns you want to add, and a column name to use for joining the new data to the existing dataset.</p> <p>For example, imagine we have a dataset of embeddings and ids:</p> <pre><code>table = pa.table({\n   \"id\": pa.array([1, 2, 3]),\n   \"embedding\": pa.array([np.array([1, 2, 3]), np.array([4, 5, 6]),\n                          np.array([7, 8, 9])])\n})\ndataset = lance.write_dataset(table, \"embeddings\", mode=\"overwrite\")\n</code></pre> <p>Now if we want to add a column of labels we have generated, we can do so by merging a new table:</p> <pre><code>new_data = pa.table({\n   \"id\": pa.array([1, 2, 3]),\n   \"label\": pa.array([\"horse\", \"rabbit\", \"cat\"])\n})\ndataset.merge(new_data, \"id\")\nprint(dataset.to_table().to_pandas())\n#    id  embedding   label\n# 0   1  [1, 2, 3]   horse\n# 1   2  [4, 5, 6]  rabbit\n# 2   3  [7, 8, 9]     cat\n</code></pre>"},{"location":"guide/data_evolution/#dropping-columns","title":"Dropping columns","text":"<p>Finally, you can drop columns from a dataset using the <code>lance.LanceDataset.drop_columns</code> method. This is a metadata-only operation and does not delete the data on disk. This makes it very quick.</p> <pre><code>table = pa.table({\"id\": pa.array([1, 2, 3]),\n                 \"name\": pa.array([\"Alice\", \"Bob\", \"Carla\"])})\ndataset = lance.write_dataset(table, \"names\", mode=\"overwrite\")\ndataset.drop_columns([\"name\"])\nprint(dataset.schema)\n# id: int64\n</code></pre> <p>To actually remove the data from disk, the files must be rewritten to remove the columns and then the old files must be deleted. This can be done using <code>lance.dataset.DatasetOptimizer.compact_files()</code> followed by <code>lance.LanceDataset.cleanup_old_versions()</code>.</p>"},{"location":"guide/data_evolution/#renaming-columns","title":"Renaming columns","text":"<p>Columns can be renamed using the <code>lance.LanceDataset.alter_columns</code> method.</p> <pre><code>table = pa.table({\"id\": pa.array([1, 2, 3])})\ndataset = lance.write_dataset(table, \"ids\")\ndataset.alter_columns({\"path\": \"id\", \"name\": \"new_id\"})\nprint(dataset.to_table().to_pandas())\n#    new_id\n# 0       1\n# 1       2\n# 2       3\n</code></pre> <p>This works for nested columns as well. To address a nested column, use a dot (<code>.</code>) to separate the levels of nesting. For example:</p> <pre><code>data = [\n  {\"meta\": {\"id\": 1, \"name\": \"Alice\"}},\n  {\"meta\": {\"id\": 2, \"name\": \"Bob\"}},\n]\nschema = pa.schema([\n    (\"meta\", pa.struct([\n        (\"id\", pa.int32()),\n        (\"name\", pa.string()),\n    ]))\n])\ndataset = lance.write_dataset(data, \"nested_rename\")\ndataset.alter_columns({\"path\": \"meta.id\", \"name\": \"new_id\"})\nprint(dataset.to_table().to_pandas())\n#                                  meta\n# 0  {'new_id': 1, 'name': 'Alice'}\n# 1    {'new_id': 2, 'name': 'Bob'}\n</code></pre>"},{"location":"guide/data_evolution/#casting-column-data-types","title":"Casting column data types","text":"<p>In addition to changing column names, you can also change the data type of a column using the <code>lance.LanceDataset.alter_columns</code> method. This requires rewriting that column to new data files, but does not require rewriting the other columns.</p> <p>Note</p> <p>If the column has an index, the index will be dropped if the column type is changed.</p> <p>This method can be used to change the vector type of a column. For example, we can change a float32 embedding column into a float16 column to save disk space at the cost of lower precision:</p> <pre><code>table = pa.table({\n   \"id\": pa.array([1, 2, 3]),\n   \"embedding\": pa.FixedShapeTensorArray.from_numpy_ndarray(\n       np.random.rand(3, 128).astype(\"float32\"))\n})\ndataset = lance.write_dataset(table, \"embeddings\")\ndataset.alter_columns({\"path\": \"embedding\",\n                       \"data_type\": pa.list_(pa.float16(), 128)})\nprint(dataset.schema)\n# id: int64\n# embedding: fixed_size_list&lt;item: halffloat&gt;[128]\n#   child 0, item: halffloat\n</code></pre>"},{"location":"guide/distributed_write/","title":"Distributed Write","text":"<p>Warning</p> <p>Lance provides out-of-the-box Ray and Spark integrations.</p> <p>This page is intended for users who wish to perform distributed operations in a custom manner, i.e. using <code>slurm</code> or <code>Kubernetes</code> without the Lance integration.</p>"},{"location":"guide/distributed_write/#overview","title":"Overview","text":"<p>The Lance format is designed to support parallel writing across multiple distributed workers. A distributed write operation can be performed by two phases:</p> <ol> <li>Parallel Writes: Generate new <code>lance.LanceFragment</code> in parallel across multiple workers.</li> <li>Commit: Collect all the <code>lance.FragmentMetadata</code> and commit into a single dataset in a single <code>lance.LanceOperation</code>.</li> </ol> <p></p>"},{"location":"guide/distributed_write/#write-new-data","title":"Write new data","text":"<p>Writing or appending new data is straightforward with <code>lance.fragment.write_fragments</code>.</p> <pre><code>import json\nfrom lance.fragment import write_fragments\n\n# Run on each worker\ndata_uri = \"./dist_write\"\nschema = pa.schema([\n    (\"a\", pa.int32()),\n    (\"b\", pa.string()),\n])\n\n# Run on worker 1\ndata1 = {\n    \"a\": [1, 2, 3],\n    \"b\": [\"x\", \"y\", \"z\"],\n}\nfragments_1 = write_fragments(data1, data_uri, schema=schema)\nprint(\"Worker 1: \", fragments_1)\n\n# Run on worker 2\ndata2 = {\n    \"a\": [4, 5, 6],\n    \"b\": [\"u\", \"v\", \"w\"],\n}\nfragments_2 = write_fragments(data2, data_uri, schema=schema)\nprint(\"Worker 2: \", fragments_2)\n</code></pre> <p>Output: <pre><code>Worker 1:  [FragmentMetadata(id=0, files=...)]\nWorker 2:  [FragmentMetadata(id=0, files=...)]\n</code></pre></p> <p>Now, use <code>lance.fragment.FragmentMetadata.to_json</code> to serialize the fragment metadata, and collect all serialized metadata on a single worker to execute the final commit operation.</p> <pre><code>import json\nfrom lance import FragmentMetadata, LanceOperation\n\n# Serialize Fragments into JSON data\nfragments_json1 = [json.dumps(fragment.to_json()) for fragment in fragments_1]\nfragments_json2 = [json.dumps(fragment.to_json()) for fragment in fragments_2]\n\n# On one worker, collect all fragments\nall_fragments = [FragmentMetadata.from_json(f) for f in \\\n    fragments_json1 + fragments_json2]\n\n# Commit the fragments into a single dataset\n# Use LanceOperation.Overwrite to overwrite the dataset or create new dataset.\nop = lance.LanceOperation.Overwrite(schema, all_fragments)\nread_version = 0 # Because it is empty at the time.\nlance.LanceDataset.commit(\n    data_uri,\n    op,\n    read_version=read_version,\n)\n\n# We can read the dataset using the Lance API:\ndataset = lance.dataset(data_uri)\nassert len(dataset.get_fragments()) == 2\nassert dataset.version == 1\nprint(dataset.to_table().to_pandas())\n</code></pre> <p>Output: <pre><code>     a  b\n0  1  x\n1  2  y\n2  3  z\n3  4  u\n4  5  v\n5  6  w\n</code></pre></p>"},{"location":"guide/distributed_write/#append-data","title":"Append data","text":"<p>Appending additional data follows a similar process. Use <code>lance.LanceOperation.Append</code> to commit the new fragments, ensuring that the <code>read_version</code> is set to the current dataset's version.</p> <pre><code>import lance\n\nds = lance.dataset(data_uri)\nread_version = ds.version # record the read version\n\nop = lance.LanceOperation.Append(all_fragments)\nlance.LanceDataset.commit(\n    data_uri,\n    op,\n    read_version=read_version,\n)\n</code></pre>"},{"location":"guide/distributed_write/#add-new-columns","title":"Add New Columns","text":"<p>Lance Format excels at operations such as adding columns. Thanks to its two-dimensional layout (see this blog post), adding new columns is highly efficient since it avoids copying the existing data files. Instead, the process simply creates new data files and links them to the existing dataset using metadata-only operations.</p> <pre><code>import lance\nfrom pyarrow import RecordBatch\nimport pyarrow.compute as pc\n\ndataset = lance.dataset(\"./add_columns_example\")\nassert len(dataset.get_fragments()) == 2\nassert dataset.to_table().combine_chunks() == pa.Table.from_pydict({\n    \"name\": [\"alice\", \"bob\", \"charlie\", \"craig\", \"dave\", \"eve\"],\n    \"age\": [25, 33, 44, 55, 66, 77],\n}, schema=schema)\n\n\ndef name_len(names: RecordBatch) -&gt; RecordBatch:\n    return RecordBatch.from_arrays(\n        [pc.utf8_length(names[\"name\"])],\n        [\"name_len\"],\n    )\n\n# On Worker 1\nfrag1 = dataset.get_fragments()[0]\nnew_fragment1, new_schema = frag1.merge_columns(name_len, [\"name\"])\n\n# On Worker 2\nfrag2 = dataset.get_fragments()[1]\nnew_fragment2, _ = frag2.merge_columns(name_len, [\"name\"])\n\n# On Worker 3 - Commit\nall_fragments = [new_fragment1, new_fragment2]\nop = lance.LanceOperation.Merge(all_fragments, schema=new_schema)\nlance.LanceDataset.commit(\n    \"./add_columns_example\",\n    op,\n    read_version=dataset.version,\n)\n\n# Verify dataset\ndataset = lance.dataset(\"./add_columns_example\")\nprint(dataset.to_table().to_pandas())\n</code></pre> <p>Output: <pre><code>      name  age  name_len\n0    alice   25         5\n1      bob   33         3\n2  charlie   44         7\n3    craig   55         5\n4     dave   66         4\n5      eve   77         3\n</code></pre></p>"},{"location":"guide/distributed_write/#update-columns","title":"Update Columns","text":"<p>Currently, Lance supports the fragment level update columns ability to update existing columns in a distributed manner.</p> <p>This operation performs a left-outer-hash-join with the right table (new data) on the column specified by <code>left_on</code> and <code>right_on</code>. For every row in the current fragment, the updated column value is: 1. If no matched row on the right side, the column value of the left side row. 2. If there is exactly one corresponding row on the right side, the column value    of the matching row. 3. If there are multiple corresponding rows, the column value of a random row.</p> <pre><code>import lance\nimport pyarrow as pa\n\n# Create initial dataset with two fragments\n# First fragment\ndata1 = pa.table(\n    {\n        \"id\": [1, 2, 3, 4],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n        \"score\": [85, 90, 75, 80],\n    }\n)\ndataset_uri = \"./my_dataset.lance\"\ndataset = lance.write_dataset(data1, dataset_uri)\n\n# Second fragment\ndata2 = pa.table(\n    {\n        \"id\": [5, 6, 7, 8],\n        \"name\": [\"Eve\", \"Frank\", \"Grace\", \"Henry\"],\n        \"score\": [88, 92, 78, 82],\n    }\n)\ndataset = lance.write_dataset(data2, dataset_uri, mode=\"append\")\n\n# Prepare update data for fragment 0 using 'id' as join key\nupdate_data1 = pa.table(\n    {\n        \"id\": [1, 3],\n        \"name\": [\"Alan\", \"Chase\"],\n        \"score\": [95, 85],\n    }\n)\n\n# Prepare update data for fragment 1\nupdate_data2 = pa.table(\n    {\n        \"id\": [5, 7],\n        \"name\": [\"Eva\", \"Gracie\"],\n        \"score\": [98, 88],\n    }\n)\n\n# Update fragment 0\nfragment0 = dataset.get_fragment(0)\nupdated_fragment0, fields_modified0 = fragment0.update_columns(\n    update_data1, left_on=\"id\", right_on=\"id\"\n)\n\n# Update fragment 1\nfragment1 = dataset.get_fragment(1)\nupdated_fragment1, fields_modified1 = fragment1.update_columns(\n    update_data2, left_on=\"id\", right_on=\"id\"\n)\n\nunion_fields_modified = list(set(fields_modified0 + fields_modified1))\n# Commit the changes for both fragments\nop = lance.LanceOperation.Update(\n    updated_fragments=[updated_fragment0, updated_fragment1],\n    fields_modified=union_fields_modified,\n)\nupdated_dataset = lance.LanceDataset.commit(\n    str(dataset_uri), op, read_version=dataset.version\n)\n\n# Verify the update\ndataset = lance.dataset(dataset_uri)\nprint(dataset.to_table().to_pandas())\n</code></pre> <p>Output: <pre><code>   id    name  score\n0   1    Alan     95\n1   2     Bob     90\n2   3   Chase     85\n3   4   David     80\n4   5     Eva     98\n5   6   Frank     92\n6   7  Gracie     88\n7   8   Henry     82\n</code></pre></p>"},{"location":"guide/json/","title":"JSON Support","text":"<p>Lance provides comprehensive support for storing and querying JSON data, enabling you to work with semi-structured data efficiently. This guide covers how to store JSON data in Lance datasets and use JSON functions to query and filter your data.</p>"},{"location":"guide/json/#getting-started","title":"Getting Started","text":"<pre><code>import lance\nimport pyarrow as pa\nimport json\n\n# Create a table with JSON data\njson_data = {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"}\njson_arr = pa.array([json.dumps(json_data)], type=pa.json_())\ntable = pa.table({\"id\": [1], \"data\": json_arr})\n\n# Write the dataset\nlance.write_dataset(table, \"dataset.lance\")\n</code></pre>"},{"location":"guide/json/#storage-format","title":"Storage Format","text":"<p>Lance stores JSON data internally as JSONB (binary JSON) using the <code>lance.json</code> extension type. This provides:</p> <ul> <li>Efficient storage through binary encoding</li> <li>Fast query performance for nested field access</li> <li>Compatibility with Apache Arrow's JSON type</li> </ul> <p>When you read JSON data back from Lance, it's automatically converted to Arrow's JSON type for seamless integration with your data processing pipelines.</p>"},{"location":"guide/json/#json-functions","title":"JSON Functions","text":"<p>Lance provides a comprehensive set of JSON functions for querying and filtering JSON data. These functions can be used in filter expressions with methods like <code>to_table()</code>, <code>scanner()</code>, and SQL queries through DataFusion integration.</p>"},{"location":"guide/json/#data-access-functions","title":"Data Access Functions","text":""},{"location":"guide/json/#json_extract","title":"json_extract","text":"<p>Extracts a value from JSON using JSONPath syntax.</p> <p>Syntax: <code>json_extract(json_column, json_path)</code></p> <p>Returns: JSON-formatted string representation of the extracted value</p> <p>Example: <pre><code># Sample data: {\"user\": {\"name\": \"Alice\", \"age\": 30}}\nresult = dataset.to_table(\n    filter=\"json_extract(data, '$.user.name') = '\\\"Alice\\\"'\"\n)\n# Returns: \"\\\"Alice\\\"\" for strings, \"30\" for numbers, \"true\" for booleans\n</code></pre></p> <p>Note</p> <p><code>json_extract</code> returns values in JSON format. String values include quotes (e.g., <code>\"Alice\"</code>),  numbers are returned as-is (e.g., <code>30</code>), and booleans as <code>true</code>/<code>false</code>.</p>"},{"location":"guide/json/#json_get","title":"json_get","text":"<p>Retrieves a field or array element from JSON, returning it as JSONB for further processing.</p> <p>Syntax: <code>json_get(json_column, key_or_index)</code></p> <p>Parameters: - <code>key_or_index</code>: Field name (string) or array index (numeric string like \"0\", \"1\")</p> <p>Returns: JSONB binary value (can be used for nested access)</p> <p>Example: <pre><code># Access nested JSON by chaining json_get calls\n# Sample data: {\"user\": {\"profile\": {\"name\": \"Alice\"}}}\nresult = dataset.to_table(\n    filter=\"json_get_string(json_get(json_get(data, 'user'), 'profile'), 'name') = 'Alice'\"\n)\n\n# Access array elements by index\n# Sample data: [\"first\", \"second\", \"third\"]\nresult = dataset.to_table(\n    filter=\"json_get_string(data, '0') = 'first'\"  # Gets first array element\n)\n</code></pre></p>"},{"location":"guide/json/#type-safe-value-extraction","title":"Type-Safe Value Extraction","text":"<p>These functions extract values with strict type conversion. The conversion uses JSONB's built-in strict mode, which requires values to be of compatible types:</p>"},{"location":"guide/json/#json_get_string","title":"json_get_string","text":"<p>Extracts a string value from JSON.</p> <p>Syntax: <code>json_get_string(json_column, key_or_index)</code></p> <p>Parameters: - <code>key_or_index</code>: Field name or array index (as string)</p> <p>Returns: String value (without JSON quotes), null if conversion fails</p> <p>Type Conversion: Uses strict conversion - numbers and booleans are converted to their string representation</p> <p>Example: <pre><code>result = dataset.to_table(\n    filter=\"json_get_string(data, 'name') = 'Alice'\"\n)\n\n# Array access example\n# Sample data: [\"first\", \"second\"]\nresult = dataset.to_table(\n    filter=\"json_get_string(data, '1') = 'second'\"  # Gets second array element\n)\n</code></pre></p>"},{"location":"guide/json/#json_get_int","title":"json_get_int","text":"<p>Extracts an integer value with strict type conversion.</p> <p>Syntax: <code>json_get_int(json_column, key_or_index)</code></p> <p>Returns: 64-bit integer, null if conversion fails</p> <p>Type Conversion: Uses JSONB's strict <code>to_i64()</code> conversion: - Numbers are truncated to integers - Strings must be parseable as numbers - Booleans: true \u2192 1, false \u2192 0</p> <p>Example: <pre><code># {\"age\": 30} works, {\"age\": \"30\"} may work if JSONB allows string parsing\nresult = dataset.to_table(\n    filter=\"json_get_int(data, 'age') &gt; 25\"\n)\n</code></pre></p>"},{"location":"guide/json/#json_get_float","title":"json_get_float","text":"<p>Extracts a floating-point value with strict type conversion.</p> <p>Syntax: <code>json_get_float(json_column, key_or_index)</code></p> <p>Returns: 64-bit float, null if conversion fails</p> <p>Type Conversion: Uses JSONB's strict <code>to_f64()</code> conversion: - Integers are converted to floats - Strings must be parseable as numbers - Booleans: true \u2192 1.0, false \u2192 0.0</p> <p>Example: <pre><code>result = dataset.to_table(\n    filter=\"json_get_float(data, 'score') &gt;= 90.5\"\n)\n</code></pre></p>"},{"location":"guide/json/#json_get_bool","title":"json_get_bool","text":"<p>Extracts a boolean value with strict type conversion.</p> <p>Syntax: <code>json_get_bool(json_column, key_or_index)</code></p> <p>Returns: Boolean, null if conversion fails</p> <p>Type Conversion: Uses JSONB's strict <code>to_bool()</code> conversion: - Numbers: 0 \u2192 false, non-zero \u2192 true - Strings: \"true\" \u2192 true, \"false\" \u2192 false (exact match required) - Other values may fail conversion</p> <p>Example: <pre><code>result = dataset.to_table(\n    filter=\"json_get_bool(data, 'active') = true\"\n)\n</code></pre></p>"},{"location":"guide/json/#existence-and-array-functions","title":"Existence and Array Functions","text":""},{"location":"guide/json/#json_exists","title":"json_exists","text":"<p>Checks if a JSONPath exists in the JSON data.</p> <p>Syntax: <code>json_exists(json_column, json_path)</code></p> <p>Returns: Boolean</p> <p>Example: <pre><code># Find records that have an age field\nresult = dataset.to_table(\n    filter=\"json_exists(data, '$.user.age')\"\n)\n</code></pre></p>"},{"location":"guide/json/#json_array_contains","title":"json_array_contains","text":"<p>Checks if a JSON array contains a specific value.</p> <p>Syntax: <code>json_array_contains(json_column, json_path, value)</code></p> <p>Returns: Boolean</p> <p>Comparison Logic:  - Compares array elements as JSON strings - For string matching, tries both with and without quotes - Example: searching for 'python' matches both <code>\"python\"</code> and <code>python</code> in the array</p> <p>Example: <pre><code># Sample data: {\"tags\": [\"python\", \"ml\", \"data\"]}\nresult = dataset.to_table(\n    filter=\"json_array_contains(data, '$.tags', 'python')\"\n)\n</code></pre></p>"},{"location":"guide/json/#json_array_length","title":"json_array_length","text":"<p>Returns the length of a JSON array.</p> <p>Syntax: <code>json_array_length(json_column, json_path)</code></p> <p>Returns:  - Integer: length of the array - null: if path doesn't exist - Error: if path points to a non-array value</p> <p>Example: <pre><code># Find records with more than 3 tags\nresult = dataset.to_table(\n    filter=\"json_array_length(data, '$.tags') &gt; 3\"\n)\n\n# Empty arrays return 0\nresult = dataset.to_table(\n    filter=\"json_array_length(data, '$.empty_array') = 0\"\n)\n</code></pre></p>"},{"location":"guide/json/#usage-examples","title":"Usage Examples","text":""},{"location":"guide/json/#working-with-nested-json","title":"Working with Nested JSON","text":"<pre><code>import lance\nimport pyarrow as pa\nimport json\n\n# Create nested JSON data\ndata = [\n    {\n        \"id\": 1,\n        \"user\": {\n            \"profile\": {\n                \"name\": \"Alice\",\n                \"settings\": {\n                    \"theme\": \"dark\",\n                    \"notifications\": True\n                }\n            },\n            \"scores\": [95, 87, 92]\n        }\n    },\n    {\n        \"id\": 2,\n        \"user\": {\n            \"profile\": {\n                \"name\": \"Bob\",\n                \"settings\": {\n                    \"theme\": \"light\",\n                    \"notifications\": False\n                }\n            },\n            \"scores\": [88, 91, 85]\n        }\n    }\n]\n\n# Convert to Lance dataset\njson_strings = [json.dumps(d) for d in data]\ntable = pa.table({\n    \"data\": pa.array(json_strings, type=pa.json_())\n})\n\nlance.write_dataset(table, \"nested.lance\")\ndataset = lance.dataset(\"nested.lance\")\n\n# Query nested fields using JSONPath\ndark_theme_users = dataset.to_table(\n    filter=\"json_extract(data, '$.user.profile.settings.theme') = '\\\"dark\\\"'\"\n)\n\n# Or using chained json_get\nhigh_scorers = dataset.to_table(\n    filter=\"json_array_length(data, '$.user.scores') &gt;= 3\"\n)\n</code></pre>"},{"location":"guide/json/#combining-json-with-other-data-types","title":"Combining JSON with Other Data Types","text":"<pre><code># Create mixed-type table with JSON metadata\nproducts = pa.table({\n    \"id\": [1, 2, 3],\n    \"name\": [\"Laptop\", \"Phone\", \"Tablet\"],\n    \"price\": [999.99, 599.99, 399.99],\n    \"specs\": pa.array([\n        json.dumps({\"cpu\": \"i7\", \"ram\": 16, \"storage\": 512}),\n        json.dumps({\"screen\": 6.1, \"battery\": 4000, \"5g\": True}),\n        json.dumps({\"screen\": 10.5, \"battery\": 7000, \"stylus\": True})\n    ], type=pa.json_())\n})\n\nlance.write_dataset(products, \"products.lance\")\ndataset = lance.dataset(\"products.lance\")\n\n# Find products with specific specs\nresult = dataset.to_table(\n    filter=\"price &lt; 600 AND json_get_bool(specs, '5g') = true\"\n)\n</code></pre>"},{"location":"guide/json/#handling-arrays-in-json","title":"Handling Arrays in JSON","text":"<pre><code># Create data with JSON arrays\nrecords = pa.table({\n    \"id\": [1, 2, 3],\n    \"data\": pa.array([\n        json.dumps({\"name\": \"Project A\", \"tags\": [\"python\", \"ml\", \"production\"]}),\n        json.dumps({\"name\": \"Project B\", \"tags\": [\"rust\", \"systems\"]}),\n        json.dumps({\"name\": \"Project C\", \"tags\": [\"python\", \"web\", \"api\", \"production\"]})\n    ], type=pa.json_())\n})\n\nlance.write_dataset(records, \"projects.lance\")\ndataset = lance.dataset(\"projects.lance\")\n\n# Find projects with Python\npython_projects = dataset.to_table(\n    filter=\"json_array_contains(data, '$.tags', 'python')\"\n)\n\n# Find projects with more than 3 tags\ncomplex_projects = dataset.to_table(\n    filter=\"json_array_length(data, '$.tags') &gt; 3\"\n)\n</code></pre>"},{"location":"guide/json/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Choose the right function: Use <code>json_get_*</code> functions for direct field access and type conversion; use <code>json_extract</code> for complex JSONPath queries.</li> <li>Index frequently queried paths: Consider creating computed columns for frequently accessed JSON paths to improve query performance.</li> <li>Minimize deep nesting: While Lance supports arbitrary nesting, flatter structures generally perform better.</li> <li>Understand type conversion: The <code>json_get_*</code> functions use strict type conversion, which may fail if types don't match. Plan your schema accordingly.</li> <li>Array access: When working with JSON arrays, you can access elements by index using numeric strings (e.g., \"0\", \"1\") with <code>json_get</code> functions.</li> </ol>"},{"location":"guide/json/#integration-with-datafusion","title":"Integration with DataFusion","text":"<p>All JSON functions are available when using Lance with Apache DataFusion for SQL queries. See the DataFusion Integration guide for more details on using JSON functions in SQL contexts.</p>"},{"location":"guide/json/#limitations","title":"Limitations","text":"<ul> <li>JSONPath support follows standard JSONPath syntax but may not support all advanced features</li> <li>Large JSON documents may impact query performance</li> <li>JSON functions are currently only available for filtering, not for projection in query results</li> </ul>"},{"location":"guide/migration/","title":"Migration Guides","text":"<p>Lance aims to avoid breaking changes when possible.  Currently, we are refining the Rust public API so that we can move it out of experimental status and make stronger commitments to backwards compatibility.  The python API is considered stable and breaking changes should generally be communicated (via warnings) for 1-2 months prior to being finalized to give users a chance to migrate.  This page documents the breaking changes between releases and gives advice on how to migrate.</p>"},{"location":"guide/migration/#100","title":"1.0.0","text":"<ul> <li>The <code>SearchResult</code> returned by scalar indices must now output information about null values.   Instead of containing a <code>RowIdTreeMap</code>, it now contains a <code>NullableRowIdSet</code>. Expressions that   resolve to null values must be included in search results in the null set. This ensures that   <code>NOT</code> can be applied to index search results correctly.</li> </ul>"},{"location":"guide/migration/#039","title":"0.39","text":"<ul> <li> <p>The <code>lance</code> crate no longer re-exports utilities from <code>lance-arrow</code> such as <code>RecordBatchExt</code> or <code>SchemaExt</code>.  In the short term, if you are relying on these utilities,  you can add a dependency on the <code>lance-arrow</code> crate.  However, we do not expect <code>lance-arrow</code> to ever be stable, and you may want to consider forking these utilities.</p> </li> <li> <p>Previously, we exported <code>Error</code> and <code>Result</code> as both <code>lance::Error</code> and <code>lance::error::Error</code>.  We have now reduced this to just <code>lance::Error</code>.  We have also removed some internal error utilities (such as <code>OptionExt</code>) from the public API and do not plan on reintroducing these.</p> </li> <li> <p>The Python and Rust <code>dataset::diff_meta</code> API has been removed in favor of <code>dataset::delta</code>,  which returns a <code>DatasetDelta</code> that offers both metadata diff through <code>list_transactions</code> and data diff  through <code>get_inserted_rows</code> and <code>get_updated_rows</code>.</p> </li> <li> <p>Some other minor utilities which had previously been public are now private.  It is unlikely anyone was utilizing' these.  Please open an issue if you were relying on any of these.</p> </li> <li> <p>The <code>lance-namespace</code> Rust crate now splits into <code>lance-namespace</code> that contains the main <code>LanceNamespace</code> trait  and data models, and <code>lance-namespace-impls</code> that has different implementations of the namespace.  The <code>DirectoryNamespace</code> and <code>RestNamespace</code> interfaces have been refactored to be more user friendly. The <code>DirectoryNamespace</code> also now uses Lance ObjectStore for IO instead of directly depending on Apache OpenDAL.</p> </li> </ul>"},{"location":"guide/object_store/","title":"Object Store Configuration","text":"<p>Lance supports object stores such as AWS S3 (and compatible stores), Azure Blob Store, and Google Cloud Storage. Which object store to use is determined by the URI scheme of the dataset path. For example, <code>s3://bucket/path</code> will use S3, <code>az://bucket/path</code> will use Azure, and <code>gs://bucket/path</code> will use GCS.</p> <p>These object stores take additional configuration objects. There are two ways to specify these configurations: by setting environment variables or by passing them to the <code>storage_options</code> parameter of <code>lance.dataset</code> and <code>lance.write_dataset</code>. So for example, to globally set a higher timeout, you would run in your shell:</p> <pre><code>export TIMEOUT=60s\n</code></pre> <p>If you only want to set the timeout for a single dataset, you can pass it as a storage option:</p> <pre><code>import lance\nds = lance.dataset(\"s3://path\", storage_options={\"timeout\": \"60s\"})\n</code></pre>"},{"location":"guide/object_store/#general-configuration","title":"General Configuration","text":"<p>These options apply to all object stores.</p> Key Description <code>allow_http</code> Allow non-TLS, i.e. non-HTTPS connections. Default, <code>False</code>. <code>download_retry_count</code> Number of times to retry a download. Default, <code>3</code>. This limit is applied when the HTTP request succeeds but the response is not fully downloaded, typically due to a violation of <code>request_timeout</code>. <code>allow_invalid_certificates</code> Skip certificate validation on https connections. Default, <code>False</code>. Warning: This is insecure and should only be used for testing. <code>connect_timeout</code> Timeout for only the connect phase of a Client. Default, <code>5s</code>. <code>request_timeout</code> Timeout for the entire request, from connection until the response body has finished. Default, <code>30s</code>. <code>user_agent</code> User agent string to use in requests. <code>proxy_url</code> URL of a proxy server to use for requests. Default, <code>None</code>. <code>proxy_ca_certificate</code> PEM-formatted CA certificate for proxy connections <code>proxy_excludes</code> List of hosts that bypass proxy. This is a comma separated list of domains and IP masks. Any subdomain of the provided domain will be bypassed. For example, <code>example.com, 192.168.1.0/24</code> would bypass <code>https://api.example.com</code>, <code>https://www.example.com</code>, and any IP in the range <code>192.168.1.0/24</code>. <code>client_max_retries</code> Number of times for a s3 client to retry the request. Default, <code>10</code>. <code>client_retry_timeout</code> Timeout for a s3 client to retry the request in seconds. Default, <code>180</code>."},{"location":"guide/object_store/#s3-configuration","title":"S3 Configuration","text":"<p>S3 (and S3-compatible stores) have additional configuration options that configure authorization and S3-specific features (such as server-side encryption).</p> <p>AWS credentials can be set in the environment variables <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, and <code>AWS_SESSION_TOKEN</code>. Alternatively, they can be passed as parameters to the <code>storage_options</code> parameter:</p> <pre><code>import lance\nds = lance.dataset(\n    \"s3://bucket/path\",\n    storage_options={\n        \"access_key_id\": \"my-access-key\",\n        \"secret_access_key\": \"my-secret-key\",\n        \"session_token\": \"my-session-token\",\n    }\n)\n</code></pre> <p>If you are using AWS SSO, you can specify the <code>AWS_PROFILE</code> environment variable. It cannot be specified in the <code>storage_options</code> parameter.</p> <p>The following keys can be used as both environment variables or keys in the <code>storage_options</code> parameter:</p> Key Description <code>aws_region</code> / <code>region</code> The AWS region the bucket is in. This can be automatically detected when using AWS S3, but must be specified for S3-compatible stores. <code>aws_access_key_id</code> / <code>access_key_id</code> The AWS access key ID to use. <code>aws_secret_access_key</code> / <code>secret_access_key</code> The AWS secret access key to use. <code>aws_session_token</code> / <code>session_token</code> The AWS session token to use. <code>aws_endpoint</code> / <code>endpoint</code> The endpoint to use for S3-compatible stores. <code>aws_virtual_hosted_style_request</code> / <code>virtual_hosted_style_request</code> Whether to use virtual hosted-style requests, where bucket name is part of the endpoint. Meant to be used with <code>aws_endpoint</code>. Default, <code>False</code>. <code>aws_s3_express</code> / <code>s3_express</code> Whether to use S3 Express One Zone endpoints. Default, <code>False</code>. See more details below. <code>aws_server_side_encryption</code> The server-side encryption algorithm to use. Must be one of <code>\"AES256\"</code>, <code>\"aws:kms\"</code>, or <code>\"aws:kms:dsse\"</code>. Default, <code>None</code>. <code>aws_sse_kms_key_id</code> The KMS key ID to use for server-side encryption. If set, <code>aws_server_side_encryption</code> must be <code>\"aws:kms\"</code> or <code>\"aws:kms:dsse\"</code>. <code>aws_sse_bucket_key_enabled</code> Whether to use bucket keys for server-side encryption."},{"location":"guide/object_store/#s3-compatible-stores","title":"S3-compatible stores","text":"<p>Lance can also connect to S3-compatible stores, such as MinIO. To do so, you must specify both region and endpoint:</p> <pre><code>import lance\nds = lance.dataset(\n    \"s3://bucket/path\",\n    storage_options={\n        \"region\": \"us-east-1\",\n        \"endpoint\": \"http://minio:9000\",\n    }\n)\n</code></pre> <p>This can also be done with the <code>AWS_ENDPOINT</code> and <code>AWS_DEFAULT_REGION</code> environment variables.</p>"},{"location":"guide/object_store/#s3-express-directory-bucket","title":"S3 Express (Directory Bucket)","text":"<p>Lance supports S3 Express One Zone buckets, a.k.a. S3 directory buckets. S3 Express buckets only support connecting from an EC2 instance within the same region. By default, Lance automatically recognize the <code>--x-s3</code> suffix of an express bucket, there is no special configuration needed.</p> <p>In case of an access point or private link that hides the bucket name, you can configure express bucket access explicitly through storage option <code>s3_express</code>.</p> <pre><code>import lance\nds = lance.dataset(\n    \"s3://my-bucket--use1-az4--x-s3/path/imagenet.lance\",\n    storage_options={\n        \"region\": \"us-east-1\",\n        \"s3_express\": \"true\",\n    }\n)\n</code></pre>"},{"location":"guide/object_store/#google-cloud-storage-configuration","title":"Google Cloud Storage Configuration","text":"<p>GCS credentials are configured by setting the <code>GOOGLE_SERVICE_ACCOUNT</code> environment variable to the path of a JSON file containing the service account credentials. Alternatively, you can pass the path to the JSON file in the <code>storage_options</code></p> <pre><code>import lance\nds = lance.dataset(\n    \"gs://my-bucket/my-dataset\",\n    storage_options={\n        \"service_account\": \"path/to/service-account.json\",\n    }\n)\n</code></pre> <p>Note</p> <p>By default, GCS uses HTTP/1 for communication, as opposed to HTTP/2. This improves maximum throughput significantly. However, if you wish to use HTTP/2 for some reason, you can set the environment variable <code>HTTP1_ONLY</code> to <code>false</code>.</p> <p>The following keys can be used as both environment variables or keys in the <code>storage_options</code> parameter:</p> Key Description <code>google_service_account</code> / <code>service_account</code> Path to the service account JSON file. <code>google_service_account_key</code> / <code>service_account_key</code> The serialized service account key. <code>google_application_credentials</code> / <code>application_credentials</code> Path to the application credentials."},{"location":"guide/object_store/#azure-blob-storage-configuration","title":"Azure Blob Storage Configuration","text":"<p>Azure Blob Storage credentials can be configured by setting the <code>AZURE_STORAGE_ACCOUNT_NAME</code> and <code>AZURE_STORAGE_ACCOUNT_KEY</code> environment variables. Alternatively, you can pass the account name and key in the <code>storage_options</code> parameter:</p> <pre><code>import lance\nds = lance.dataset(\n    \"az://my-container/my-dataset\",\n    storage_options={\n        \"account_name\": \"some-account\",\n        \"account_key\": \"some-key\",\n    }\n)\n</code></pre> <p>These keys can be used as both environment variables or keys in the <code>storage_options</code> parameter:</p> Key Description <code>azure_storage_account_name</code> / <code>account_name</code> The name of the azure storage account. <code>azure_storage_account_key</code> / <code>account_key</code> The serialized service account key. <code>azure_client_id</code> / <code>client_id</code> Service principal client id for authorizing requests. <code>azure_client_secret</code> / <code>client_secret</code> Service principal client secret for authorizing requests. <code>azure_tenant_id</code> / <code>tenant_id</code> Tenant id used in oauth flows. <code>azure_storage_sas_key</code> / <code>azure_storage_sas_token</code> / <code>sas_key</code> / <code>sas_token</code> Shared access signature. The signature is expected to be percent-encoded, much like they are provided in the azure storage explorer or azure portal. <code>azure_storage_token</code> / <code>bearer_token</code> / <code>token</code> Bearer token. <code>azure_storage_use_emulator</code> / <code>object_store_use_emulator</code> / <code>use_emulator</code> Use object store with azurite storage emulator. <code>azure_endpoint</code> / <code>endpoint</code> Override the endpoint used to communicate with blob storage. <code>azure_use_fabric_endpoint</code> / <code>use_fabric_endpoint</code> Use object store with url scheme account.dfs.fabric.microsoft.com. <code>azure_msi_endpoint</code> / <code>azure_identity_endpoint</code> / <code>identity_endpoint</code> / <code>msi_endpoint</code> Endpoint to request a imds managed identity token. <code>azure_object_id</code> / <code>object_id</code> Object id for use with managed identity authentication. <code>azure_msi_resource_id</code> / <code>msi_resource_id</code> Msi resource id for use with managed identity authentication. <code>azure_federated_token_file</code> / <code>federated_token_file</code> File containing token for Azure AD workload identity federation. <code>azure_use_azure_cli</code> / <code>use_azure_cli</code> Use azure cli for acquiring access token. <code>azure_disable_tagging</code> / <code>disable_tagging</code> Disables tagging objects. This can be desirable if not supported by the backing store."},{"location":"guide/performance/","title":"Lance Performance Guide","text":"<p>This guide provides tips and tricks for optimizing the performance of your Lance applications.</p>"},{"location":"guide/performance/#logging","title":"Logging","text":"<p>Lance uses the <code>log</code> crate to log messages. Displaying these log messages will depend on the client library you are using. For rust, you will need to configure a logging subscriber. For more details ses the log docs. The Python and Java clients configure a default logging subscriber that logs to stderr.</p> <p>The Python/Java logger can be configured with several environment variables:</p> <ul> <li><code>LANCE_LOG</code>: Controls log filtering based on log level and target. See the env_logger docs for more details. The <code>LANCE_LOG</code> environment variable replaces the <code>RUST_LOG</code> environment variable.</li> <li><code>LANCE_TRACING</code>: Controls tracing filtering based on log level. Key tracing events described below are emitted at   the <code>info</code> level. However, additional spans and events are available at the <code>debug</code> level which may be useful for   debugging performance issues. The default tracing level is <code>info</code>.</li> <li><code>LANCE_LOG_STYLE</code>: Controls whether colors are used in the log messages. Valid values are <code>auto</code>, <code>always</code>, <code>never</code>.</li> <li><code>LANCE_LOG_TS_PRECISION</code>: The precision of the timestamp in the log messages. Valid values are <code>ns</code>, <code>us</code>, <code>ms</code>, <code>s</code>.</li> <li><code>LANCE_LOG_FILE</code>: Redirects Rust log messages to the specified file path instead of stderr. When set, Lance will create the file and any necessary parent directories. If the file cannot be created (e.g., due to permission issues), Lance will fall back to logging to stderr.</li> </ul>"},{"location":"guide/performance/#trace-events","title":"Trace Events","text":"<p>Lance uses tracing to log events. If you are running <code>pylance</code> then these events will be emitted to as log messages. For Rust connections you can use the <code>tracing</code> crate to capture these events.</p>"},{"location":"guide/performance/#file-audit","title":"File Audit","text":"<p>File audit events are emitted when significant files are created or deleted.</p> Event Parameter Description <code>lance::file_audit</code> <code>mode</code> The mode of I/O operation (create, delete, delete_unverified) <code>lance::file_audit</code> <code>type</code> The type of file affected (manifest, data file, index file, deletion file)"},{"location":"guide/performance/#io-events","title":"I/O Events","text":"<p>I/O events are emitted when significant I/O operations are performed, particularly those related to indices. These events are NOT emitted when the index is loaded from the in-memory cache. Correct cache utilization is important for performance and these events are intended to help you debug cache usage.</p> Event Parameter Description <code>lance::io_events</code> <code>type</code> The type of I/O operation (open_scalar_index, open_vector_index, load_vector_part, load_scalar_part)"},{"location":"guide/performance/#execution-events","title":"Execution Events","text":"<p>Execution events are emitted when an execution plan is run. These events are useful for debugging query performance.</p> Event Parameter Description <code>lance::execution</code> <code>type</code> The type of execution event (plan_run is the only type today) <code>lance::execution</code> <code>output_rows</code> The number of rows in the output of the plan <code>lance::execution</code> <code>iops</code> The number of I/O operations performed by the plan <code>lance::execution</code> <code>bytes_read</code> The number of bytes read by the plan <code>lance::execution</code> <code>indices_loaded</code> The number of indices loaded by the plan <code>lance::execution</code> <code>parts_loaded</code> The number of index partitions loaded by the plan <code>lance::execution</code> <code>index_comparisons</code> The number of comparisons performed inside the various indices"},{"location":"guide/performance/#threading-model","title":"Threading Model","text":"<p>Lance is designed to be thread-safe and performant. Lance APIs can be called concurrently unless explicitly stated otherwise. Users may create multiple tables and share tables between threads. Operations may run in parallel on the same table, but some operations may lead to conflicts. For details see conflict resolution.</p> <p>Most Lance operations will use multiple threads to perform work in parallel. There are two thread pools in lance: the IO thread pool and the compute thread pool. The IO thread pool is used for reading and writing data from disk. The compute thread pool is used for performing computations on data. The number of threads in each pool can be configured by the user.</p> <p>The IO thread pool is used for reading and writing data from disk. The number of threads in the IO thread pool is determined by the object store that the operation is working with. Local object stores will use 8 threads by default. Cloud object stores will use 64 threads by default. This is a fairly conservative default and you may need 128 or 256 threads to saturate network bandwidth on some cloud providers. The <code>LANCE_IO_THREADS</code> environment variable can be used to override the number of IO threads. If you increase this variable you may also want to increase the <code>io_buffer_size</code>.</p> <p>The compute thread pool is used for performing computations on data. The number of threads in the compute thread pool is determined by the number of cores on the machine. The number of threads in the compute thread pool can be overridden by setting the <code>LANCE_CPU_THREADS</code> environment variable. This is commonly done when running multiple Lance processes on the same machine (e.g when working with tools like Ray). Keep in mind that decoding data is a compute intensive operation, even if a workload seems I/O bound (like scanning a table) it may still need quite a few compute threads to achieve peak performance.</p>"},{"location":"guide/performance/#memory-requirements","title":"Memory Requirements","text":"<p>Lance is designed to be memory efficient. Operations should stream data from disk and not require loading the entire dataset into memory. However, there are a few components of Lance that can use a lot of memory.</p>"},{"location":"guide/performance/#metadata-cache","title":"Metadata Cache","text":"<p>Lance uses a metadata cache to speed up operations. This cache holds various pieces of metadata such as file metadata, dataset manifests, etc. This cache is an LRU cache that is sized by bytes. The default size is 1 GiB.</p> <p>The metadata cache is not shared between tables by default. For best performance you should create a single table and share it across your application. Alternatively, you can create a single session and specify it when you open tables.</p> <p>Keys are often a composite of multiple fields and all keys are scoped to the dataset URI. The following items are stored in the metadata cache:</p> Item Key What is stored Dataset Manifests Dataset URI, version, and etag The manifest for the dataset Transactions Dataset URI, version The transaction for the dataset Deletion Files Dataset URI, fragment_id, version, id, file_type The deletion vector for a frag Row Id Mask Dataset URI, version The row id sequence for the dataset Row Id Index Dataset URI, version The row id index for the dataset Row Id Sequence Dataset URI, fragment_id The row id sequence for a fragment Index Metadata Dataset URI, version The index metadata for the dataset Index Details\u00b9 Dataset URI, index uuid The index details for an index File Global Meta Dataset URI, file path The global metadata for a file File Column Meta Dataset URI, file path, column index The search cache for a column <p>Notes:</p> <ol> <li>This is only stored for very old indexes which don't store their details in the manifest.</li> </ol>"},{"location":"guide/performance/#index-cache","title":"Index Cache","text":"<p>Lance uses an index cache to speed up queries. This caches vector and scalar indices in memory. The max size of this cache can be configured when creating a <code>LanceDataset</code> using the <code>index_cache_size_bytes</code> parameter. This cache is an LRU cached that is sized by bytes. The default size is 6 GiB. You can view the size of this cache by inspecting the result of <code>dataset.session().size_bytes()</code>.</p> <p>The index cache is not shared between tables. For best performance you should create a single table and share it across your application.</p> <p>Note: <code>index_cache_size</code> (specified in entries) was deprecated since version 0.30.0. Use <code>index_cache_size_bytes</code> (specified in bytes) for new code.</p>"},{"location":"guide/performance/#scanning-data","title":"Scanning Data","text":"<p>Searches (e.g. vector search, full text search) do not use a lot of memory to hold data because they don't typically return a lot of data. However, scanning data can use a lot of memory. Scanning is a streaming operation but we need enough memory to hold the data that we are scanning. The amount of memory needed is largely determined by the <code>io_buffer_size</code> and the <code>batch_size</code> variables.</p> <p>Each I/O thread should have enough memory to buffer an entire page of data. Pages today are typically between 8 and 32 MB. This means, as a rule of thumb, you should generally have about 32MB of memory per I/O thread. The default <code>io_buffer_size</code> is 2GB which is enough to buffer 64 pages of data. If you increase the number of I/O threads you should also increase the <code>io_buffer_size</code>.</p> <p>Scans will also decode data (and run any filtering or compute) in parallel on CPU threads. The amount of data decoded at any one time is determined by the <code>batch_size</code> and the size of your rows. Each CPU thread will need enough memory to hold one batch. Once batches are delivered to your application, they are no longer tracked by Lance and so if memory is a concern then you should also be careful not to accumulate memory in your own application (e.g. by running <code>to_table</code> or otherwise collecting all batches in memory.)</p> <p>The default <code>batch_size</code> is 8192 rows. When you are working with mostly scalar data you want to keep batches around 1MB and so the amount of memory needed by the compute threads is fairly small. However, when working with large data you may need to turn down the <code>batch_size</code> to keep memory usage under control. For example, when working with 1024-dimensional vector embeddings (e.g. 32-bit floats) then 8192 rows would be 32MB of data. If you spread that across 16 CPU threads then you would need 512MB of compute memory per scan. You might find working with 1024 rows per batch is more appropriate.</p> <p>In summary, scans could use up to <code>(2 * io_buffer_size) + (batch_size * num_compute_threads)</code> bytes of memory. Keep in mind that <code>io_buffer_size</code> is a soft limit (e.g. we cannot read less than one page at a time right now) and so it is not necessarily a bug if you see memory usage exceed this limit by a small margin.</p> <p>The above limits refer to limits per-scan. There is an additional limit on the number of IOPS that is applied across the entire process. This limit is specified by the <code>LANCE_PROCESS_IO_THREADS_LIMIT</code> environment variable. The default is 128 which is more than enough for most workloads. You can increase this limit if you are working with a high-throughput workload. You can even disable this limit entirely by setting it to zero. Note that this can often lead to issues with excessive retries and timeouts from the object store.</p>"},{"location":"guide/performance/#indexes","title":"Indexes","text":"<p>Training and searching indexes can have unique requirements for compute and memory. This section provides some guidance on what can be expected for different index types.</p>"},{"location":"guide/performance/#btree-index","title":"BTree Index","text":"<p>The BTree index is a two-level structure that provides efficient range queries and sorted access. It strikes a balance between an expensive memory structure containing all values and an expensive disk structure that can't be efficiently searched.</p> <p>Training a BTree index is done by sorting the column. This is done using an external sort to constrain the total memory usage to a reasonable amount. Updating a BTree index does not require re-sorting the entire column. The new values are sorted and the existing values are merged into the new sorted values in linear time.</p>"},{"location":"guide/performance/#storage-requirements","title":"Storage Requirements","text":"<p>The BTree index is essentially a sorted copy of a column. The storage requirements are therefore the same as the column but an additional 4 bytes per value is required to store the row ID and there is a small lookup structure which should be roughly 0.001% of the size of the column.</p>"},{"location":"guide/performance/#memory-requirements_1","title":"Memory Requirements","text":"<p>Training a BTree index requires some RAM but the current implementation spills to disk rather aggressively and so the total memory usage is fairly low.</p> <p>When searching a BTree index, the index is loaded into the index cache in pages. Each page contains 4096 values.</p>"},{"location":"guide/performance/#performance","title":"Performance","text":"<p>The sort stage is the most expensive step in training a BTree index. The time complexity is O(n log n) where n is the number of rows in the column. At very large scales this can be a bottleneck and a distributed sort may be necessary. Lance currently does not have anything builtin for this but work is underway to add this functionality. Training an index in parts as the data grows may be slightly more efficient than training the entire index at once if you have the flexibility to do so.</p> <p>When the BTree index is fully loaded into the index cache, the search time scales linearly with the number of rows that match the query. When the BTree index is not fully loaded into the index cache, the search time will be controlled by the number of pages that need to be loaded from disk and the speed of storage. The parts_loaded metric in the execution metrics can tell you how many pages were loaded from disk to satisfy a query.</p>"},{"location":"guide/performance/#bitmap-index","title":"Bitmap Index","text":"<p>The Bitmap index is an inverted lookup table that stores a bitmap for each possible value in the column. These bitmaps are compressed and serialized as a Roaring Bitmap.</p> <p>A bitmap index is currently trained by accumulating the column into a hash map from value to a vector of row ids. Each value is then serialized into a bitmap and stored in a file.</p>"},{"location":"guide/performance/#storage-requirements_1","title":"Storage Requirements","text":"<p>The size of a bitmap index is difficult to calculate precisely but will generally scale with the number of unique values in the column since a unique bitmap is required for each value and a single bitmap with all rows will compress more efficiently than many bitmaps with a small number of rows.</p>"},{"location":"guide/performance/#memory-requirements_2","title":"Memory Requirements","text":"<p>Since training a bitmap index requires collecting the values into a hash map you will need at least 8 bytes of memory per row. In addition, if you have many unique values, then you will need additional memory for the keys of the hash map. Training large bitmaps with many unique values at scale can be memory intensive.</p> <p>When a bitmap index is searched, bitmaps are loaded into the session cache individually. The size of the bitmap will depend on the number of rows that match the token.</p>"},{"location":"guide/performance/#performance_1","title":"Performance","text":"<p>When the bitmap index is fully loaded into the index cache, the search time scales linearly with the number of values that the query requires. This makes the bitmap very fast for equality queries or very small ranges. Queries against large ranges are currently extremely slow and the btree index is much faster for large range queries.</p> <p>When a bitmap index is not fully loaded into the index cache, the search time will be controlled by the number of bitmaps that need to be loaded from disk and the speed of storage. The parts_loaded metric in the execution metrics can tell you how many bitmaps were loaded from disk to satisfy a query.</p>"},{"location":"guide/read_and_write/","title":"Read and Write Data","text":""},{"location":"guide/read_and_write/#writing-lance-dataset","title":"Writing Lance Dataset","text":"<p>If you're familiar with Apache PyArrow, you'll find that creating a Lance dataset is straightforward. Begin by writing a <code>pyarrow.Table</code> using the <code>lance.write_dataset</code> function.</p> <pre><code>import lance\nimport pyarrow as pa\n\ntable = pa.Table.from_pylist([{\"name\": \"Alice\", \"age\": 20},\n                              {\"name\": \"Bob\", \"age\": 30}])\nds = lance.write_dataset(table, \"./alice_and_bob.lance\")\n</code></pre> <p>If the dataset is too large to fully load into memory, you can stream data using <code>lance.write_dataset</code> also supports <code>Iterator</code> of <code>pyarrow.RecordBatch</code> es. You will need to provide a <code>pyarrow.Schema</code> for the dataset in this case.</p> <pre><code>def producer() -&gt; Iterator[pa.RecordBatch]:\n    \"\"\"An iterator of RecordBatches.\"\"\"\n    yield pa.RecordBatch.from_pylist([{\"name\": \"Alice\", \"age\": 20}])\n    yield pa.RecordBatch.from_pylist([{\"name\": \"Bob\", \"age\": 30}])\n\nschema = pa.schema([\n    (\"name\", pa.string()),\n    (\"age\", pa.int32()),\n])\n\nds = lance.write_dataset(producer(),\n                         \"./alice_and_bob.lance\",\n                         schema=schema, mode=\"overwrite\")\nprint(ds.count_rows())  # Output: 2\n</code></pre> <p><code>lance.write_dataset</code> supports writing <code>pyarrow.Table</code>, <code>pandas.DataFrame</code>, <code>pyarrow.dataset.Dataset</code>, and <code>Iterator[pyarrow.RecordBatch]</code>.</p>"},{"location":"guide/read_and_write/#adding-rows","title":"Adding Rows","text":"<p>To insert data into your dataset, you can use either <code>LanceDataset.insert</code> or <code>lance.write_dataset</code> with <code>mode=append</code>.</p> <pre><code>import lance\nimport pyarrow as pa\n\ntable = pa.Table.from_pylist([{\"name\": \"Alice\", \"age\": 20},\n                              {\"name\": \"Bob\", \"age\": 30}])\nds = lance.write_dataset(table, \"./insert_example.lance\")\n\nnew_table = pa.Table.from_pylist([{\"name\": \"Carla\", \"age\": 37}])\nds.insert(new_table)\nprint(ds.to_table().to_pandas())\n#     name  age\n# 0  Alice   20\n# 1    Bob   30\n# 2  Carla   37\n\nnew_table2 = pa.Table.from_pylist([{\"name\": \"David\", \"age\": 42}])\nds = lance.write_dataset(new_table2, ds, mode=\"append\")\nprint(ds.to_table().to_pandas())\n#     name  age\n# 0  Alice   20\n# 1    Bob   30\n# 2  Carla   37\n# 3  David   42\n</code></pre>"},{"location":"guide/read_and_write/#deleting-rows","title":"Deleting rows","text":"<p>Lance supports deleting rows from a dataset using a SQL filter, as described in Filter push-down. For example, to delete Bob's row from the dataset above, one could use:</p> <pre><code>import lance\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\ndataset.delete(\"name = 'Bob'\")\ndataset2 = lance.dataset(\"./alice_and_bob.lance\")\nprint(dataset2.to_table().to_pandas())\n#     name  age\n# 0  Alice   20\n</code></pre> <p>Note</p> <p>Lance Format is immutable. Each write operation creates a new version of the dataset, so users must reopen the dataset to see the changes. Likewise, rows are removed by marking them as deleted in a separate deletion index, rather than rewriting the files. This approach is faster and avoids invalidating any indices that reference the files, ensuring that subsequent queries do not return the deleted rows.</p>"},{"location":"guide/read_and_write/#updating-rows","title":"Updating rows","text":"<p>Lance supports updating rows based on SQL expressions with the <code>lance.LanceDataset.update</code> method. For example, if we notice that Bob's name in our dataset has been sometimes written as <code>Blob</code>, we can fix that with:</p> <pre><code>import lance\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\ndataset.update({\"name\": \"'Bob'\"}, where=\"name = 'Blob'\")\n</code></pre> <p>The update values are SQL expressions, which is why <code>'Bob'</code> is wrapped in single quotes. This means we can use complex expressions that reference existing columns if we wish. For example, if two years have passed and we wish to update the ages of Alice and Bob in the same example, we could write:</p> <pre><code>import lance\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\ndataset.update({\"age\": \"age + 2\"})\n</code></pre> <p>If you are trying to update a set of individual rows with new values then it is often more efficient to use the merge insert operation described below.</p> <pre><code>import lance\n\n# Change the ages of both Alice and Bob\nnew_table = pa.Table.from_pylist([{\"name\": \"Alice\", \"age\": 30},\n                                  {\"name\": \"Bob\", \"age\": 20}])\n\n# This works, but is inefficient, see below for a better approach\ndataset = lance.dataset(\"./alice_and_bob.lance\")\nfor idx in range(new_table.num_rows):\n  name = new_table[0][idx].as_py()\n  new_age = new_table[1][idx].as_py()\n  dataset.update({\"age\": new_age}, where=f\"name='{name}'\")\n</code></pre>"},{"location":"guide/read_and_write/#merge-insert","title":"Merge Insert","text":"<p>Lance supports a merge insert operation. This can be used to add new data in bulk while also (potentially) matching against existing data. This operation can be used for a number of different use cases.</p>"},{"location":"guide/read_and_write/#bulk-update","title":"Bulk Update","text":"<p>The <code>lance.LanceDataset.update</code> method is useful for updating rows based on a filter. However, if we want to replace existing rows with new rows then a <code>lance.LanceDataset.merge_insert</code> operation would be more efficient:</p> <pre><code>import lance\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\nprint(dataset.to_table().to_pandas())\n#     name  age\n# 0  Alice   20\n# 1    Bob   30\n\n# Change the ages of both Alice and Bob\nnew_table = pa.Table.from_pylist([{\"name\": \"Alice\", \"age\": 2},\n                                  {\"name\": \"Bob\", \"age\": 3}])\n# This will use `name` as the key for matching rows.  Merge insert\n# uses a JOIN internally and so you typically want this column to\n# be a unique key or id of some kind.\nrst = dataset.merge_insert(\"name\") \\\n       .when_matched_update_all() \\\n       .execute(new_table)\nprint(dataset.to_table().to_pandas())\n#     name  age\n# 0  Alice    2\n# 1    Bob    3\n</code></pre> <p>Note that, similar to the update operation, rows that are modified will be removed and inserted back into the table, changing their position to the end. Also, the relative order of these rows could change because we are using a hash-join operation internally.</p>"},{"location":"guide/read_and_write/#insert-if-not-exists","title":"Insert if not Exists","text":"<p>Sometimes we only want to insert data if we haven't already inserted it before. This can happen, for example, when we have a batch of data but we don't know which rows we've added previously and we don't want to create duplicate rows. We can use the merge insert operation to achieve this:</p> <pre><code># Bob is already in the table, but Carla is new\nnew_table = pa.Table.from_pylist([{\"name\": \"Bob\", \"age\": 30},\n                                  {\"name\": \"Carla\", \"age\": 37}])\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\n\n# This will insert Carla but leave Bob unchanged\n_ = dataset.merge_insert(\"name\") \\\n       .when_not_matched_insert_all() \\\n       .execute(new_table)\n# Verify that Carla was added but Bob remains unchanged\nprint(dataset.to_table().to_pandas())\n#     name  age\n# 0  Alice   20\n# 1    Bob   30\n# 2  Carla   37\n</code></pre>"},{"location":"guide/read_and_write/#update-or-insert-upsert","title":"Update or Insert (Upsert)","text":"<p>Sometimes we want to combine both of the above behaviors. If a row already exists we want to update it. If the row does not exist we want to add it. This operation is sometimes called \"upsert\". We can use the merge insert operation to do this as well:</p> <pre><code>import lance\nimport pyarrow as pa\n\n# Change Carla's age and insert David\nnew_table = pa.Table.from_pylist([{\"name\": \"Carla\", \"age\": 27},\n                                  {\"name\": \"David\", \"age\": 42}])\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\n\n# This will update Carla and insert David\n_ = dataset.merge_insert(\"name\") \\\n       .when_matched_update_all() \\\n       .when_not_matched_insert_all() \\\n       .execute(new_table)\n# Verify the results\nprint(dataset.to_table().to_pandas())\n#     name  age\n# 0  Alice   20\n# 1    Bob   30\n# 2  Carla   27\n# 3  David   42\n</code></pre>"},{"location":"guide/read_and_write/#replace-a-portion-of-data","title":"Replace a Portion of Data","text":"<p>A less common, but still useful, behavior can be to replace some region of existing rows (defined by a filter) with new data. This is similar to performing both a delete and an insert in a single transaction. For example:</p> <pre><code>import lance\nimport pyarrow as pa\n\nnew_table = pa.Table.from_pylist([{\"name\": \"Edgar\", \"age\": 46},\n                                  {\"name\": \"Francene\", \"age\": 44}])\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\nprint(dataset.to_table().to_pandas())\n#       name  age\n# 0    Alice   20\n# 1      Bob   30\n# 2  Charlie   45\n# 3    Donna   50\n\n# This will remove anyone above 40 and insert our new data\n_ = dataset.merge_insert(\"name\") \\\n       .when_not_matched_insert_all() \\\n       .when_not_matched_by_source_delete(\"age &gt;= 40\") \\\n       .execute(new_table)\n# Verify the results - people over 40 replaced with new data\nprint(dataset.to_table().to_pandas())\n#        name  age\n# 0     Alice   20\n# 1       Bob   30\n# 2     Edgar   46\n# 3  Francene   44\n</code></pre>"},{"location":"guide/read_and_write/#reading-lance-dataset","title":"Reading Lance Dataset","text":"<p>To open a Lance dataset, use the <code>lance.dataset</code> function:</p> <pre><code>import lance\nds = lance.dataset(\"s3://bucket/path/imagenet.lance\")\n# Or local path\nds = lance.dataset(\"./imagenet.lance\")\n</code></pre> <p>Note</p> <p>Lance supports local file system, AWS <code>s3</code> and Google Cloud Storage(<code>gs</code>) as storage backends at the moment. Read more in Object Store Configuration.</p> <p>The most straightforward approach for reading a Lance dataset is to utilize the <code>lance.LanceDataset.to_table</code> method in order to load the entire dataset into memory.</p> <pre><code>table = ds.to_table()\n</code></pre> <p>Due to Lance being a high-performance columnar format, it enables efficient reading of subsets of the dataset by utilizing Column (projection) push-down and filter (predicates) push-downs.</p> <pre><code>table = ds.to_table(\n    columns=[\"image\", \"label\"],\n    filter=\"label = 2 AND text IS NOT NULL\",\n    limit=1000,\n    offset=3000)\n</code></pre> <p>Lance understands the cost of reading heavy columns such as <code>image</code>. Consequently, it employs an optimized query plan to execute the operation efficiently.</p>"},{"location":"guide/read_and_write/#iterative-read","title":"Iterative Read","text":"<p>If the dataset is too large to fit in memory, you can read it in batches using the <code>lance.LanceDataset.to_batches</code> method:</p> <pre><code>for batch in ds.to_batches(columns=[\"image\"], filter=\"label = 10\"):\n    # do something with batch\n    compute_on_batch(batch)\n</code></pre> <p>Unsurprisingly, <code>lance.LanceDataset.to_batches</code> takes the same parameters as <code>lance.LanceDataset.to_table</code> function.</p>"},{"location":"guide/read_and_write/#filter-push-down","title":"Filter push-down","text":"<p>Lance embraces the utilization of standard SQL expressions as predicates for dataset filtering. By pushing down the SQL predicates directly to the storage system, the overall I/O load during a scan is significantly reduced.</p> <p>Currently, Lance supports a growing list of expressions.</p> <ul> <li><code>&gt;</code>, <code>&gt;=</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>=</code></li> <li><code>AND</code>, <code>OR</code>, <code>NOT</code></li> <li><code>IS NULL</code>, <code>IS NOT NULL</code></li> <li><code>IS TRUE</code>, <code>IS NOT TRUE</code>, <code>IS FALSE</code>, <code>IS NOT FALSE</code></li> <li><code>IN</code></li> <li><code>LIKE</code>, <code>NOT LIKE</code></li> <li><code>regexp_match(column, pattern)</code></li> <li><code>CAST</code></li> </ul> <p>For example, the following filter string is acceptable:</p> <pre><code>((label IN [10, 20]) AND (note['email'] IS NOT NULL))\n    OR NOT note['created']\n</code></pre> <p>Nested fields can be accessed using the subscripts. Struct fields can be subscripted using field names, while list fields can be subscripted using indices.</p> <p>If your column name contains special characters or is a SQL Keyword, you can use backtick (<code>`</code>) to escape it. For nested fields, each segment of the path must be wrapped in backticks.</p> <pre><code>`CUBE` = 10 AND `column name with space` IS NOT NULL\n  AND `nested with space`.`inner with space` &lt; 2\n</code></pre> <p>Warning</p> <p>Field names containing periods (<code>.</code>) are not supported.</p> <p>Literals for dates, timestamps, and decimals can be written by writing the string value after the type name. For example</p> <pre><code>date_col = date '2021-01-01'\nand timestamp_col = timestamp '2021-01-01 00:00:00'\nand decimal_col = decimal(8,3) '1.000'\n</code></pre> <p>For timestamp columns, the precision can be specified as a number in the type parameter. Microsecond precision (6) is the default.</p> SQL Time unit <code>timestamp(0)</code> Seconds <code>timestamp(3)</code> Milliseconds <code>timestamp(6)</code> Microseconds <code>timestamp(9)</code> Nanoseconds <p>Lance internally stores data in Arrow format. The mapping from SQL types to Arrow is:</p> SQL type Arrow type <code>boolean</code> <code>Boolean</code> <code>tinyint</code> / <code>tinyint unsigned</code> <code>Int8</code> / <code>UInt8</code> <code>smallint</code> / <code>smallint unsigned</code> <code>Int16</code> / <code>UInt16</code> <code>int</code> or <code>integer</code> / <code>int unsigned</code> or <code>integer unsigned</code> <code>Int32</code> / <code>UInt32</code> <code>bigint</code> / <code>bigint unsigned</code> <code>Int64</code> / <code>UInt64</code> <code>float</code> <code>Float32</code> <code>double</code> <code>Float64</code> <code>decimal(precision, scale)</code> <code>Decimal128</code> <code>date</code> <code>Date32</code> <code>timestamp</code> <code>Timestamp</code> (1) <code>string</code> <code>Utf8</code> <code>binary</code> <code>Binary</code> <p>(1) See precision mapping in previous table.</p>"},{"location":"guide/read_and_write/#random-read","title":"Random read","text":"<p>One distinct feature of Lance, as columnar format, is that it allows you to read random samples quickly.</p> <pre><code># Access the 2nd, 101th and 501th rows\ndata = ds.take([1, 100, 500], columns=[\"image\", \"label\"])\n</code></pre> <p>The ability to achieve fast random access to individual rows plays a crucial role in facilitating various workflows such as random sampling and shuffling in ML training. Additionally, it empowers users to construct secondary indices, enabling swift execution of queries for enhanced performance.</p>"},{"location":"guide/read_and_write/#table-maintenance","title":"Table Maintenance","text":"<p>Some operations over time will cause a Lance dataset to have a poor layout. For example, many small appends will lead to a large number of small fragments. Or deleting many rows will lead to slower queries due to the need to filter out deleted rows.</p> <p>To address this, Lance provides methods for optimizing dataset layout.</p>"},{"location":"guide/read_and_write/#compact-data-files","title":"Compact data files","text":"<p>Data files can be rewritten so there are fewer files. When passing a <code>target_rows_per_fragment</code> to <code>lance.dataset.DatasetOptimizer.compact_files</code>, Lance will skip any fragments that are already above that row count, and rewrite others. Fragments will be merged according to their fragment ids, so the inherent ordering of the data will be preserved.</p> <p>Note</p> <p>Compaction creates a new version of the table. It does not delete the old version of the table and the files referenced by it.</p> <pre><code>import lance\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\ndataset.optimize.compact_files(target_rows_per_fragment=1024 * 1024)\n</code></pre> <p>During compaction, Lance can also remove deleted rows. Rewritten fragments will not have deletion files. This can improve scan performance since the soft deleted rows don't have to be skipped during the scan.</p> <p>When files are rewritten, the original row addresses are invalidated. This means the affected files are no longer part of any ANN index if they were before. Because of this, it's recommended to rewrite files before re-building indices.</p>"},{"location":"guide/tags/","title":"Manage Tags","text":"<p>Lance, much like Git, employs the <code>LanceDataset.tags</code> property to label specific versions within a dataset's history.</p> <p><code>Tags</code> are particularly useful for tracking the evolution of datasets, especially in machine learning workflows where datasets are frequently updated. For example, you can <code>create</code>, <code>update</code>, and <code>delete</code> or <code>list</code> tags.</p> <p>Note</p> <p>Creating or deleting tags does not generate new dataset versions. Tags exist as auxiliary metadata stored in a separate directory.</p> <pre><code>import lance\nds = lance.dataset(\"./tags.lance\")\nprint(len(ds.versions()))\n# 2\nprint(ds.tags.list())\n# {}\nds.tags.create(\"v1-prod\", 1)\nprint(ds.tags.list())\n# {'v1-prod': {'version': 1, 'manifest_size': ...}}\nds.tags.update(\"v1-prod\", 2)\nprint(ds.tags.list())\n# {'v1-prod': {'version': 2, 'manifest_size': ...}}\nds.tags.delete(\"v1-prod\")\nprint(ds.tags.list())\n# {}\nprint(ds.tags.list_ordered())\n# []\nds.tags.create(\"v1-prod\", 1)\nprint(ds.tags.list_ordered())\n# [('v1-prod', {'version': 1, 'manifest_size': ...})]\nds.tags.update(\"v1-prod\", 2)\nprint(ds.tags.list_ordered())\n# [('v1-prod', {'version': 2, 'manifest_size': ...})]\nds.tags.delete(\"v1-prod\")\nprint(ds.tags.list_ordered())\n# []\n</code></pre> <p>Note</p> <p>Tagged versions are exempted from the <code>LanceDataset.cleanup_old_versions()</code> process.</p> <p>To remove a version that has been tagged, you must first <code>LanceDataset.tags.delete()</code> the associated tag. </p>"},{"location":"guide/tokenizer/","title":"Tokenizers","text":"<p>Currently, Lance has built-in support for Jieba and Lindera. However, it doesn't come with its own language models. If tokenization is needed, you can download language models by yourself. You can specify the location where the language models are stored by setting the environment variable LANCE_LANGUAGE_MODEL_HOME. If it's not set, the default value is</p> <pre><code>${system data directory}/lance/language_models\n</code></pre> <p>It also supports configuring user dictionaries, which makes it convenient for users to expand their own dictionaries without retraining the language models.</p>"},{"location":"guide/tokenizer/#language-models-of-jieba","title":"Language Models of Jieba","text":""},{"location":"guide/tokenizer/#downloading-the-model","title":"Downloading the Model","text":"<pre><code>python -m lance.download jieba\n</code></pre> <p>The language model is stored by default in <code>${LANCE_LANGUAGE_MODEL_HOME}/jieba/default</code>.</p>"},{"location":"guide/tokenizer/#using-the-model","title":"Using the Model","text":"<pre><code>ds.create_scalar_index(\"text\", \"INVERTED\", base_tokenizer=\"jieba/default\")\n</code></pre>"},{"location":"guide/tokenizer/#user-dictionaries","title":"User Dictionaries","text":"<p>Create a file named config.json in the root directory of the current model.</p> <pre><code>{\n    \"main\": \"dict.txt\",\n    \"users\": [\"path/to/user/dict.txt\"]\n}\n</code></pre> <ul> <li>The \"main\" field is optional. If not filled, the default is \"dict.txt\".</li> <li>\"users\" is the path of the user dictionary. For the format of the user dictionary, please refer to https://github.com/messense/jieba-rs/blob/main/src/data/dict.txt.</li> </ul>"},{"location":"guide/tokenizer/#language-models-of-lindera","title":"Language Models of Lindera","text":""},{"location":"guide/tokenizer/#downloading-the-model_1","title":"Downloading the Model","text":"<pre><code>python -m lance.download lindera -l [ipadic|ko-dic|unidic]\n</code></pre> <p>Note that the language models of Lindera need to be compiled. Please install lindera-cli first. For detailed steps, please refer to https://github.com/lindera/lindera/tree/main/lindera-cli.</p> <p>The language model is stored by default in ${LANCE_LANGUAGE_MODEL_HOME}/lindera/[ipadic|ko-dic|unidic]</p>"},{"location":"guide/tokenizer/#using-the-model_1","title":"Using the Model","text":"<pre><code>ds.create_scalar_index(\"text\", \"INVERTED\", base_tokenizer=\"lindera/ipadic\")\n</code></pre>"},{"location":"guide/tokenizer/#user-dictionaries_1","title":"User Dictionaries","text":"<p>Create a file named config.yml in the root directory of your model, or specify a custom YAML file using the <code>LINDERA_CONFIG_PATH</code> environment variable. If both are provided, the config.yml in the root directory will be used. For more detailed configuration methods, see the lindera documentation at https://github.com/lindera/lindera/.</p> <pre><code>segmenter:\n    mode: \"normal\"\n    dictionary:\n        # Note: in lance, the `kind` field is not supported. You need to specify the model path using the `path` field instead.\n        path: /path/to/lindera/ipadic/main\n</code></pre>"},{"location":"guide/tokenizer/#create-your-own-language-model","title":"Create your own language model","text":"<p>Put your language model into <code>LANCE_LANGUAGE_MODEL_HOME</code>. </p>"},{"location":"integrations/datafusion/","title":"Apache DataFusion Integration","text":"<p>Lance datasets can be queried with Apache Datafusion,  an extensible query engine written in Rust that uses Apache Arrow as its in-memory format.  This means you can write complex SQL queries to analyze your data in Lance.</p> <p>The integration allows users to pass down column selections and basic filters to Lance,  reducing the amount of scanned data when executing your query.  Additionally, the integration allows streaming data from Lance datasets, which allows users to do aggregation larger-than-memory.</p>"},{"location":"integrations/datafusion/#rust","title":"Rust","text":"<p>Lance includes a DataFusion table provider <code>lance::datafusion::LanceTableProvider</code>. Users can register a Lance dataset as a table in DataFusion and run SQL with it:</p>"},{"location":"integrations/datafusion/#simple-sql","title":"Simple SQL","text":"<pre><code>use datafusion::prelude::SessionContext;\nuse lance::datafusion::LanceTableProvider;\n\nlet ctx = SessionContext::new();\n\nctx.register_table(\"dataset\",\n    Arc::new(LanceTableProvider::new(\n    Arc::new(dataset.clone()),\n    /* with_row_id */ false,\n    /* with_row_addr */ false,\n    )))?;\n\nlet df = ctx.sql(\"SELECT * FROM dataset LIMIT 10\").await?;\nlet result = df.collect().await?;\n</code></pre>"},{"location":"integrations/datafusion/#join-2-tables","title":"Join 2 Tables","text":"<pre><code>use datafusion::prelude::SessionContext;\nuse lance::datafusion::LanceTableProvider;\n\nlet ctx = SessionContext::new();\n\nctx.register_table(\"orders\",\n    Arc::new(LanceTableProvider::new(\n    Arc::new(orders_dataset.clone()),\n    /* with_row_id */ false,\n    /* with_row_addr */ false,\n    )))?;\n\nctx.register_table(\"customers\",\n    Arc::new(LanceTableProvider::new(\n    Arc::new(customers_dataset.clone()),\n    /* with_row_id */ false,\n    /* with_row_addr */ false,\n    )))?;\n\nlet df = ctx.sql(\"\n    SELECT o.order_id, o.amount, c.customer_name \n    FROM orders o \n    JOIN customers c ON o.customer_id = c.customer_id\n    LIMIT 10\n\").await?;\n\nlet result = df.collect().await?;\n</code></pre>"},{"location":"integrations/datafusion/#register-udf","title":"Register UDF","text":"<p>Lance provides some built-in UDFs, which users can manually register and use in queries. The following example demonstrates how to register and use <code>contains_tokens</code>.</p> <pre><code>use datafusion::prelude::SessionContext;\nuse lance::datafusion::LanceTableProvider;\nuse lance_datafusion::udf::register_functions;\n\nlet ctx = SessionContext::new();\n\n// Register built-in UDFs\nregister_functions(&amp;ctx);\n\nctx.register_table(\"dataset\",\n    Arc::new(LanceTableProvider::new(\n    Arc::new(dataset.clone()),\n    /* with_row_id */ false,\n    /* with_row_addr */ false,\n    )))?;\n\nlet df = ctx.sql(\"SELECT * FROM dataset WHERE contains_tokens(text, 'cat')\").await?;\nlet result = df.collect().await?;\n</code></pre>"},{"location":"integrations/datafusion/#json-functions","title":"JSON Functions","text":"<p>Lance provides comprehensive JSON support through a set of built-in UDFs that are automatically registered when you use <code>register_functions()</code>. These functions enable you to query and filter JSON data efficiently.</p> <p>For a complete guide to JSON functions including: - <code>json_extract</code> - Extract values using JSONPath - <code>json_get</code>, <code>json_get_string</code>, <code>json_get_int</code>, <code>json_get_float</code>, <code>json_get_bool</code> - Type-safe value extraction - <code>json_exists</code> - Check if a path exists - <code>json_array_contains</code>, <code>json_array_length</code> - Array operations</p> <p>See the JSON Support Guide for detailed documentation and examples.</p> <p>Example: Querying JSON in SQL <pre><code>// After registering functions as shown above\nlet df = ctx.sql(\"\n    SELECT * FROM dataset \n    WHERE json_get_string(metadata, 'category') = 'electronics'\n    AND json_array_contains(metadata, '$.tags', 'featured')\n\").await?;\n</code></pre></p>"},{"location":"integrations/datafusion/#python","title":"Python","text":"<p>In Python, this integration is done via Datafusion FFI. An FFI table provider <code>FFILanceTableProvider</code> is included in <code>pylance</code>. For example, if I want to query <code>my_lance_dataset</code>:</p> <pre><code>from datafusion import SessionContext # pip install datafusion\nfrom lance import FFILanceTableProvider\n\nctx = SessionContext()\n\ntable1 = FFILanceTableProvider(\n    my_lance_dataset, with_row_id=True, with_row_addr=True\n)\nctx.register_table(\"table1\", table1)\nctx.table(\"table1\")\nctx.sql(\"SELECT * FROM table1 LIMIT 10\")\n</code></pre>"},{"location":"integrations/duckdb/","title":"DuckDB","text":"<p>Lance datasets can be queried in SQL with DuckDB, an in-process OLAP relational database. Using DuckDB means you can write complex SQL queries (that may not yet be supported in Lance), without needing to move your data out of Lance.</p> <p>Note</p> <p>This integration is done via a DuckDB extension, whose source code and latest documentation (via <code>README.md</code>) is available here. To ensure you see the most up-to-date examples and syntax, check out the repo and the  DuckDB extension documentation page.</p>"},{"location":"integrations/duckdb/#installation","title":"Installation","text":""},{"location":"integrations/duckdb/#python-dependencies","title":"Python dependencies","text":"<ul> <li>To use DuckDB's CLI, install it using the steps shown in their docs.</li> <li>To run the code in Python, install Lance, DuckDB and PyArrow as shown below.</li> </ul> <pre><code>pip install pylance duckdb pyarrow\n</code></pre>"},{"location":"integrations/duckdb/#install-the-lance-extension-in-duckdb","title":"Install the Lance extension in DuckDB","text":"<p>We're now ready to begin querying Lance using DuckDB! First, install the extension.</p> SQLPython <pre><code>INSTALL lance FROM community;\nLOAD lance;\n</code></pre> <pre><code>import duckdb\n\nduckdb.sql(\n    \"\"\"\n    INSTALL lance FROM community;\n    LOAD lance;\n    \"\"\"\n)\n</code></pre> Update extensions <p>If you already have the extension installed locally, run the following command to update it to the latest version: <pre><code>UPDATE EXTENSIONS;\n</code></pre></p>"},{"location":"integrations/duckdb/#examples","title":"Examples","text":"<p>All examples below reuse a small dataset with three rows (duck, horse, dragon) and a <code>vector</code> column with representative values. In the real world, you'd have a high-dimensional array generated by an embedding model, and a much larger Lance dataset.</p>"},{"location":"integrations/duckdb/#write-a-duckdb-table-as-a-lance-dataset","title":"Write a DuckDB table as a Lance dataset","text":"<p>Use DuckDB's <code>COPY ... TO ...</code> to materialize query results as a Lance dataset.</p> SQLPython <pre><code>COPY (\n  SELECT *\n  FROM (\n    VALUES\n      ('duck', 'quack', [0.9, 0.7, 0.1]::FLOAT[]),\n      ('horse', 'neigh', [0.3, 0.1, 0.5]::FLOAT[]),\n      ('dragon', 'roar', [0.5, 0.2, 0.7]::FLOAT[])\n  ) AS t(animal, noise, vector)\n) TO './lance_duck.lance' (FORMAT lance, mode 'overwrite');\n</code></pre> <pre><code>import duckdb\n\nduckdb.sql(\n    \"\"\"\n    COPY (\n      SELECT *\n      FROM (\n        VALUES\n          ('duck', 'quack', [0.9, 0.7, 0.1]::FLOAT[]),\n          ('horse', 'neigh', [0.3, 0.1, 0.5]::FLOAT[]),\n          ('dragon', 'roar', [0.5, 0.2, 0.7]::FLOAT[])\n      ) AS t(animal, noise, vector)\n    ) TO './lance_duck.lance' (FORMAT lance, mode 'overwrite');\n    \"\"\"\n)\n</code></pre>"},{"location":"integrations/duckdb/#query-a-lance-dataset-from-duckdb","title":"Query a Lance dataset from DuckDB","text":"<p>Now that the Lance dataset is written, let's query it using SQL in DuckDB.</p> SQLPython <pre><code>SELECT *\n  FROM './lance_duck.lance'\n  LIMIT 5;\n</code></pre> <pre><code>import duckdb\n\nr1 = duckdb.sql(\n    \"\"\"\n    SELECT *\n      FROM './lance_duck.lance'\n      LIMIT 5;\n    \"\"\"\n)\nprint(r1)\n</code></pre> <p>This returns:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 animal  \u2502  noise  \u2502     vector      \u2502\n\u2502 varchar \u2502 varchar \u2502     float[]     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 duck    \u2502 quack   \u2502 [0.9, 0.7, 0.1] \u2502\n\u2502 horse   \u2502 neigh   \u2502 [0.3, 0.1, 0.5] \u2502\n\u2502 dragon  \u2502 roar    \u2502 [0.5, 0.2, 0.7] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> Query S3 paths directly <p>To access object store URIs (such as <code>s3://...</code>), configure a <code>TYPE LANCE</code> secret.</p> <pre><code>CREATE SECRET (\n  TYPE LANCE,\n  PROVIDER credential_chain,\n  SCOPE 's3://bucket/'\n);\n\nSELECT *\n  FROM 's3://bucket/path/to/dataset.lance'\n  LIMIT 5;\n</code></pre>"},{"location":"integrations/duckdb/#create-a-lance-dataset-via-create-table-directory-namespace","title":"Create a Lance dataset via CREATE TABLE (directory namespace)","text":"<p>When you <code>ATTACH</code> a directory as a Lance namespace, you can create new datasets using <code>CREATE TABLE</code> or <code>CREATE TABLE AS SELECT</code>. The dataset is written to <code>&lt;namespace_root&gt;/&lt;table_name&gt;.lance</code>.</p> SQLPython <pre><code>ATTACH './lance_ns' AS lance_ns (TYPE LANCE);\n\nCREATE TABLE lance_ns.main.duck_animals AS\n  SELECT *\n  FROM (\n    VALUES\n      ('duck', 'quack', [0.9, 0.7, 0.1]::FLOAT[]),\n      ('horse', 'neigh', [0.3, 0.1, 0.5]::FLOAT[]),\n      ('dragon', 'roar', [0.5, 0.2, 0.7]::FLOAT[])\n  ) AS t(animal, noise, vector);\n</code></pre> <pre><code>import duckdb\n\nduckdb.sql(\n    \"\"\"\n    ATTACH './lance_ns' AS lance_ns (TYPE LANCE);\n\n    CREATE TABLE lance_ns.main.duck_animals AS\n      SELECT *\n      FROM (\n        VALUES\n          ('duck', 'quack', [0.9, 0.7, 0.1]::FLOAT[]),\n          ('horse', 'neigh', [0.3, 0.1, 0.5]::FLOAT[]),\n          ('dragon', 'roar', [0.5, 0.2, 0.7]::FLOAT[])\n      ) AS t(animal, noise, vector);\n    \"\"\"\n)\n</code></pre> <p>You can then query the namespace as follows:</p> <pre><code>SELECT count(*) FROM lance_ns.main.duck_animals;\n</code></pre> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 count_star() \u2502\n\u2502    int64     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502      3       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"integrations/duckdb/#vector-search","title":"Vector search","text":"<p>You can perform vector search on a column. This returns the <code>_distance</code> (smaller is closer, so sort in ascending order for nearest neighbors). The example vector here is similar to the query \"duck\".</p> SQLPython <pre><code>SELECT animal, noise, vector, _distance\n  FROM lance_vector_search(\n    './lance_duck.lance',\n    'vector',\n    [0.8, 0.7, 0.2]::FLOAT[],\n    k = 1,\n    prefilter = true\n  )\n  ORDER BY _distance ASC;\n</code></pre> <pre><code>import duckdb\n\nr2 = duckdb.sql(\n    \"\"\"\n    SELECT animal, noise, vector, _distance\n      FROM lance_vector_search(\n        './lance_duck.lance',\n        'vector',\n        [0.8, 0.7, 0.2]::FLOAT[],\n        k = 1,\n        prefilter = true\n      )\n      ORDER BY _distance ASC;\n    \"\"\"\n)\nprint(r2)\n</code></pre> <p>This returns: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 animal  \u2502  noise  \u2502     vector      \u2502\n\u2502 varchar \u2502 varchar \u2502     float[]     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 duck    \u2502 quack   \u2502 [0.9, 0.7, 0.1] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"integrations/duckdb/#full-text-search","title":"Full-text search","text":"<p>Run keyword-based BM25 search as shown below. This returns a <code>_score</code>, which is sorted in descending order to get the most relevant results.</p> SQLPython <pre><code>SELECT animal, noise, vector, _score\n  FROM lance_fts(\n    './lance_duck.lance',\n    'animal',\n    'the brave knight faced the dragon',\n    k = 1,\n    prefilter = true\n  )\n  ORDER BY _score DESC;\n</code></pre> <pre><code>import duckdb\n\nr3 = duckdb.sql(\n    \"\"\"\n    SELECT animal, noise, vector, _score\n      FROM lance_fts(\n        './lance_duck.lance',\n        'animal',\n        'the brave knight faced the dragon',\n        k = 1,\n        prefilter = true\n      )\n      ORDER BY _score DESC;\n    \"\"\"\n)\nprint(r3)\n</code></pre> <p>This returns:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 animal  \u2502  noise  \u2502     vector      \u2502\n\u2502 varchar \u2502 varchar \u2502     float[]     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dragon  \u2502 roar    \u2502 [0.5, 0.2, 0.7] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"integrations/duckdb/#hybrid-search","title":"Hybrid search","text":"<p>Hybrid search combines vector and FTS scores, returning a <code>_hybrid_score</code> in addition to <code>_distance</code> / <code>_score</code>. To get the most relevant results, sort in descending order.</p> SQLPython <pre><code>SELECT animal, noise, vector, _hybrid_score, _distance, _score\n  FROM lance_hybrid_search(\n    './lance_duck.lance',\n    'vector',\n    [0.8, 0.7, 0.2]::FLOAT[],\n    'animal',\n    'the duck surprised the dragon',\n    k = 2,\n    prefilter = false,\n    alpha = 0.5,\n    oversample_factor = 4\n  )\n  ORDER BY _hybrid_score DESC;\n</code></pre> <pre><code>import duckdb\n\nr4 = duckdb.sql(\n    \"\"\"\n    SELECT animal, noise, vector, _hybrid_score, _distance, _score\n      FROM lance_hybrid_search(\n        './lance_duck.lance',\n        'vector',\n        [0.8, 0.7, 0.2]::FLOAT[],\n        'animal',\n        'the duck surprised the dragon',\n        k = 2,\n        prefilter = false,\n        alpha = 0.5,\n        oversample_factor = 4\n      )\n      ORDER BY _hybrid_score DESC;\n    \"\"\"\n)\nprint(r4)\n</code></pre> <p>This returns: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 animal  \u2502  noise  \u2502     vector      \u2502\n\u2502 varchar \u2502 varchar \u2502     float[]     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 duck    \u2502 quack   \u2502 [0.9, 0.7, 0.1] \u2502\n\u2502 dragon  \u2502 roar    \u2502 [0.5, 0.2, 0.7] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Warning</p> <p>DuckDB treats <code>column</code> as a keyword in some contexts. It's recommended to use <code>text_column</code> / <code>vector_column</code> as column names for the Lance extension.</p>"},{"location":"integrations/duckdb/#source-repo","title":"Source repo","text":"<p>Check out the lance-duckdb project for the latest source code, and go through <code>README.md</code> for the latest API docs. Additional pages are listed below.</p>"},{"location":"integrations/duckdb/#full-sql-reference","title":"Full SQL reference","text":"<p>sql.md lists the current SQL surface supported by this extension. It's recommended to refer to this page for the most up-to-date information.</p>"},{"location":"integrations/duckdb/#cloud-storage-reference","title":"Cloud storage reference","text":"<p>cloud.md lists the current supported backends that allow you to access data on various cloud providers.</p> <ul> <li>S3 / S3-compatible: <code>s3://...</code> (also accepts <code>s3a://...</code> and <code>s3n://...</code>, normalized to <code>s3://...</code>)</li> <li>Google Cloud Storage: <code>gs://...</code></li> <li>Azure Blob Storage: <code>az://...</code></li> <li>Alibaba Cloud OSS: <code>oss://...</code></li> <li>Hugging Face Hub (OpenDAL): <code>hf://...</code></li> </ul>"},{"location":"integrations/huggingface/","title":"HuggingFace Integration","text":"<p>The HuggingFace Hub has become the go to place for ML practitioners to find pre-trained models and useful datasets.</p> <p>HuggingFace datasets can be written directly into Lance format by using the <code>lance.write_dataset</code> method. You can write the entire dataset or a particular split. For example:</p> <pre><code>import datasets # pip install datasets\nimport lance\n\nlance.write_dataset(datasets.load_dataset(\n    \"poloclub/diffusiondb\", split=\"train[:10]\",\n), \"diffusiondb_train.lance\")\n</code></pre>"},{"location":"integrations/pytorch/","title":"PyTorch Integration","text":"<p>Machine learning users can use <code>lance.torch.data.LanceDataset</code>, a subclass of <code>torch.utils.data.IterableDataset</code>, that to use Lance data directly PyTorch training and inference loops.</p> <p>It starts with creating a ML dataset for training. With the HuggingFace integration, it takes just one line of Python to convert a HuggingFace dataset to a Lance dataset.</p> <pre><code>import datasets # pip install datasets\nimport lance\n\nhf_ds = datasets.load_dataset(\n    \"poloclub/diffusiondb\",\n    split=\"train\",\n    # name=\"2m_first_1k\",  # for a smaller subset of the dataset\n)\nlance.write_dataset(hf_ds, \"diffusiondb_train.lance\")\n</code></pre> <p>Then, you can use the Lance dataset in PyTorch training and inference loops.</p> <p>Note:</p> <ol> <li> <p>the PyTorch dataset automatically convert data into <code>torch.Tensor</code></p> </li> <li> <p>lance is not fork-safe. If you are using multiprocessing, use spawn instead. The safe dataloader uses the spawning method.</p> </li> </ol>"},{"location":"integrations/pytorch/#unsafe-dataloader","title":"UnSafe Dataloader","text":"<pre><code>import torch\nimport lance.torch.data\n\n# Load lance dataset into a PyTorch IterableDataset.\n# with only columns \"image\" and \"prompt\".\ndataset = lance.torch.data.LanceDataset(\n    \"diffusiondb_train.lance\",\n    columns=[\"image\", \"prompt\"],\n    batch_size=128,\n    batch_readahead=8,  # Control multi-threading reads.\n)\n\n# Create a PyTorch DataLoader\ndataloader = torch.utils.data.DataLoader(dataset)\n\n# Inference loop\nfor batch in dataloader:\n    inputs, targets = batch[\"prompt\"], batch[\"image\"]\n    outputs = model(inputs)\n    ...\n</code></pre>"},{"location":"integrations/pytorch/#safe-dataloader","title":"Safe Dataloader","text":"<pre><code>from lance.torch.data import SafeLanceDataset, get_safe_loader\n\ndataset = SafeLanceDataset(temp_lance_dataset)\n# use spawn method to avoid fork-safe issue\nloader = get_safe_loader(\n    dataset,\n    num_workers=2,\n    batch_size=16,\n    drop_last=False,\n)\n\ntotal_samples = 0\nfor batch in loader:\n    total_samples += batch[\"id\"].shape[0]\n</code></pre> <p><code>lance.torch.data.LanceDataset</code> can composite with the <code>lance.sampler.Sampler</code> classes to control the sampling strategy. For example, you can use <code>lance.sampler.ShardedFragmentSampler</code> to use it in a distributed training environment. If not specified, it is a full scan.</p> <pre><code>from lance.sampler import ShardedFragmentSampler\nfrom lance.torch.data import LanceDataset\n\n# Load lance dataset into a PyTorch IterableDataset.\n# with only columns \"image\" and \"prompt\".\ndataset = LanceDataset(\n    \"diffusiondb_train.lance\",\n    columns=[\"image\", \"prompt\"],\n    batch_size=128,\n    batch_readahead=8,  # Control multi-threading reads.\n    sampler=ShardedFragmentSampler(\n        rank=1,  # Rank of the current process\n        world_size=8,  # Total number of processes\n    ),\n)\n</code></pre> <p>Available samplers:</p> <ul> <li><code>lance.sampler.ShardedFragmentSampler</code></li> <li><code>lance.sampler.ShardedBatchSampler</code></li> </ul> <p>Warning</p> <p>For multiprocessing you should probably not use fork as lance is multi-threaded internally and fork and multi-thread do not work well. Refer to this discussion. </p>"},{"location":"integrations/tensorflow/","title":"Tensorflow Integration","text":"<p>Lance can be used as a regular tf.data.Dataset in Tensorflow.</p> <p>Warning</p> <p>This feature is experimental and the APIs may change in the future.</p>"},{"location":"integrations/tensorflow/#reading-from-lance","title":"Reading from Lance","text":"<p>Using <code>lance.tf.data.from_lance</code>, you can create an <code>tf.data.Dataset</code> easily.</p> <pre><code>import tensorflow as tf\nimport lance\n\n# Create tf dataset\nds = lance.tf.data.from_lance(\"s3://my-bucket/my-dataset\")\n\n# Chain tf dataset with other tf primitives\n\nfor batch in ds.shuffling(32).map(lambda x: tf.io.decode_png(x[\"image\"])):\n    print(batch)\n</code></pre> <p>Backed by the Lance columnar format, using <code>lance.tf.data.from_lance</code> supports efficient column selection, filtering, and more.</p> <pre><code>ds = lance.tf.data.from_lance(\n    \"s3://my-bucket/my-dataset\",\n    columns=[\"image\", \"label\"],\n    filter=\"split = 'train' AND collected_time &gt; timestamp '2020-01-01'\",\n    batch_size=256)\n</code></pre> <p>By default, Lance will infer the Tensor spec from the projected columns. You can also specify <code>tf.TensorSpec</code> manually.</p> <pre><code>batch_size = 256\nds = lance.tf.data.from_lance(\n    \"s3://my-bucket/my-dataset\",\n    columns=[\"image\", \"labels\"],\n    batch_size=batch_size,\n    output_signature={\n        \"image\": tf.TensorSpec(shape=(), dtype=tf.string),\n        \"labels\": tf.RaggedTensorSpec(\n            dtype=tf.int32, shape=(batch_size, None), ragged_rank=1),\n    },\n</code></pre>"},{"location":"integrations/tensorflow/#distributed-training-and-shuffling","title":"Distributed Training and Shuffling","text":"<p>Since a Lance Dataset is a set of Fragments, we can distribute and shuffle Fragments to different workers.</p> <pre><code>import tensorflow as tf\nfrom lance.tf.data import from_lance, lance_fragments\n\nworld_size = 32\nrank = 10\nseed = 123  #\nepoch = 100\n\ndataset_uri = \"s3://my-bucket/my-dataset\"\n\n# Shuffle fragments distributedly.\nfragments =\n    lance_fragments(\"s3://my-bucket/my-dataset\")\n    .shuffling(32, seed=seed)\n    .repeat(epoch)\n    .enumerate()\n    .filter(lambda i, _: i % world_size == rank)\n    .map(lambda _, fid: fid)\n\nds = from_lance(\n    uri,\n    columns=[\"image\", \"label\"],\n    fragments=fragments,\n    batch_size=32\n    )\nfor batch in ds:\n    print(batch)\n</code></pre> <p>Warning</p> <p>For multiprocessing you should probably not use fork as lance is multi-threaded internally and fork and multi-thread do not work well. Refer to this discussion. </p>"},{"location":"integrations/ray/","title":"Lance-Ray Integration","text":"<p>Welcome to the Lance-Ray documentation!  Lance-Ray combines the distributed computing capabilities of Ray  with the efficient Lance storage format,  enabling scalable data processing workflows with optimal performance.</p>"},{"location":"integrations/ray/#features","title":"Features","text":"<ul> <li>Distributed Lance Operations: Leverage Ray's distributed computing for Lance dataset operations</li> <li>Seamless Data Conversion: Easy conversion between Ray datasets and Lance datasets</li> <li>Optimized I/O: Efficient reading and writing of Lance datasets with Ray integration</li> <li>Schema Validation: Automatic schema compatibility checking between Ray and Lance</li> <li>Flexible Filtering: Support for complex filtering pushdown on distributed Lance data</li> <li>Data Evolution: Support for data evolution to add new columns and distributedly backfill data using a Ray UDF</li> <li>Catalog Integration: Support for working with Lance datasets stored in various catalog services (e.g. Hive MetaStore, Iceberg REST Catalog, Unity, Gravitino, AWS Glue, etc.)</li> </ul>"},{"location":"integrations/ray/#quickstart","title":"Quickstart","text":""},{"location":"integrations/ray/#installation","title":"Installation","text":"<pre><code>pip install lance-ray\n</code></pre> <p>To install a prerelease version:</p> <pre><code>pip install lance-ray==0.2.0b1 --extra-index-url https://pypi.fury.io/lance-format/\n</code></pre>"},{"location":"integrations/ray/#simple-example","title":"Simple Example","text":"<pre><code>import ray\nfrom lance_ray import read_lance, write_lance\n\n# Initialize Ray\nray.init()\n\n# Create a Ray dataset\ndata = ray.data.range(1000).map(lambda row: {\"id\": row[\"id\"], \"value\": row[\"id\"] * 2})\n\n# Write to Lance format\nwrite_lance(data, \"my_dataset.lance\")\n\n# Read Lance dataset back as Ray dataset\nray_dataset = read_lance(\"my_dataset.lance\")\n\n# Perform distributed operations\nresult = ray_dataset.filter(lambda row: row[\"value\"] &gt; 100).count()\nprint(f\"Filtered count: {result}\")\n</code></pre>"},{"location":"integrations/ray/data-evolution/","title":"Data Evolution","text":""},{"location":"integrations/ray/data-evolution/#add_columns","title":"<code>add_columns</code>","text":"<pre><code>add_columns(\n    uri=None, \n    *, \n    namespace=None, \n    table_id=None, \n    transform, \n    **kwargs)\n</code></pre> <p>Add columns to an existing Lance dataset using Ray's distributed processing.</p> <p>Parameters:</p> <ul> <li><code>uri</code>: Path to the Lance dataset (either uri OR namespace+table_id required)</li> <li><code>namespace</code>: LanceNamespace instance for metadata catalog integration (requires table_id)</li> <li><code>table_id</code>: Table identifier as list of strings (requires namespace)</li> <li><code>transform</code>: Transform function to apply for adding columns</li> <li><code>filter</code>: Optional filter expression to apply</li> <li><code>read_columns</code>: Optional list of columns to read from original dataset</li> <li><code>reader_schema</code>: Optional schema for the reader</li> <li><code>read_version</code>: Optional version to read</li> <li><code>ray_remote_args</code>: Optional kwargs for Ray remote tasks</li> <li><code>storage_options</code>: Optional storage configuration dictionary</li> <li><code>batch_size</code>: Batch size for processing (default: 1024)</li> <li><code>concurrency</code>: Optional number of concurrent processes</li> </ul> <p>Returns: None</p>"},{"location":"integrations/ray/distributed-indexing/","title":"Distributed Index Building","text":"<p>Lance-Ray provides distributed index building functionality that leverages Ray's distributed computing capabilities to efficiently create text indices for Lance datasets. This is particularly useful for large-scale datasets as it can distribute index building work across multiple Ray worker nodes.</p>"},{"location":"integrations/ray/distributed-indexing/#distributed-apis","title":"Distributed APIs","text":""},{"location":"integrations/ray/distributed-indexing/#scalar-indexing","title":"Scalar Indexing","text":"<p><code>create_scalar_index()</code> - Distributedly create scalar index using ray. Currently only Inverted/FTS/BTREE are supported. Will add more index type support in the future.</p>"},{"location":"integrations/ray/distributed-indexing/#how-it-works","title":"How It Works","text":"<p>The <code>create_scalar_index</code> function allows you to create full-text search indices for Lance datasets using the Ray distributed computing framework. This function distributes the index building process across multiple Ray worker nodes, with each node responsible for building indices for a subset of dataset fragments. These indices are then merged and committed as a single index.</p> <p>Backward Compatibility:    - Automatically detect availability of new APIs across different Lance versions    - Gracefully fallback to raise tips when new APIs are unavailable</p> <p><code>create_scalar_index</code></p> <pre><code>def create_scalar_index(\n    uri: Optional[str] = None,\n    *,\n    column: str,\n    index_type: Union[\n        Literal[\"BTREE\"],\n        Literal[\"BITMAP\"],\n        Literal[\"LABEL_LIST\"],\n        Literal[\"INVERTED\"],\n        Literal[\"FTS\"],\n        Literal[\"NGRAM\"],\n        Literal[\"ZONEMAP\"],\n        IndexConfig,\n    ],\n    table_id: Optional[list[str]] = None,\n    name: Optional[str] = None,\n    replace: bool = True,\n    train: bool = True,\n    fragment_ids: Optional[list[int]] = None,\n    index_uuid: Optional[str] = None,\n    num_workers: int = 4,\n    storage_options: Optional[dict[str, str]] = None,\n    namespace_impl: Optional[str] = None,\n    namespace_properties: Optional[dict[str, str]] = None,\n    ray_remote_args: Optional[dict[str, Any]] = None,\n    **kwargs: Any,\n) -&gt; \"lance.LanceDataset\":\n</code></pre>"},{"location":"integrations/ray/distributed-indexing/#parameters","title":"Parameters","text":"Parameter Type Description <code>uri</code> <code>str</code>, optional The URI of the Lance dataset. Either <code>uri</code> OR (<code>namespace_impl</code> + <code>table_id</code>) must be provided. <code>column</code> <code>str</code> Column name to index <code>index_type</code> <code>str</code> or <code>IndexConfig</code> Index type, can be <code>\"INVERTED\"</code>, <code>\"FTS\"</code>, <code>\"BTREE\"</code>, <code>\"BITMAP\"</code>, <code>\"LABEL_LIST\"</code>, <code>\"NGRAM\"</code>, <code>\"ZONEMAP\"</code>, or <code>IndexConfig</code> object <code>table_id</code> <code>list[str]</code>, optional The table identifier as a list of strings. <code>name</code> <code>str</code>, optional Index name, auto-generated if not provided <code>replace</code> <code>bool</code>, optional Whether to replace existing index with the same name, default is <code>True</code> <code>train</code> <code>bool</code>, optional Whether to train the index, default is <code>True</code> <code>fragment_ids</code> <code>list[int]</code>, optional Optional list of fragment IDs to build index on <code>index_uuid</code> <code>str</code>, optional Optional fragment UUID for distributed indexing <code>num_workers</code> <code>int</code>, optional Number of Ray worker nodes to use, default is 4 <code>storage_options</code> <code>Dict[str, str]</code>, optional Storage options for the dataset <code>namespace_impl</code> <code>str</code>, optional The namespace implementation type (e.g., <code>\"rest\"</code>, <code>\"dir\"</code>) <code>namespace_properties</code> <code>Dict[str, str]</code>, optional Properties for connecting to the namespace <code>ray_remote_args</code> <code>Dict[str, Any]</code>, optional Ray task options (e.g., <code>num_cpus</code>, <code>resources</code>) <code>**kwargs</code> <code>Any</code> Additional arguments passed to <code>create_scalar_index</code> <p>Note: For distributed scalar indexing, currently only <code>\"INVERTED\"</code>, <code>\"FTS\"</code> and <code>\"BTREE\"</code> index types are supported.</p>"},{"location":"integrations/ray/distributed-indexing/#return-value","title":"Return Value","text":"<p>The function returns an updated Lance dataset with the newly created index.</p>"},{"location":"integrations/ray/distributed-indexing/#vector-indexing","title":"Vector Indexing","text":"<p><code>create_index()</code> - Distributedly create vector indices using Ray. It leverages Ray to parallelize the index building process across multiple workers.</p>"},{"location":"integrations/ray/distributed-indexing/#supported-index-types","title":"Supported Index Types","text":"<p>The following vector index types are supported for distributed building: - <code>IVF_FLAT</code> - <code>IVF_SQ</code> - <code>IVF_PQ</code></p>"},{"location":"integrations/ray/distributed-indexing/#create_index","title":"<code>create_index</code>","text":"<pre><code>def create_index(\n    uri: Union[str, \"lance.LanceDataset\"],\n    column: str,\n    index_type: str,\n    name: Optional[str] = None,\n    *,\n    replace: bool = True,\n    num_workers: int = 4,\n    storage_options: Optional[dict[str, str]] = None,\n    ray_remote_args: Optional[dict[str, Any]] = None,\n    metric: str = \"l2\",\n    num_partitions: Optional[int] = None,\n    num_sub_vectors: Optional[int] = None,\n    **kwargs: Any,\n) -&gt; \"lance.LanceDataset\":\n</code></pre>"},{"location":"integrations/ray/distributed-indexing/#parameters_1","title":"Parameters","text":"Parameter Type Description <code>uri</code> <code>str</code> or <code>lance.LanceDataset</code> Lance dataset or its URI <code>column</code> <code>str</code> Vector column name to index <code>index_type</code> <code>str</code> Vector index type (e.g., <code>\"IVF_PQ\"</code>, <code>\"IVF_SQ\"</code>, <code>\"IVF_FLAT\"</code>) <code>name</code> <code>str</code>, optional Index name, auto-generated if not provided <code>replace</code> <code>bool</code>, optional Whether to replace existing index, default is <code>True</code> <code>num_workers</code> <code>int</code>, optional Number of Ray workers to use, default is 4 <code>storage_options</code> <code>Dict[str, str]</code>, optional Storage options for the dataset <code>ray_remote_args</code> <code>Dict[str, Any]</code>, optional Ray task options (e.g., <code>num_cpus</code>, <code>resources</code>) <code>metric</code> <code>str</code>, optional Distance metric to use (e.g., <code>\"l2\"</code>, <code>\"cosine\"</code>, <code>\"dot\"</code>, <code>\"hamming\"</code>), default is <code>\"l2\"</code> <code>num_partitions</code> <code>int</code>, optional Number of IVF partitions <code>num_sub_vectors</code> <code>int</code>, optional Number of PQ sub-vectors <code>**kwargs</code> <code>Any</code> Additional arguments to pass (e.g., <code>sample_rate</code>)"},{"location":"integrations/ray/distributed-indexing/#return-value_1","title":"Return Value","text":"<p>The function returns an updated Lance dataset with the newly created vector index.</p>"},{"location":"integrations/ray/distributed-indexing/#examples","title":"Examples","text":""},{"location":"integrations/ray/distributed-indexing/#fts-index-scalar","title":"FTS Index (Scalar)","text":"<pre><code>import lance\nimport lance_ray as lr\n\n# Create or load Lance dataset\ndataset = lance.dataset(\"path/to/dataset\")\n\n# Build distributed index\nupdated_dataset = lr.create_scalar_index(\n   uri=dataset.uri,\n   column=\"text\",\n   index_type=\"INVERTED\",\n   num_workers=4\n)\n\n# Verify index creation\nindices = updated_dataset.list_indices()\nprint(f\"Index list: {indices}\")\n\n# Use index for search\nresults = updated_dataset.scanner(\n   full_text_query=\"search term\",\n   columns=[\"id\", \"text\"]\n).to_table()\nprint(f\"Search results: {results}\")\n</code></pre>"},{"location":"integrations/ray/distributed-indexing/#btree-index-scalar","title":"BTREE Index (Scalar)","text":"<pre><code># Assume a LanceDataset with a numeric column \"id\" exists at this path\nimport lance_ray as lr\n\nupdated_dataset = lr.create_scalar_index(\n    uri=\"path/to/dataset\",\n    column=\"id\",\n    index_type=\"BTREE\",\n    name=\"btree_multiple_fragment_idx\",\n    replace=False,\n    num_workers=4,\n)\n\n# Example queries\nupdated_dataset.scanner(filter=\"id = 100\", columns=[\"id\", \"text\"]).to_table()\nupdated_dataset.scanner(filter=\"id &gt;= 200 AND id &lt; 800\", columns=[\"id\", \"text\"]).to_table()\n</code></pre>"},{"location":"integrations/ray/distributed-indexing/#vector-index-ivf_pq-ivf_sq-ivf_flat","title":"Vector Index (IVF_PQ / IVF_SQ / IVF_FLAT)","text":"<pre><code>import lance_ray as lr\n\n# Build a distributed IVF_PQ index\nupdated_dataset = lr.create_index(\n    uri=\"path/to/dataset.lance\",\n    column=\"vector\",\n    index_type=\"IVF_PQ\",\n    name=\"idx_ivf_pq\",\n    num_workers=4,\n    num_partitions=256,\n    num_sub_vectors=16,\n    metric=\"l2\"\n)\n\n# Build a distributed IVF_SQ index\nupdated_dataset = lr.create_index(\n    uri=\"path/to/dataset.lance\",\n    column=\"vector\",\n    index_type=\"IVF_SQ\",\n    name=\"idx_ivf_sq\",\n    num_workers=4,\n    num_partitions=256,\n)\n\n# Build a distributed IVF_FLAT index\nupdated_dataset = lr.create_index(\n    uri=\"path/to/dataset.lance\",\n    column=\"vector\",\n    index_type=\"IVF_FLAT\",\n    name=\"idx_ivf_flat\",\n    num_workers=4,\n    num_partitions=256,\n)\n</code></pre>"},{"location":"integrations/ray/distributed-indexing/#custom-ray-options","title":"Custom Ray Options","text":"<pre><code>updated_dataset = lr.create_scalar_index(\n   uri=\"path/to/dataset\",\n   column=\"text\",\n   index_type=\"INVERTED\",\n   num_workers=4,\n   ray_remote_args={\"num_cpus\": 2, \"resources\": {\"custom_resource\": 1}}\n)\n</code></pre>"},{"location":"integrations/ray/distributed-indexing/#index-replacement-control","title":"Index Replacement Control","text":"<pre><code># Create index with custom name\nupdated_dataset = lr.create_scalar_index(\n   uri=\"path/to/dataset\",\n   column=\"text\",\n   index_type=\"INVERTED\",\n   name=\"my_text_index\",\n   num_workers=4\n)\n\n# Try to create another index with the same name (will replace by default)\nupdated_dataset = lr.create_scalar_index(\n   uri=\"path/to/dataset\",\n   column=\"text\",\n   index_type=\"INVERTED\",\n   name=\"my_text_index\",  # Same name as before\n   replace=True,          # Explicitly allow replacement (default behavior)\n   num_workers=4\n)\n\n# Prevent index replacement\nimport lance_ray as lr\n\ntry:\n    updated_dataset = lr.create_scalar_index(\n       uri=\"path/to/dataset\",\n       column=\"text\",\n       index_type=\"INVERTED\",\n       name=\"my_text_index\",  # Same name as existing index\n       replace=False,         # Prevent replacement\n       num_workers=4\n    )\nexcept ValueError as e:\n    print(f\"Index creation failed: {e}\")\n    # Handle the error appropriately\n</code></pre>"},{"location":"integrations/ray/distributed-indexing/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>For very large datasets, it's recommended to use more powerful CPU/memory ray worker nodes. Increasing <code>num_workers</code> can improve index building speed, but requires more computational nodes.</li> <li>Too many num_workers can cause large number of partitions, which cause FTS queries slowness as lots of index partitions need to be loaded when searching.</li> <li>If <code>num_workers</code> is greater than the number of fragments, it will be automatically adjusted to match the fragment count</li> </ul>"},{"location":"integrations/ray/distributed-indexing/#important-notes","title":"Important Notes","text":"<ul> <li>Index Type Support: For distributed indexing, currently only <code>\"INVERTED\"</code>/<code>\"FTS\"</code>/<code>\"BTREE\"</code> index types are supported, even though the function signature accepts other index types.</li> <li>Default Behavior: The <code>replace</code> parameter defaults to <code>True</code>, meaning existing indices with the same name will be replaced without warning. Set <code>replace=False</code> to prevent accidental overwrites.</li> <li>Fragment Selection: Use <code>fragment_ids</code> parameter to build indices on specific fragments only. This is useful for incremental index building or testing.</li> <li>Error Handling: When <code>replace=False</code> and an index with the same name exists, a <code>ValueError</code> or <code>RuntimeError</code> will be raised depending on the execution context.</li> </ul>"},{"location":"integrations/ray/examples/","title":"Examples","text":"<p>Here are some examples to try out. See the <code>examples/</code> directory for more comprehensive usage examples.</p>"},{"location":"integrations/ray/examples/#basic-read-write","title":"Basic Read &amp; Write","text":"<pre><code>import pandas as pd\nimport ray\nfrom lance_ray import read_lance, write_lance\n\n# Initialize Ray\nray.init()\n\n# Create sample data\nsample_data = {\n    \"user_id\": range(100),\n    \"name\": [f\"User_{i}\" for i in range(100)],\n    \"age\": [20 + (i % 50) for i in range(100)],\n    \"score\": [50.0 + (i % 100) * 0.5 for i in range(100)],\n}\ndf = pd.DataFrame(sample_data)\n\n# Create Ray dataset\nds = ray.data.from_pandas(df)\n\n# Write to Lance format\nwrite_lance(ds, \"sample_dataset.lance\")\n\n# Read Lance dataset back\nds = read_lance(\"sample_dataset.lance\")\n\n# Perform distributed operations\nfiltered_ds = ds.filter(lambda row: row[\"age\"] &gt; 30)\nprint(f\"Filtered count: {filtered_ds.count()}\")\n\n# Read with column selection and filtering\nds_filtered = read_lance(\n    \"sample_dataset.lance\",\n    columns=[\"user_id\", \"name\", \"score\"],\n    filter=\"score &gt; 75.0\"\n)\nprint(f\"Schema: {ds_filtered.schema()}\")\n</code></pre>"},{"location":"integrations/ray/examples/#data-evolution","title":"Data Evolution","text":"<pre><code># Add columns using metadata catalog\nfrom lance_ray import add_columns\nimport pyarrow as pa\n\ndef add_computed_column(batch: pa.RecordBatch) -&gt; pa.RecordBatch:\n    df = batch.to_pandas()\n    df['computed'] = df['value'] * 2 + df['id']\n    return pa.RecordBatch.from_pandas(df[[\"computed\"]])\n\nadd_columns(\n    uri=\"sample_dataset.lance\",\n    transform=add_computed_column,\n    concurrency=4\n)\n</code></pre>"},{"location":"integrations/ray/examples/#using-namespace","title":"Using Namespace","text":"<p>For enterprise environments with metadata catalogs, you can use Lance Namespace integration:</p> <pre><code>import ray\nimport lance_namespace as ln\nfrom lance_ray import read_lance, write_lance\n\n# Initialize Ray\nray.init()\n\n# Connect to a metadata catalog (directory-based example)\nnamespace = ln.connect(\"dir\", {\"root\": \"/path/to/tables\"})\n\n# Create a Ray dataset\ndata = ray.data.range(1000).map(lambda row: {\"id\": row[\"id\"], \"value\": row[\"id\"] * 2})\n\n# Write to Lance format using metadata catalog\nwrite_lance(data, namespace=namespace, table_id=[\"my_table\"])\n\n# Read Lance dataset back using metadata catalog\nray_dataset = read_lance(namespace=namespace, table_id=[\"my_table\"])\n\n# Perform distributed operations\nresult = ray_dataset.filter(lambda row: row[\"value\"] &gt; 100).count()\nprint(f\"Filtered count: {result}\")\n</code></pre> <p>The package dependency comes with the directory and REST namespace implementations to use by default. To use another implementation, install the specific extra dependency.  For example to use it with AWS Glue catalog:</p> <pre><code>pip install lance-namespace[glue]\n</code></pre> <p>And then you can do:</p> <pre><code>import ray\nimport lance_namespace as ln\nfrom lance_ray import read_lance, write_lance\n\n# Initialize Ray\nray.init()\n\n# Connect to AWS Glue catalog \n# using the default account and region in the current AWS environment\nnamespace = ln.connect(\"glue\", {})\n\n# Create a Ray dataset\ndata = ray.data.range(1000).map(lambda row: {\"id\": row[\"id\"], \"value\": row[\"id\"] * 2})\n\n# Write to Lance format using metadata catalog\nwrite_lance(\n    data, \n    uri=\"s3://my-bucket/my-table\", \n    namespace=namespace, \n    table_id=[\"default\", \"my_table\"]\n)\n\n# Read Lance dataset back using metadata catalog\nray_dataset = read_lance(namespace=namespace, table_id=[\"default\", \"my_table\"])\n\n# Perform distributed operations\nresult = ray_dataset.filter(lambda row: row[\"value\"] &gt; 100).count()\nprint(f\"Filtered count: {result}\")\n</code></pre>"},{"location":"integrations/ray/read/","title":"Reading Lance Datasets","text":""},{"location":"integrations/ray/read/#read_lance","title":"<code>read_lance</code>","text":"<pre><code>read_lance(\n    uri=None, \n    *, \n    namespace=None, \n    table_id=None, \n    columns=None, \n    filter=None, \n    storage_options=None, \n    **kwargs)\n</code></pre> <p>Read a Lance dataset and return a Ray Dataset.</p> <p>Parameters:</p> <ul> <li><code>uri</code>: The URI of the Lance dataset to read from (either uri OR namespace+table_id required)</li> <li><code>namespace</code>: LanceNamespace instance for metadata catalog integration (requires table_id)</li> <li><code>table_id</code>: Table identifier as list of strings (requires namespace)</li> <li><code>columns</code>: Optional list of column names to read</li> <li><code>filter</code>: Optional filter expression to apply</li> <li><code>storage_options</code>: Optional storage configuration dictionary</li> <li><code>scanner_options</code>: Optional scanner configuration dictionary</li> <li><code>ray_remote_args</code>: Optional kwargs for Ray remote tasks</li> <li><code>concurrency</code>: Optional maximum number of concurrent Ray tasks</li> <li><code>override_num_blocks</code>: Optional override for number of output blocks</li> </ul> <p>Returns: Ray Dataset</p>"},{"location":"integrations/ray/write/","title":"Writing to Lance Dataset","text":""},{"location":"integrations/ray/write/#write_lance","title":"<code>write_lance</code>","text":"<pre><code>write_lance(\n    ds, \n    uri=None, \n    *, \n    namespace=None, \n    table_id=None, \n    schema=None, \n    mode=\"create\", \n    **kwargs)\n</code></pre> <p>Write a Ray Dataset to Lance format.</p> <p>Parameters:</p> <ul> <li><code>ds</code>: Ray Dataset to write</li> <li><code>uri</code>: Path to the destination Lance dataset (either uri OR namespace+table_id required)</li> <li><code>namespace</code>: LanceNamespace instance for metadata catalog integration (requires table_id)</li> <li><code>table_id</code>: Table identifier as list of strings (requires namespace)</li> <li><code>schema</code>: Optional PyArrow schema</li> <li><code>mode</code>: Write mode - \"create\", \"append\", or \"overwrite\"</li> <li><code>min_rows_per_file</code>: Minimum rows per file (default: 1024 * 1024)</li> <li><code>max_rows_per_file</code>: Maximum rows per file (default: 64 * 1024 * 1024)</li> <li><code>data_storage_version</code>: Optional data storage version</li> <li><code>storage_options</code>: Optional storage configuration dictionary</li> <li><code>ray_remote_args</code>: Optional kwargs for Ray remote tasks</li> <li><code>concurrency</code>: Optional maximum number of concurrent Ray tasks</li> </ul> <p>Returns: None</p>"},{"location":"integrations/spark/","title":"Spark Lance Connector","text":""},{"location":"integrations/spark/#introduction","title":"Introduction","text":"<p>The Apache Spark Connector for Lance allows Apache Spark to efficiently read datasets stored in Lance format.</p> <p>By using the Apache Spark Connector for Lance, you can leverage Apache Spark's powerful data processing, SQL querying, and machine learning training capabilities on the AI data lake powered by Lance.</p>"},{"location":"integrations/spark/#features","title":"Features","text":"<p>The connector is built using the Spark DatasourceV2 (DSv2) API. Please check this presentation to learn more about DSv2 features. Specifically, you can use the Apache Spark Connector for Lance to:</p> <ul> <li>Read &amp; Write Lance Datasets: Seamlessly read and write datasets stored in the Lance format using Spark.</li> <li>Distributed, Parallel Scans: Leverage Spark's distributed computing capabilities to perform parallel scans on Lance datasets.</li> <li>Column and Filter Pushdown: Optimize query performance by pushing down column selections and filters to the data source.</li> </ul>"},{"location":"integrations/spark/#quick-start","title":"Quick Start","text":"<p>The project contains a docker image in the <code>docker</code> folder you can build and run a simple example notebook. To do so, clone the repo and run:</p> <pre><code>make docker-build\nmake docker-up\n</code></pre> <p>And then open the notebook at <code>http://localhost:8888</code>.</p>"},{"location":"integrations/spark/config/","title":"Configuration","text":"<p>Spark DSV2 catalog integrates with Lance through Lance Namespace.</p>"},{"location":"integrations/spark/config/#spark-sql-extensions","title":"Spark SQL Extensions","text":"<p>Lance provides SQL extensions that add additional functionality beyond standard Spark SQL. To enable these extensions, configure your Spark application with:</p> PySparkScalaJavaSpark ShellSpark Submit <pre><code>spark = SparkSession.builder \\\n    .appName(\"lance-example\") \\\n    .config(\"spark.sql.extensions\", \"org.lance.spark.extensions.LanceSparkSessionExtensions\") \\\n    .getOrCreate()\n</code></pre> <pre><code>val spark = SparkSession.builder()\n    .appName(\"lance-example\")\n    .config(\"spark.sql.extensions\", \"org.lance.spark.extensions.LanceSparkSessionExtensions\")\n    .getOrCreate()\n</code></pre> <pre><code>SparkSession spark = SparkSession.builder()\n    .appName(\"lance-example\")\n    .config(\"spark.sql.extensions\", \"org.lance.spark.extensions.LanceSparkSessionExtensions\")\n    .getOrCreate();\n</code></pre> <pre><code>spark-shell \\\n  --packages org.lance:lance-spark-bundle-3.5_2.12:0.0.7 \\\n  --conf spark.sql.extensions=org.lance.spark.extensions.LanceSparkSessionExtensions\n</code></pre> <pre><code>spark-submit \\\n  --packages org.lance:lance-spark-bundle-3.5_2.12:0.0.7 \\\n  --conf spark.sql.extensions=org.lance.spark.extensions.LanceSparkSessionExtensions \\\n  your-application.jar\n</code></pre>"},{"location":"integrations/spark/config/#features-requiring-extensions","title":"Features Requiring Extensions","text":"<p>The following features require the Lance Spark SQL extension to be enabled:</p> <ul> <li>ADD COLUMN with backfill - Add new columns and backfill existing rows with data</li> <li>OPTIMIZE - Compact table fragments for improved query performance</li> <li>VACUUM - Remove old versions and reclaim storage space</li> </ul>"},{"location":"integrations/spark/config/#basic-setup","title":"Basic Setup","text":"<p>Configure Spark with the <code>LanceNamespaceSparkCatalog</code> by setting the appropriate Spark catalog implementation and namespace-specific options:</p> Parameter Type Required Description <code>spark.sql.catalog.{name}</code> String \u2713 Set to <code>org.lance.spark.LanceNamespaceSparkCatalog</code> <code>spark.sql.catalog.{name}.impl</code> String \u2713 Namespace implementation, short name like <code>dir</code>, <code>rest</code>, <code>hive3</code>, <code>glue</code> or full Java implementation class <code>spark.sql.catalog.{name}.storage.*</code> - \u2717 Lance IO storage options. See Lance Object Store Guide for all available options. <code>spark.sql.catalog.{name}.single_level_ns</code> String \u2717 Virtual level for 2-level namespaces. See Note on Namespace Levels. <code>spark.sql.catalog.{name}.parent</code> String \u2717 Parent prefix for multi-level namespaces. See Note on Namespace Levels. <code>spark.sql.catalog.{name}.parent_delimiter</code> String \u2717 Delimiter for parent prefix (default: <code>$</code>). See Note on Namespace Levels."},{"location":"integrations/spark/config/#arrow-allocator","title":"Arrow Allocator","text":"<p>The Arrow buffer allocator size can be configured via environment variable:</p> Environment Variable Default Description <code>LANCE_ALLOCATOR_SIZE</code> <code>Long.MAX_VALUE</code> Arrow allocator size in bytes (global)."},{"location":"integrations/spark/config/#example-namespace-implementations","title":"Example Namespace Implementations","text":""},{"location":"integrations/spark/config/#directory-namespace","title":"Directory Namespace","text":"ScalaJavaSpark ShellSpark Submit <pre><code>import org.apache.spark.sql.SparkSession\n\nval spark = SparkSession.builder()\n    .appName(\"lance-dir-example\")\n    .config(\"spark.sql.catalog.lance\", \"org.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"dir\")\n    .config(\"spark.sql.catalog.lance.root\", \"/path/to/lance/database\")\n    .getOrCreate()\n</code></pre> <pre><code>import org.apache.spark.sql.SparkSession;\n\nSparkSession spark = SparkSession.builder()\n    .appName(\"lance-dir-example\")\n    .config(\"spark.sql.catalog.lance\", \"org.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"dir\")\n    .config(\"spark.sql.catalog.lance.root\", \"/path/to/lance/database\")\n    .getOrCreate();\n</code></pre> <pre><code>spark-shell \\\n  --packages org.lance:lance-spark-bundle-3.5_2.12:0.0.7 \\\n  --conf spark.sql.catalog.lance=org.lance.spark.LanceNamespaceSparkCatalog \\\n  --conf spark.sql.catalog.lance.impl=dir \\\n  --conf spark.sql.catalog.lance.root=/path/to/lance/database\n</code></pre> <pre><code>spark-submit \\\n  --packages org.lance:lance-spark-bundle-3.5_2.12:0.0.7 \\\n  --conf spark.sql.catalog.lance=org.lance.spark.LanceNamespaceSparkCatalog \\\n  --conf spark.sql.catalog.lance.impl=dir \\\n  --conf spark.sql.catalog.lance.root=/path/to/lance/database \\\n  your-application.jar\n</code></pre>"},{"location":"integrations/spark/config/#directory-configuration-parameters","title":"Directory Configuration Parameters","text":"Parameter Required Description <code>spark.sql.catalog.{name}.root</code> \u2717 Storage root location (default: current directory) <p>Example settings:</p> Local StorageS3MinIO <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"lance-dir-local-example\") \\\n    .config(\"spark.sql.catalog.lance\", \"org.lance.spark.LanceNamespaceSparkCatalog\") \\\n    .config(\"spark.sql.catalog.lance.impl\", \"dir\") \\\n    .config(\"spark.sql.catalog.lance.root\", \"/path/to/lance/database\") \\\n    .getOrCreate()\n</code></pre> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"lance-dir-minio-example\") \\\n    .config(\"spark.sql.catalog.lance\", \"org.lance.spark.LanceNamespaceSparkCatalog\") \\\n    .config(\"spark.sql.catalog.lance.impl\", \"dir\") \\\n    .config(\"spark.sql.catalog.lance.root\", \"s3://bucket-name/lance-data\") \\\n    .config(\"spark.sql.catalog.lance.storage.access_key_id\", \"abc\") \\\n    .config(\"spark.sql.catalog.lance.storage.secret_access_key\", \"def\")\n    .config(\"spark.sql.catalog.lance.storage.session_token\", \"ghi\") \\\n    .getOrCreate()\n</code></pre> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"lance-dir-minio-example\") \\\n    .config(\"spark.sql.catalog.lance\", \"org.lance.spark.LanceNamespaceSparkCatalog\") \\\n    .config(\"spark.sql.catalog.lance.impl\", \"dir\") \\\n    .config(\"spark.sql.catalog.lance.root\", \"s3://bucket-name/lance-data\") \\\n    .config(\"spark.sql.catalog.lance.storage.endpoint\", \"http://minio:9000\") \\\n    .config(\"spark.sql.catalog.lance.storage.aws_allow_http\", \"true\") \\\n    .config(\"spark.sql.catalog.lance.storage.access_key_id\", \"admin\") \\\n    .config(\"spark.sql.catalog.lance.storage.secret_access_key\", \"password\") \\\n    .getOrCreate()\n</code></pre>"},{"location":"integrations/spark/config/#rest-namespace","title":"REST Namespace","text":"<p>Here we use LanceDB Cloud as an example of the REST namespace:</p> PySparkScalaJavaSpark ShellSpark Submit <pre><code>spark = SparkSession.builder \\\n    .appName(\"lance-rest-example\") \\\n    .config(\"spark.sql.catalog.lance\", \"org.lance.spark.LanceNamespaceSparkCatalog\") \\\n    .config(\"spark.sql.catalog.lance.impl\", \"rest\") \\\n    .config(\"spark.sql.catalog.lance.headers.x-api-key\", \"your-api-key\") \\\n    .config(\"spark.sql.catalog.lance.headers.x-lancedb-database\", \"your-database\") \\\n    .config(\"spark.sql.catalog.lance.uri\", \"https://your-database.us-east-1.api.lancedb.com\") \\\n    .getOrCreate()\n</code></pre> <pre><code>val spark = SparkSession.builder()\n    .appName(\"lance-rest-example\")\n    .config(\"spark.sql.catalog.lance\", \"org.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"rest\")\n    .config(\"spark.sql.catalog.lance.headers.x-api-key\", \"your-api-key\")\n    .config(\"spark.sql.catalog.lance.headers.x-lancedb-database\", \"your-database\")\n    .config(\"spark.sql.catalog.lance.uri\", \"https://your-database.us-east-1.api.lancedb.com\")\n    .getOrCreate()\n</code></pre> <pre><code>SparkSession spark = SparkSession.builder()\n    .appName(\"lance-rest-example\")\n    .config(\"spark.sql.catalog.lance\", \"org.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"rest\")\n    .config(\"spark.sql.catalog.lance.headers.x-api-key\", \"your-api-key\")\n    .config(\"spark.sql.catalog.lance.headers.x-lancedb-database\", \"your-database\")\n    .config(\"spark.sql.catalog.lance.uri\", \"https://your-database.us-east-1.api.lancedb.com\")\n    .getOrCreate();\n</code></pre> <pre><code>spark-shell \\\n  --packages org.lance:lance-spark-bundle-3.5_2.12:0.0.7 \\\n  --conf spark.sql.catalog.lance=org.lance.spark.LanceNamespaceSparkCatalog \\\n  --conf spark.sql.catalog.lance.impl=rest \\\n  --conf spark.sql.catalog.lance.headers.x-api-key=your-api-key \\\n  --conf spark.sql.catalog.lance.headers.x-lancedb-database=your-database \\\n  --conf spark.sql.catalog.lance.uri=https://your-database.us-east-1.api.lancedb.com\n</code></pre> <pre><code>spark-submit \\\n  --packages org.lance:lance-spark-bundle-3.5_2.12:0.0.7 \\\n  --conf spark.sql.catalog.lance=org.lance.spark.LanceNamespaceSparkCatalog \\\n  --conf spark.sql.catalog.lance.impl=rest \\\n  --conf spark.sql.catalog.lance.headers.x-api-key=your-api-key \\\n  --conf spark.sql.catalog.lance.headers.x-lancedb-database=your-database \\\n  --conf spark.sql.catalog.lance.uri=https://your-database.us-east-1.api.lancedb.com \\\n  your-application.jar\n</code></pre>"},{"location":"integrations/spark/config/#rest-configuration-parameters","title":"REST Configuration Parameters","text":"Parameter Required Description <code>spark.sql.catalog.{name}.uri</code> \u2713 REST API endpoint URL (e.g., <code>https://api.lancedb.com</code>) <code>spark.sql.catalog.{name}.headers.*</code> \u2717 HTTP headers for authentication (e.g., <code>headers.x-api-key</code>)"},{"location":"integrations/spark/config/#aws-glue-namespace","title":"AWS Glue Namespace","text":"<p>AWS Glue is Amazon's managed metastore service that provides a centralized catalog for your data assets.</p> PySparkScalaJava <pre><code>spark = SparkSession.builder \\\n    .appName(\"lance-glue-example\") \\\n    .config(\"spark.sql.catalog.lance\", \"org.lance.spark.LanceNamespaceSparkCatalog\") \\\n    .config(\"spark.sql.catalog.lance.impl\", \"glue\") \\\n    .config(\"spark.sql.catalog.lance.region\", \"us-east-1\") \\\n    .config(\"spark.sql.catalog.lance.catalog_id\", \"123456789012\") \\\n    .config(\"spark.sql.catalog.lance.access_key_id\", \"your-access-key\") \\\n    .config(\"spark.sql.catalog.lance.secret_access_key\", \"your-secret-key\") \\\n    .config(\"spark.sql.catalog.lance.root\", \"s3://your-bucket/lance\") \\\n    .getOrCreate()\n</code></pre> <pre><code>val spark = SparkSession.builder()\n    .appName(\"lance-glue-example\")\n    .config(\"spark.sql.catalog.lance\", \"org.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"glue\")\n    .config(\"spark.sql.catalog.lance.region\", \"us-east-1\")\n    .config(\"spark.sql.catalog.lance.catalog_id\", \"123456789012\")\n    .config(\"spark.sql.catalog.lance.access_key_id\", \"your-access-key\")\n    .config(\"spark.sql.catalog.lance.secret_access_key\", \"your-secret-key\")\n    .config(\"spark.sql.catalog.lance.root\", \"s3://your-bucket/lance\")\n    .getOrCreate()\n</code></pre> <pre><code>SparkSession spark = SparkSession.builder()\n    .appName(\"lance-glue-example\")\n    .config(\"spark.sql.catalog.lance\", \"org.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"glue\")\n    .config(\"spark.sql.catalog.lance.region\", \"us-east-1\")\n    .config(\"spark.sql.catalog.lance.catalog_id\", \"123456789012\")\n    .config(\"spark.sql.catalog.lance.access_key_id\", \"your-access-key\")\n    .config(\"spark.sql.catalog.lance.secret_access_key\", \"your-secret-key\")\n    .config(\"spark.sql.catalog.lance.root\", \"s3://your-bucket/lance\")\n    .getOrCreate();\n</code></pre>"},{"location":"integrations/spark/config/#additional-dependencies","title":"Additional Dependencies","text":"<p>Using the Glue namespace requires additional dependencies beyond the main Lance Spark bundle: - <code>lance-namespace-glue</code>: Lance Glue namespace implementation - AWS Glue related dependencies: The easiest way is to use <code>software.amazon.awssdk:bundle</code> which includes all necessary AWS SDK components, though you can specify individual dependencies if preferred</p> <p>Example with Spark Shell: <pre><code>spark-shell \\\n  --packages org.lance:lance-spark-bundle-3.5_2.12:0.0.7,org.lance:lance-namespace-glue:0.0.7,software.amazon.awssdk:bundle:2.20.0 \\\n  --conf spark.sql.catalog.lance=org.lance.spark.LanceNamespaceSparkCatalog \\\n  --conf spark.sql.catalog.lance.impl=glue \\\n  --conf spark.sql.catalog.lance.root=s3://your-bucket/lance\n</code></pre></p>"},{"location":"integrations/spark/config/#glue-configuration-parameters","title":"Glue Configuration Parameters","text":"Parameter Required Description <code>spark.sql.catalog.{name}.region</code> \u2717 AWS region for Glue operations (e.g., <code>us-east-1</code>). If not specified, derives from the default AWS region chain <code>spark.sql.catalog.{name}.catalog_id</code> \u2717 Glue catalog ID, defaults to the AWS account ID of the caller <code>spark.sql.catalog.{name}.endpoint</code> \u2717 Custom Glue service endpoint for connecting to compatible metastores <code>spark.sql.catalog.{name}.access_key_id</code> \u2717 AWS access key ID for static credentials <code>spark.sql.catalog.{name}.secret_access_key</code> \u2717 AWS secret access key for static credentials <code>spark.sql.catalog.{name}.session_token</code> \u2717 AWS session token for temporary credentials <code>spark.sql.catalog.{name}.root</code> \u2717 Storage root location (e.g., <code>s3://bucket/path</code>), defaults to current directory"},{"location":"integrations/spark/config/#apache-hive-namespace","title":"Apache Hive Namespace","text":"<p>Lance supports both Hive 2.x and Hive 3.x metastores for metadata management.</p>"},{"location":"integrations/spark/config/#hive-3x-namespace","title":"Hive 3.x Namespace","text":"PySparkScalaJava <pre><code>spark = SparkSession.builder \\\n    .appName(\"lance-hive3-example\") \\\n    .config(\"spark.sql.catalog.lance\", \"org.lance.spark.LanceNamespaceSparkCatalog\") \\\n    .config(\"spark.sql.catalog.lance.impl\", \"hive3\") \\\n    .config(\"spark.sql.catalog.lance.parent\", \"hive\") \\\n    .config(\"spark.sql.catalog.lance.hadoop.hive.metastore.uris\", \"thrift://metastore:9083\") \\\n    .config(\"spark.sql.catalog.lance.client.pool-size\", \"5\") \\\n    .config(\"spark.sql.catalog.lance.root\", \"hdfs://namenode:8020/lance\") \\\n    .getOrCreate()\n</code></pre> <pre><code>val spark = SparkSession.builder()\n    .appName(\"lance-hive3-example\")\n    .config(\"spark.sql.catalog.lance\", \"org.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"hive3\")\n    .config(\"spark.sql.catalog.lance.parent\", \"hive\")\n    .config(\"spark.sql.catalog.lance.hadoop.hive.metastore.uris\", \"thrift://metastore:9083\")\n    .config(\"spark.sql.catalog.lance.client.pool-size\", \"5\")\n    .config(\"spark.sql.catalog.lance.root\", \"hdfs://namenode:8020/lance\")\n    .getOrCreate()\n</code></pre> <pre><code>SparkSession spark = SparkSession.builder()\n    .appName(\"lance-hive3-example\")\n    .config(\"spark.sql.catalog.lance\", \"org.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"hive3\")\n    .config(\"spark.sql.catalog.lance.parent\", \"hive\")\n    .config(\"spark.sql.catalog.lance.hadoop.hive.metastore.uris\", \"thrift://metastore:9083\")\n    .config(\"spark.sql.catalog.lance.client.pool-size\", \"5\")\n    .config(\"spark.sql.catalog.lance.root\", \"hdfs://namenode:8020/lance\")\n    .getOrCreate();\n</code></pre>"},{"location":"integrations/spark/config/#hive-2x-namespace","title":"Hive 2.x Namespace","text":"PySparkScalaJava <pre><code>spark = SparkSession.builder \\\n    .appName(\"lance-hive2-example\") \\\n    .config(\"spark.sql.catalog.lance\", \"org.lance.spark.LanceNamespaceSparkCatalog\") \\\n    .config(\"spark.sql.catalog.lance.impl\", \"hive2\") \\\n    .config(\"spark.sql.catalog.lance.hadoop.hive.metastore.uris\", \"thrift://metastore:9083\") \\\n    .config(\"spark.sql.catalog.lance.client.pool-size\", \"3\") \\\n    .config(\"spark.sql.catalog.lance.root\", \"hdfs://namenode:8020/lance\") \\\n    .getOrCreate()\n</code></pre> <pre><code>val spark = SparkSession.builder()\n    .appName(\"lance-hive2-example\")\n    .config(\"spark.sql.catalog.lance\", \"org.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"hive2\")\n    .config(\"spark.sql.catalog.lance.hadoop.hive.metastore.uris\", \"thrift://metastore:9083\")\n    .config(\"spark.sql.catalog.lance.client.pool-size\", \"3\")\n    .config(\"spark.sql.catalog.lance.root\", \"hdfs://namenode:8020/lance\")\n    .getOrCreate()\n</code></pre> <pre><code>SparkSession spark = SparkSession.builder()\n    .appName(\"lance-hive2-example\")\n    .config(\"spark.sql.catalog.lance\", \"org.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"hive2\")\n    .config(\"spark.sql.catalog.lance.hadoop.hive.metastore.uris\", \"thrift://metastore:9083\")\n    .config(\"spark.sql.catalog.lance.client.pool-size\", \"3\")\n    .config(\"spark.sql.catalog.lance.root\", \"hdfs://namenode:8020/lance\")\n    .getOrCreate();\n</code></pre>"},{"location":"integrations/spark/config/#additional-dependencies_1","title":"Additional Dependencies","text":"<p>Using Hive namespaces requires additional JARs beyond the main Lance Spark bundle: - For Hive 2.x: <code>lance-namespace-hive2</code> - For Hive 3.x: <code>lance-namespace-hive3</code></p> <p>Example with Spark Shell for Hive 3.x: <pre><code>spark-shell \\\n  --packages org.lance:lance-spark-bundle-3.5_2.12:0.0.7,org.lance:lance-namespace-hive3:0.0.7 \\\n  --conf spark.sql.catalog.lance=org.lance.spark.LanceNamespaceSparkCatalog \\\n  --conf spark.sql.catalog.lance.impl=hive3 \\\n  --conf spark.sql.catalog.lance.hadoop.hive.metastore.uris=thrift://metastore:9083 \\\n  --conf spark.sql.catalog.lance.root=hdfs://namenode:8020/lance\n</code></pre></p>"},{"location":"integrations/spark/config/#hive-configuration-parameters","title":"Hive Configuration Parameters","text":"Parameter Required Description <code>spark.sql.catalog.{name}.hadoop.*</code> \u2717 Additional Hadoop configuration options, will override the default Hadoop configuration <code>spark.sql.catalog.{name}.client.pool-size</code> \u2717 Connection pool size for metastore clients (default: 3) <code>spark.sql.catalog.{name}.root</code> \u2717 Storage root location for Lance tables (default: current directory)"},{"location":"integrations/spark/config/#note-on-namespace-levels","title":"Note on Namespace Levels","text":"<p>Spark provides at least a 3 level hierarchy of catalog \u2192 multi-level namespace \u2192 table. Most users treat Spark as a 3 level hierarchy with 1 level namespace.</p>"},{"location":"integrations/spark/config/#for-namespaces-with-less-than-3-levels","title":"For Namespaces with Less Than 3 Levels","text":"<p>Since Lance allows a 2 level hierarchy of root namespace \u2192 table for namespaces like <code>DirectoryNamespace</code>, the <code>LanceNamespaceSparkCatalog</code> provides a configuration <code>single_level_ns</code> which puts an additional dummy level to match the Spark hierarchy and make it root namespace \u2192 dummy extra level \u2192 table.</p> <p>Currently, this is automatically set with <code>single_level_ns=default</code> for <code>DirectoryNamespace</code> and when <code>RestNamespace</code> if it cannot respond to <code>ListNamespaces</code> operation. If you have a custom namespace implementation of the same behavior, you can also set the config to add the extra level.</p>"},{"location":"integrations/spark/config/#for-namespaces-with-more-than-3-levels","title":"For Namespaces with More Than 3 Levels","text":"<p>Some namespace implementations like Hive3 and DirectoryNamespace support more than 3 levels of hierarchy. For example, Hive3 has a 4 level hierarchy: root metastore \u2192 catalog \u2192 database \u2192 table.</p> <p>To handle this, the <code>LanceNamespaceSparkCatalog</code> provides <code>parent</code> and <code>parent_delimiter</code> configurations which allow you to specify a parent prefix that gets prepended to all namespace operations.</p> <p>For example, with Hive3:</p> <ul> <li>Setting <code>parent=hive</code> (using default <code>parent_delimiter=$</code>)</li> <li>When Spark requests namespace <code>[\"database1\"]</code>, it gets transformed to <code>[\"hive$database1\"]</code> for the API call</li> <li>This allows the 4-level Hive 3 structure to work within Spark's 3-level model</li> </ul> <p>The parent configuration effectively \"anchors\" your Spark catalog at a specific level within the deeper namespace hierarchy, making the extra levels transparent to Spark users while maintaining compatibility with the underlying namespace implementation.</p>"},{"location":"integrations/spark/install/","title":"Install","text":""},{"location":"integrations/spark/install/#maven-central-packages","title":"Maven Central Packages","text":"<p>The connector packages are published to Maven Central under the <code>org.lance</code> namespace. Choose the appropriate artifact based on your use case:</p>"},{"location":"integrations/spark/install/#available-artifacts","title":"Available Artifacts","text":"Artifact Type Name Pattern Description Example Base Jar <code>lance-spark-base_&lt;scala_version&gt;</code> Jar with logic shared by different versions of Spark Lance connectors. lance-spark-base_2.12 Lean Jar <code>lance-spark-&lt;spark-version&gt;_&lt;scala_version&gt;</code> Jar with only the Spark Lance connector logic lance-spark-3.5_2.12 Bundled Jar <code>lance-spark-bundle-&lt;spark-version&gt;_&lt;scala_version&gt;</code> Jar with all necessary non-Spark dependencies lance-spark-bundle-3.5_2.12"},{"location":"integrations/spark/install/#choosing-the-right-artifact","title":"Choosing the Right Artifact","text":"<ul> <li>Bundled Jar: Recommended for most users. Use this if you want to quickly get started or use the connector in a Spark shell/notebook environment.</li> <li>Lean Jar: Use this if you're building a custom Spark application and want to manage and bundle dependencies yourself.</li> <li>Base Jar: Internal use only. Use this if you would like to build a custom Spark Lance connector with a different Spark version.</li> </ul>"},{"location":"integrations/spark/install/#dependency-configuration","title":"Dependency Configuration","text":""},{"location":"integrations/spark/install/#in-spark-application-code","title":"In Spark Application Code","text":"<p>Typically, you use the bundled jar in your Spark application as a provided (compile only) dependency. The actual jar is supplied to the Spark cluster separately. If you want to also include the bundled jar in your own bundle, remove the provided (compile only) annotation.</p> MavenGradlesbt <pre><code>&lt;!-- For Spark 3.5 (Scala 2.12) --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.lance&lt;/groupId&gt;\n    &lt;artifactId&gt;lance-spark-bundle-3.5_2.12&lt;/artifactId&gt;\n    &lt;version&gt;0.0.7&lt;/version&gt;\n    &lt;scope&gt;provided&lt;/scope&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>dependencies {\n    // For Spark 3.5 (Scala 2.12)\n    compileOnly 'org.lance:lance-spark-bundle-3.5_2.12:0.0.7'\n}\n</code></pre> <pre><code>libraryDependencies ++= Seq(\n  // For Spark 3.5 (Scala 2.12)\n  \"org.lance\" %% \"lance-spark-bundle-3.5_2.12\" % \"0.0.7\" % \"provided\"\n)\n</code></pre>"},{"location":"integrations/spark/install/#in-spark-cluster","title":"In Spark Cluster","text":"<p>You can either download the bundled jar dependency from Maven and add it to your Spark classpath, or supply the dependency dynamically to a Spark cluster through <code>--packages</code> flag. See Configuration for more details.</p>"},{"location":"integrations/spark/install/#requirements","title":"Requirements","text":""},{"location":"integrations/spark/install/#java","title":"Java","text":"Java Version Support Status Notes Java 8 \u274c Not Supported No longer supported Java 11 \u2705 Supported Minimum required version Java 17 \u2705 Supported Latest LTS version (Spark 4.0 dropped Java 8 and 11 support) Java 21+ \u26a0\ufe0f Untested May work but not officially tested"},{"location":"integrations/spark/install/#scala","title":"Scala","text":"Scala Version Support Status Notes Scala 2.12 \u2705 Supported Fully supported Scala 2.13 \u2705 Supported Fully supported Scala 3.x \u274c Not Supported Not currently planned"},{"location":"integrations/spark/install/#apache-spark","title":"Apache Spark","text":"Spark Version Support Status Notes Spark 4.0 \u2705 Supported Scala 2.13 only (Spark 4.0 dropped Scala 2.12 support) Spark 3.5 \u2705 Supported Scala 2.12 and 2.13 Spark 3.4 \u2705 Supported Scala 2.12 and 2.13 Spark 3.1, 3.2, 3.3 \u26a0\ufe0f Untested May work but not officially tested Spark 2.x \u274c Not Supported"},{"location":"integrations/spark/install/#operating-system","title":"Operating System","text":"Operating System Architecture Support Status Notes Linux x86_64 \u2705 Supported Linux ARM64 \u2705 Supported Including Apple Silicon via Rosetta macOS x86_64 \u2705 Supported Intel-based Macs macOS ARM64 \u2705 Supported Apple Silicon (M1/M2/M3) Windows x86_64 \ud83d\udea7 In Progress Support planned for future releases"},{"location":"integrations/spark/performance/","title":"Performance Tuning","text":"<p>This guide covers performance tuning for Lance Spark operations in large-scale ETL and batch processing scenarios.</p>"},{"location":"integrations/spark/performance/#understanding-lances-default-optimization","title":"Understanding Lance's Default Optimization","text":"<p>Lance is optimized by default for random access patterns - fast point lookups, vector searches, and selective column reads. These defaults work well for ML/AI workloads where you frequently access individual records or small batches.</p> <p>For large-scale batch ETL and scan-heavy OLAP operations (writing millions of rows, full table scans, bulk exports), you can tune Lance's environment variables and Spark options to better utilize available resources.</p>"},{"location":"integrations/spark/performance/#general-recommendations","title":"General Recommendations","text":"<p>For optimal performance with Lance, we recommend:</p> <ol> <li> <p>Use fixed-size lists for vector columns - Store embedding vectors as <code>ARRAY&lt;FLOAT&gt;</code> with a fixed dimension. This enables efficient SIMD operations and better compression. See Vector Columns.</p> </li> <li> <p>Use blob encoding for multimodal data - Store large binary data (images, audio, video) using Lance's unique blob encoding to enable efficient access. See Blob Columns.</p> </li> <li> <p>Use large varchar for large text - When storing very large string values for use cases like full text search, use the large varchar type to avoid out of memory issues. See Large String Columns.</p> </li> </ol>"},{"location":"integrations/spark/performance/#write-performance","title":"Write Performance","text":""},{"location":"integrations/spark/performance/#upload-concurrency","title":"Upload Concurrency","text":"<p>Set via environment variable <code>LANCE_UPLOAD_CONCURRENCY</code> (default: 10).</p> <p>Controls the number of concurrent multipart upload streams to S3.  Increasing this to match your CPU core count can improve throughput.</p> <pre><code>export LANCE_UPLOAD_CONCURRENCY=32\n</code></pre>"},{"location":"integrations/spark/performance/#upload-part-size","title":"Upload Part Size","text":"<p>Set via environment variable <code>LANCE_INITIAL_UPLOAD_SIZE</code> (default: 5MB).</p> <p>Controls the initial part size for S3 multipart uploads.  Larger part sizes reduce the number of API calls and can improve throughput for large writes.  However, larger part sizes use more memory and may increase latency for small writes.  Use the default for interactive workloads.</p> <p>Note</p> <p>Lance automatically increments the multipart upload size by 5MB every 100 uploads,   so large file writes progressively use increasingly large upload parts.   There is no configuration for a fixed upload size.</p> <pre><code>export LANCE_INITIAL_UPLOAD_SIZE=33554432  # 32MB\n</code></pre>"},{"location":"integrations/spark/performance/#queued-write-buffer-experimental","title":"Queued Write Buffer (Experimental)","text":"<p>Set via Spark write option <code>use_queued_write_buffer</code> (default: false).</p> <p>Enables a buffered write mode that improves throughput for large batch writes. When enabled, Lance uses a queue-based buffer instead of the default semaphore-based buffer that batches data more efficiently before writing to storage, avoiding small lock waiting operations between Spark producer and Lance writer.</p> <p>Note</p> <p>This feature is currently in experimental mode.   It will be set to true by default after the community considers it mature. </p> <pre><code>df.write \\\n    .format(\"lance\") \\\n    .option(\"use_queued_write_buffer\", \"true\") \\\n    .mode(\"append\") \\\n    .saveAsTable(\"my_table\")\n</code></pre>"},{"location":"integrations/spark/performance/#max-rows-per-file","title":"Max Rows Per File","text":"<p>Set via Spark write option <code>max_row_per_file</code> (default: 1,000,000).</p> <p>Controls the maximum number of rows per Lance fragment file. There is no specific recommended value, but be aware the default is 1 million rows. If you store many multimodal data columns (images, audio, embeddings) without using Lance blob encoding, or store a lot of long text columns, the file size might become very large. From Lance's perspective, having very large files does not impact your read performance. But you may want to reduce this value depending on the limits in your choice of object storage.</p> <pre><code>df.write \\\n    .format(\"lance\") \\\n    .option(\"max_row_per_file\", \"500000\") \\\n    .mode(\"append\") \\\n    .saveAsTable(\"my_table\")\n</code></pre>"},{"location":"integrations/spark/performance/#read-performance","title":"Read Performance","text":""},{"location":"integrations/spark/performance/#io-threads","title":"I/O Threads","text":"<p>Set via environment variable <code>LANCE_IO_THREADS</code> (default: 64).</p> <p>Controls the number of I/O threads used for parallel reads from storage.  For large scans, increasing this to match your CPU core count enables more concurrent S3 requests.</p> <pre><code>export LANCE_IO_THREADS=128\n</code></pre>"},{"location":"integrations/spark/performance/#batch-size","title":"Batch Size","text":"<p>Set via Spark read option <code>batch_size</code> (default: 512).</p> <p>Controls the number of rows per batch during vectorized reads. Lance uses a relatively small batch size for random access use cases. If you would like comparable performance to analytic formats like Iceberg or Parquet, they use a default of 5000 batch size.</p> <pre><code>df = spark.read \\\n    .option(\"batch_size\", \"5000\") \\\n    .table(\"my_table\") \\\n    .select(\"embedding\") \\\n    .collect()\n</code></pre>"},{"location":"integrations/spark/operations/ddl/create-index/","title":"CREATE INDEX","text":"<p>Creates a scalar index on a Lance table to accelerate queries.</p> <p>Spark Extension Required</p> <p>This feature requires the Lance Spark SQL extension to be enabled. See Spark SQL Extensions for configuration details.</p>"},{"location":"integrations/spark/operations/ddl/create-index/#overview","title":"Overview","text":"<p>The <code>CREATE INDEX</code> command builds an index on one or more columns of a Lance table. Indexing can improve the performance of queries that filter on the indexed columns. This operation is performed in a distributed manner, building indexes for each data fragment in parallel.</p>"},{"location":"integrations/spark/operations/ddl/create-index/#basic-usage","title":"Basic Usage","text":"<p>The command uses the <code>ALTER TABLE</code> syntax to add an index.</p> SQL <pre><code>ALTER TABLE lance.db.users CREATE INDEX user_id_idx USING btree (id);\n</code></pre>"},{"location":"integrations/spark/operations/ddl/create-index/#index-methods","title":"Index Methods","text":"<p>The following index methods are supported:</p> Method Description <code>btree</code> B-tree index for efficient range queries and point lookups on scalar columns. <code>fts</code> Full-text search (inverted) index for text search on string columns."},{"location":"integrations/spark/operations/ddl/create-index/#options","title":"Options","text":"<p>The <code>CREATE INDEX</code> command supports options via the <code>WITH</code> clause to control index creation. These options are specific to the chosen index method.</p>"},{"location":"integrations/spark/operations/ddl/create-index/#btree-options","title":"BTree Options","text":"<p>For the <code>btree</code> method, the following options are supported:</p> Option Type Description <code>zone_size</code> Long The number of rows per zone in the B-tree index."},{"location":"integrations/spark/operations/ddl/create-index/#fts-options","title":"FTS Options","text":"<p>For the <code>fts</code> method, the following options are required:</p> Option Type Description <code>base_tokenizer</code> String Tokenizer type: \"simple\" (whitespace/punctuation) or \"ngram\". <code>language</code> String Language for text processing (e.g., \"English\"). <code>max_token_length</code> Integer Maximum token length (e.g., 40). <code>lower_case</code> Boolean Convert text to lowercase. <code>stem</code> Boolean Enable stemming to reduce words to root form. <code>remove_stop_words</code> Boolean Remove common stop words from index. <code>ascii_folding</code> Boolean Normalize accented characters (e.g., '\u00e9' to 'e'). <code>with_position</code> Boolean Enable phrase queries. Increases index size. <p>For advanced tokenizer configuration, refer to the Lance FTS documentation.</p>"},{"location":"integrations/spark/operations/ddl/create-index/#examples","title":"Examples","text":""},{"location":"integrations/spark/operations/ddl/create-index/#basic-index-creation","title":"Basic Index Creation","text":"<p>Create a simple B-tree index on a single column:</p> SQL <pre><code>ALTER TABLE lance.db.users CREATE INDEX idx_id USING btree (id);\n</code></pre>"},{"location":"integrations/spark/operations/ddl/create-index/#indexing-multiple-columns","title":"Indexing Multiple Columns","text":"<p>Create a composite index on multiple columns.</p> SQL <pre><code>ALTER TABLE lance.db.logs CREATE INDEX idx_ts_level USING btree (timestamp, level);\n</code></pre>"},{"location":"integrations/spark/operations/ddl/create-index/#indexing-with-options","title":"Indexing with Options","text":"<p>Create an index and specify the <code>zone_size</code> for the B-tree:</p> SQL <pre><code>ALTER TABLE lance.db.users CREATE INDEX idx_id_zoned USING btree (id) WITH (zone_size = 2048);\n</code></pre>"},{"location":"integrations/spark/operations/ddl/create-index/#full-text-search-index","title":"Full-Text Search Index","text":"<p>Create an FTS index on a text column:</p> SQL <pre><code>ALTER TABLE lance.db.documents CREATE INDEX doc_fts USING fts (content) WITH (\n    base_tokenizer = 'simple',\n    language = 'English',\n    max_token_length = 40,\n    lower_case = true,\n    stem = false,\n    remove_stop_words = false,\n    ascii_folding = false,\n    with_position = true\n);\n</code></pre>"},{"location":"integrations/spark/operations/ddl/create-index/#output","title":"Output","text":"<p>The <code>CREATE INDEX</code> command returns the following information about the operation:</p> Column Type Description <code>fragments_indexed</code> Long The number of fragments that were indexed. <code>index_name</code> String The name of the created index."},{"location":"integrations/spark/operations/ddl/create-index/#when-to-use-an-index","title":"When to Use an Index","text":"<p>Consider creating an index when:</p> <ul> <li>You frequently filter a large table on a specific column.</li> <li>Your queries involve point lookups or small range scans.</li> </ul>"},{"location":"integrations/spark/operations/ddl/create-index/#how-it-works","title":"How It Works","text":"<p>The <code>CREATE INDEX</code> command operates as follows:</p> <ol> <li>Distributed Index Building: For each fragment in the Lance dataset, a separate task is launched to build an index on the specified column(s).</li> <li>Metadata Merging: Once all per-fragment indexes are built, their metadata is collected and merged.</li> <li>Transactional Commit: A new table version is committed with the new index information. The operation is atomic and ensures that concurrent reads are not affected.</li> </ol>"},{"location":"integrations/spark/operations/ddl/create-index/#notes-and-limitations","title":"Notes and Limitations","text":"<ul> <li>Index Methods: The <code>btree</code> and <code>fts</code> methods are supported for scalar index creation.</li> <li>Index Replacement: If you create an index with the same name as an existing one, the old index will be replaced by the new one. This is because the underlying implementation uses <code>replace(true)</code>.</li> </ul>"},{"location":"integrations/spark/operations/ddl/create-namespace/","title":"CREATE NAMESPACE","text":"<p>Create new child namespaces in the current Lance namespace.</p> <p>Create a simple namespace:</p> <pre><code>CREATE NAMESPACE company;\n</code></pre> <p>Create nested namespace:</p> <pre><code>CREATE NAMESPACE company.analytics;\n</code></pre> <p>Create namespace if it doesn't exist:</p> <pre><code>CREATE NAMESPACE IF NOT EXISTS company;\n</code></pre> <p>Create namespace with properties:</p> <pre><code>CREATE NAMESPACE company WITH PROPERTIES (\n    'description' = 'Company data namespace',\n    'owner' = 'data-team'\n);\n</code></pre>"},{"location":"integrations/spark/operations/ddl/create-table/","title":"CREATE TABLE","text":"<p>Create new Lance tables with SQL DDL statements or DataFrames.</p>"},{"location":"integrations/spark/operations/ddl/create-table/#basic-table-creation","title":"Basic Table Creation","text":"SQLPythonScalaJava <pre><code>CREATE TABLE users (\n    id BIGINT NOT NULL,\n    name STRING,\n    email STRING,\n    created_at TIMESTAMP\n);\n</code></pre> <pre><code># Create DataFrame\ndata = [\n(1, \"Alice\", \"alice@example.com\"),\n(2, \"Bob\", \"bob@example.com\"),\n(3, \"Charlie\", \"charlie@example.com\")\n]\ndf = spark.createDataFrame(data, [\"id\", \"name\", \"email\"])\n\n# Write as new table using catalog\ndf.writeTo(\"users\").create()\n</code></pre> <pre><code>import spark.implicits._\n\n// Create DataFrame\nval data = Seq(\n    (1, \"Alice\", \"alice@example.com\"),\n    (2, \"Bob\", \"bob@example.com\"),\n    (3, \"Charlie\", \"charlie@example.com\")\n)\nval df = data.toDF(\"id\", \"name\", \"email\")\n\n// Write as new table using catalog\ndf.writeTo(\"users\").create()\n</code></pre> <pre><code>import org.apache.spark.sql.types.*;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.RowFactory;\n\n// Create DataFrame\nList&lt;Row&gt; data = Arrays.asList(\n    RowFactory.create(1L, \"Alice\", \"alice@example.com\"),\n    RowFactory.create(2L, \"Bob\", \"bob@example.com\"),\n    RowFactory.create(3L, \"Charlie\", \"charlie@example.com\")\n);\n\nStructType schema = new StructType(new StructField[]{\n    new StructField(\"id\", DataTypes.LongType, false, Metadata.empty()),\n    new StructField(\"name\", DataTypes.StringType, true, Metadata.empty()),\n    new StructField(\"email\", DataTypes.StringType, true, Metadata.empty())\n});\n\nDataset&lt;Row&gt; df = spark.createDataFrame(data, schema);\n\n// Write as new table using catalog\ndf.writeTo(\"users\").create();\n</code></pre>"},{"location":"integrations/spark/operations/ddl/create-table/#complex-data-types","title":"Complex Data Types","text":"SQL <pre><code>CREATE TABLE events (\n    event_id BIGINT NOT NULL,\n    user_id BIGINT,\n    event_type STRING,\n    tags ARRAY&lt;STRING&gt;,\n    metadata STRUCT&lt;\n        source: STRING,\n        version: INT,\n        processed_at: TIMESTAMP\n    &gt;,\n    occurred_at TIMESTAMP\n);\n</code></pre>"},{"location":"integrations/spark/operations/ddl/create-table/#vector-columns","title":"Vector Columns","text":"<p>Lance supports vector (embedding) columns for AI workloads. These columns are stored internally as Arrow <code>FixedSizeList[n]</code> where <code>n</code> is the vector dimension. Since Spark SQL doesn't have a native fixed-size array type, you must use <code>ARRAY&lt;FLOAT&gt;</code> or <code>ARRAY&lt;DOUBLE&gt;</code> with table properties to specify the fixed dimension. The Lance-Spark connector will automatically convert these to the appropriate Arrow FixedSizeList format during write operations.</p>"},{"location":"integrations/spark/operations/ddl/create-table/#supported-types","title":"Supported Types","text":"<ul> <li>Element Types: <code>FLOAT</code> (float32), <code>DOUBLE</code> (float64)</li> <li>Requirements:<ul> <li>Vectors must be non-nullable</li> <li>All vectors in a column must have the same dimension</li> <li>Dimension is specified via table properties</li> </ul> </li> </ul>"},{"location":"integrations/spark/operations/ddl/create-table/#creating-vector-columns","title":"Creating Vector Columns","text":"<p>To create a table with vector columns, use the table property pattern <code>&lt;column_name&gt;.arrow.fixed-size-list.size</code> with the dimension as the value:</p> SQLPythonScalaJava <pre><code>CREATE TABLE embeddings_table (\n    id INT NOT NULL,\n    text STRING,\n    embeddings ARRAY&lt;FLOAT&gt; NOT NULL\n) USING lance\nTBLPROPERTIES (\n    'embeddings.arrow.fixed-size-list.size' = '128'\n);\n</code></pre> <pre><code>import numpy as np\n\n# Create DataFrame with vector data\ndata = [(i, np.random.rand(128).astype(np.float32).tolist()) for i in range(100)]\ndf = spark.createDataFrame(data, [\"id\", \"embeddings\"])\n\n# Write to Lance table with tableProperty\ndf.writeTo(\"embeddings_table\") \\\n    .tableProperty(\"embeddings.arrow.fixed-size-list.size\", \"128\") \\\n    .createOrReplace()\n</code></pre> <pre><code>import scala.util.Random\n\n// Create DataFrame with vector data\nval data = (0 until 100).map { i =&gt;\n  (i, Array.fill(128)(Random.nextFloat()))\n}\nval df = data.toDF(\"id\", \"embeddings\")\n\n// Write to Lance table with tableProperty\ndf.writeTo(\"embeddings_table\")\n  .tableProperty(\"embeddings.arrow.fixed-size-list.size\", \"128\")\n  .createOrReplace()\n</code></pre> <pre><code>// Create DataFrame with vector data\nList&lt;Row&gt; rows = new ArrayList&lt;&gt;();\nRandom random = new Random();\nfor (int i = 0; i &lt; 100; i++) {\n    float[] vector = new float[128];\n    for (int j = 0; j &lt; 128; j++) {\n        vector[j] = random.nextFloat();\n    }\n    rows.add(RowFactory.create(i, vector));\n}\n\nStructType schema = new StructType(new StructField[] {\n    DataTypes.createStructField(\"id\", DataTypes.IntegerType, false),\n    DataTypes.createStructField(\"embeddings\",\n        DataTypes.createArrayType(DataTypes.FloatType, false), false)\n});\n\nDataset&lt;Row&gt; df = spark.createDataFrame(rows, schema);\n\n// Write to Lance table with tableProperty\ndf.writeTo(\"embeddings_table\")\n    .tableProperty(\"embeddings.arrow.fixed-size-list.size\", \"128\")\n    .createOrReplace();\n</code></pre>"},{"location":"integrations/spark/operations/ddl/create-table/#blob-columns","title":"Blob Columns","text":"<p>Lance supports blob encoding for large binary data. Blob columns store large binary values (typically &gt; 64KB) out-of-line in a separate blob file, which improves query performance when not accessing the blob data directly.</p>"},{"location":"integrations/spark/operations/ddl/create-table/#supported-types_1","title":"Supported Types","text":"<ul> <li>Column Type: <code>BINARY</code></li> <li>Requirements:<ul> <li>Column must be nullable (blob data is not materialized when read, so nullability is required)</li> <li>Blob encoding is specified via table properties</li> </ul> </li> </ul>"},{"location":"integrations/spark/operations/ddl/create-table/#creating-blob-columns","title":"Creating Blob Columns","text":"<p>To create a table with blob columns, use the table property pattern <code>&lt;column_name&gt;.lance.encoding</code> with the value <code>'blob'</code>:</p> SQLPythonScalaJava <pre><code>CREATE TABLE documents (\n    id INT NOT NULL,\n    title STRING,\n    content BINARY\n) USING lance\nTBLPROPERTIES (\n    'content.lance.encoding' = 'blob'\n);\n</code></pre> <pre><code># Create DataFrame with binary data\ndata = [\n    (1, \"Document 1\", bytearray(b\"Large binary content...\" * 10000)),\n    (2, \"Document 2\", bytearray(b\"Another large file...\" * 10000))\n]\ndf = spark.createDataFrame(data, [\"id\", \"title\", \"content\"])\n\n# Write to Lance table with blob encoding\ndf.writeTo(\"documents\") \\\n    .tableProperty(\"content.lance.encoding\", \"blob\") \\\n    .createOrReplace()\n</code></pre> <pre><code>// Create DataFrame with binary data\nval data = Seq(\n  (1, \"Document 1\", Array.fill[Byte](1000000)(0x42)),\n  (2, \"Document 2\", Array.fill[Byte](1000000)(0x43))\n)\nval df = data.toDF(\"id\", \"title\", \"content\")\n\n// Write to Lance table with blob encoding\ndf.writeTo(\"documents\")\n  .tableProperty(\"content.lance.encoding\", \"blob\")\n  .createOrReplace()\n</code></pre> <pre><code>// Create DataFrame with binary data\nbyte[] largeData1 = new byte[1000000];\nbyte[] largeData2 = new byte[1000000];\nArrays.fill(largeData1, (byte) 0x42);\nArrays.fill(largeData2, (byte) 0x43);\n\nList&lt;Row&gt; data = Arrays.asList(\n    RowFactory.create(1, \"Document 1\", largeData1),\n    RowFactory.create(2, \"Document 2\", largeData2)\n);\n\nStructType schema = new StructType(new StructField[]{\n    DataTypes.createStructField(\"id\", DataTypes.IntegerType, false),\n    DataTypes.createStructField(\"title\", DataTypes.StringType, true),\n    DataTypes.createStructField(\"content\", DataTypes.BinaryType, true)\n});\n\nDataset&lt;Row&gt; df = spark.createDataFrame(data, schema);\n\n// Write to Lance table with blob encoding\ndf.writeTo(\"documents\")\n    .tableProperty(\"content.lance.encoding\", \"blob\")\n    .createOrReplace();\n</code></pre>"},{"location":"integrations/spark/operations/ddl/create-table/#large-string-columns","title":"Large String Columns","text":"<p>Lance supports large string columns for storing very large text data. By default, Arrow uses <code>Utf8</code> (VarChar) type with 32-bit offsets, which limits total string data to 2GB per batch. For columns containing very large strings (e.g., document content, base64-encoded data), you can use <code>LargeUtf8</code> (LargeVarChar) with 64-bit offsets.</p>"},{"location":"integrations/spark/operations/ddl/create-table/#when-to-use-large-strings","title":"When to Use Large Strings","text":"<p>Use large string columns when:</p> <ul> <li>Individual string values may exceed several MB</li> <li>Total string data per batch may exceed 2GB</li> <li>You encounter <code>OversizedAllocationException</code> errors during writes</li> </ul>"},{"location":"integrations/spark/operations/ddl/create-table/#creating-large-string-columns","title":"Creating Large String Columns","text":"<p>To create a table with large string columns, use the table property pattern <code>&lt;column_name&gt;.arrow.large_var_char</code> with the value <code>'true'</code>:</p> SQLPythonScalaJava <pre><code>CREATE TABLE articles (\n    id INT NOT NULL,\n    title STRING,\n    content STRING\n) USING lance\nTBLPROPERTIES (\n    'content.arrow.large_var_char' = 'true'\n);\n</code></pre> <pre><code># Create DataFrame with large string content\ndata = [\n    (1, \"Article 1\", \"Very long content...\" * 100000),\n    (2, \"Article 2\", \"Another long article...\" * 100000)\n]\ndf = spark.createDataFrame(data, [\"id\", \"title\", \"content\"])\n\n# Write to Lance table with large string support\ndf.writeTo(\"articles\") \\\n    .tableProperty(\"content.arrow.large_var_char\", \"true\") \\\n    .createOrReplace()\n</code></pre> <pre><code>// Create DataFrame with large string content\nval data = Seq(\n  (1, \"Article 1\", \"Very long content...\" * 100000),\n  (2, \"Article 2\", \"Another long article...\" * 100000)\n)\nval df = data.toDF(\"id\", \"title\", \"content\")\n\n// Write to Lance table with large string support\ndf.writeTo(\"articles\")\n  .tableProperty(\"content.arrow.large_var_char\", \"true\")\n  .createOrReplace()\n</code></pre> <pre><code>// Create DataFrame with large string content\nString largeContent1 = String.join(\"\", Collections.nCopies(100000, \"Very long content...\"));\nString largeContent2 = String.join(\"\", Collections.nCopies(100000, \"Another long article...\"));\n\nList&lt;Row&gt; data = Arrays.asList(\n    RowFactory.create(1, \"Article 1\", largeContent1),\n    RowFactory.create(2, \"Article 2\", largeContent2)\n);\n\nStructType schema = new StructType(new StructField[]{\n    DataTypes.createStructField(\"id\", DataTypes.IntegerType, false),\n    DataTypes.createStructField(\"title\", DataTypes.StringType, true),\n    DataTypes.createStructField(\"content\", DataTypes.StringType, true)\n});\n\nDataset&lt;Row&gt; df = spark.createDataFrame(data, schema);\n\n// Write to Lance table with large string support\ndf.writeTo(\"articles\")\n    .tableProperty(\"content.arrow.large_var_char\", \"true\")\n    .createOrReplace();\n</code></pre>"},{"location":"integrations/spark/operations/ddl/describe-namespace/","title":"DESCRIBE NAMESPACE","text":"<p>Display information about a namespace.</p> <p>Describe a namespace:</p> <pre><code>DESCRIBE NAMESPACE company;\n</code></pre> <p>Show extended namespace information:</p> <pre><code>DESCRIBE NAMESPACE EXTENDED company;\n</code></pre> <p>Describe nested namespace:</p> <pre><code>DESCRIBE NAMESPACE company.analytics;\n</code></pre>"},{"location":"integrations/spark/operations/ddl/describe-table/","title":"DESCRIBE TABLE","text":"<p>Inspect table structure and metadata.</p> <p>Describe table structure:</p> <pre><code>DESCRIBE TABLE users;\n</code></pre> <p>Show detailed table information:</p> <pre><code>DESCRIBE EXTENDED users;\n</code></pre>"},{"location":"integrations/spark/operations/ddl/drop-namespace/","title":"DROP NAMESPACE","text":"<p>Remove a child namespace from the current Lance namespace.</p> <p>Drop an empty namespace:</p> <pre><code>DROP NAMESPACE company;\n</code></pre> <p>Drop namespace if it exists (no error if it doesn't exist):</p> <pre><code>DROP NAMESPACE IF EXISTS company;\n</code></pre> <p>Drop namespace and all its contents (CASCADE):</p> <pre><code>DROP NAMESPACE company CASCADE;\n</code></pre> <p>Drop nested namespace:</p> <pre><code>DROP NAMESPACE company.analytics;\n</code></pre>"},{"location":"integrations/spark/operations/ddl/drop-table/","title":"DROP TABLE","text":"<p>Remove Lance tables from the catalog.</p> <p>Drop table (also removes data):</p> <pre><code>DROP TABLE users;\n</code></pre> <p>Drop table if it exists (no error if table doesn't exist):</p> <pre><code>DROP TABLE IF EXISTS users;\n</code></pre>"},{"location":"integrations/spark/operations/ddl/optimize/","title":"OPTIMIZE","text":"<p>Compact table fragments to improve query performance and reduce storage overhead.</p> <p>Spark Extension Required</p> <p>This feature requires the Lance Spark SQL extension to be enabled. See Spark SQL Extensions for configuration details.</p>"},{"location":"integrations/spark/operations/ddl/optimize/#overview","title":"Overview","text":"<p>Over time, as data is appended to Lance tables, the number of fragments (data files) can grow, which may impact query performance. The <code>OPTIMIZE</code> command compacts these fragments into larger, more efficient files.</p>"},{"location":"integrations/spark/operations/ddl/optimize/#basic-usage","title":"Basic Usage","text":"SQL <pre><code>OPTIMIZE lance.db.users;\n</code></pre>"},{"location":"integrations/spark/operations/ddl/optimize/#options","title":"Options","text":"<p>The <code>OPTIMIZE</code> command supports several options to control compaction behavior:</p> Option Type Description <code>target_rows_per_fragment</code> Long Target number of rows per fragment after compaction <code>max_rows_per_group</code> Long Maximum rows per row group within a fragment <code>max_bytes_per_file</code> Long Maximum bytes per data file <code>materialize_deletions</code> Boolean Whether to materialize soft deletes during compaction <code>materialize_deletions_threshold</code> Float Threshold ratio for materializing deletions <code>num_threads</code> Long Number of threads for compaction <code>batch_size</code> Long Batch size for processing <code>defer_index_remap</code> Boolean Whether to defer index remapping"},{"location":"integrations/spark/operations/ddl/optimize/#examples","title":"Examples","text":"<p>Optimize with a specific target rows per fragment:</p> SQL <pre><code>OPTIMIZE lance.db.users WITH (target_rows_per_fragment = 1000000);\n</code></pre> <p>Optimize with multiple options:</p> SQL <pre><code>OPTIMIZE lance.db.users WITH (\n    target_rows_per_fragment = 1000000,\n    max_rows_per_group = 10000,\n    materialize_deletions = TRUE,\n    num_threads = 4\n);\n</code></pre>"},{"location":"integrations/spark/operations/ddl/optimize/#output","title":"Output","text":"<p>The <code>OPTIMIZE</code> command returns statistics about the compaction operation:</p> Column Type Description <code>fragments_removed</code> Long Number of fragments removed <code>fragments_added</code> Long Number of new fragments created <code>files_removed</code> Long Number of data files removed <code>files_added</code> Long Number of new data files created"},{"location":"integrations/spark/operations/ddl/optimize/#when-to-optimize","title":"When to Optimize","text":"<p>Consider running OPTIMIZE when:</p> <ul> <li>Many small appends have created numerous fragments</li> <li>Query performance has degraded over time</li> <li>You want to reduce the number of files in storage</li> <li>After running many deletes (to materialize soft deletes)</li> </ul>"},{"location":"integrations/spark/operations/ddl/optimize/#how-it-works","title":"How It Works","text":"<p>Lance stores data in fragments. Each write operation (INSERT, append) creates new fragments. The OPTIMIZE command:</p> <ol> <li>Identifies small fragments that can be combined</li> <li>Merges them into larger, more efficient fragments</li> <li>Updates the table manifest to point to the new fragments</li> <li>Removes the old fragments</li> </ol> <p>This process is safe and does not block concurrent reads.</p>"},{"location":"integrations/spark/operations/ddl/show-namespaces/","title":"SHOW NAMESPACES","text":"<p>List available child namespaces.</p> <p>Show all namespaces:</p> <pre><code>SHOW NAMESPACES;\n</code></pre> <p>Show namespaces in a specific namespace:</p> <pre><code>SHOW NAMESPACES IN company;\n</code></pre> <p>Show namespaces with pattern matching:</p> <pre><code>SHOW NAMESPACES LIKE 'comp*';\n</code></pre>"},{"location":"integrations/spark/operations/ddl/show-tables/","title":"SHOW TABLES","text":"<p>Display available tables in Lance namespaces.</p> <p>Show all tables in the default namespace:</p> <pre><code>SHOW TABLES;\n</code></pre> <p>Show all tables in a specific namespace <code>ns2</code>:</p> <pre><code>SHOW TABLES IN ns2;\n</code></pre>"},{"location":"integrations/spark/operations/ddl/vacuum/","title":"VACUUM","text":"<p>Remove old versions and unreferenced data files from Lance tables to reclaim storage space.</p> <p>Spark Extension Required</p> <p>This feature requires the Lance Spark SQL extension to be enabled. See Spark SQL Extensions for configuration details.</p>"},{"location":"integrations/spark/operations/ddl/vacuum/#overview","title":"Overview","text":"<p>Lance maintains multiple versions of data for time travel and transactional consistency. Over time, old versions accumulate and can consume significant storage space. The <code>VACUUM</code> command removes old versions and their associated data files that are no longer needed.</p>"},{"location":"integrations/spark/operations/ddl/vacuum/#basic-usage","title":"Basic Usage","text":"SQL <pre><code>VACUUM lance.db.users;\n</code></pre>"},{"location":"integrations/spark/operations/ddl/vacuum/#options","title":"Options","text":"<p>The <code>VACUUM</code> command supports several options to control cleanup behavior:</p> Option Type Description <code>before_version</code> Long Remove versions older than this version number <code>before_timestamp_millis</code> Long Remove versions older than this timestamp (milliseconds since epoch) <code>delete_unverified</code> Boolean Whether to delete unverified files (default: false) <code>error_if_tagged_old_versions</code> Boolean Error if old versions have tags (default: false)"},{"location":"integrations/spark/operations/ddl/vacuum/#examples","title":"Examples","text":"<p>Remove versions older than a specific version:</p> SQL <pre><code>VACUUM lance.db.users WITH (before_version = 10);\n</code></pre> <p>Remove versions older than a specific timestamp:</p> SQL <pre><code>VACUUM lance.db.users WITH (before_timestamp_millis = 1704067200000);\n</code></pre> <p>Use multiple options:</p> SQL <pre><code>VACUUM lance.db.users WITH (\n    before_version = 10,\n    delete_unverified = TRUE\n);\n</code></pre>"},{"location":"integrations/spark/operations/ddl/vacuum/#output","title":"Output","text":"<p>The <code>VACUUM</code> command returns statistics about the cleanup operation:</p> Column Type Description <code>bytes_removed</code> Long Total bytes removed from storage <code>old_versions</code> Long Number of old versions removed"},{"location":"integrations/spark/operations/ddl/vacuum/#when-to-vacuum","title":"When to Vacuum","text":"<p>Consider running VACUUM when:</p> <ul> <li>Storage costs are increasing due to accumulated versions</li> <li>You no longer need historical versions for time travel</li> <li>After running many updates or deletes that created new versions</li> </ul>"},{"location":"integrations/spark/operations/ddl/vacuum/#important-notes","title":"Important Notes","text":"<ul> <li>Irreversible: VACUUM permanently deletes data. Removed versions cannot be recovered.</li> <li>Time travel: After vacuuming, you cannot time travel to removed versions.</li> <li>Tags: If you have tagged versions, use <code>error_if_tagged_old_versions</code> to prevent accidental deletion of important snapshots.</li> <li>Concurrent operations: VACUUM is safe to run alongside read operations but should not run concurrently with writes.</li> </ul>"},{"location":"integrations/spark/operations/dml/add-columns/","title":"ADD COLUMN FROM","text":"<p>Similar to most table formats, Lance supports traditional schema evolution:  adding, removing, and altering columns in a dataset.  Most of these operations can be performed without rewriting the data files in the dataset,  making them very efficient operations. </p> <p>In addition, Lance supports data evolution,  which allows you to also backfill existing rows with the new column data without rewriting the data files in the dataset,  making it highly suitable for use cases like ML feature engineering. This feature is implemented in Spark as <code>ALTER TABLE ADD COLUMN FROM</code></p> <p>Spark Extension Required</p> <p>This feature requires the Lance Spark SQL extension to be enabled.  See Spark SQL Extensions for configuration details.</p> <p>Example:</p> <pre><code>CREATE TEMPORARY VIEW tmp_view\nAS\nSELECT _rowaddr, _fragid, hash(name) as name_hash\nFROM users;\n\nALTER TABLE users ADD COLUMNS name_hash FROM tmp_view;\n</code></pre> <p>No table rewrite, no data movement\u2014just a new column that is instantly queryable.</p> <p>Note</p> <p>Because we use <code>_rowaddr</code> and <code>_fragid</code> to address the target dataset's rows for the new column's data,  the temporary view should contain <code>_rowaddr</code> and <code>_fragid</code>.</p>"},{"location":"integrations/spark/operations/dml/delete/","title":"DELETE FROM","text":"<p>Currently, delete only supports for Spark 3.5+.</p> <p>Delete with condition:</p> <pre><code>DELETE FROM users WHERE id = 4;\n</code></pre> <p>Delete all rows:</p> <pre><code>DELETE FROM users;\n</code></pre>"},{"location":"integrations/spark/operations/dml/insert-into/","title":"INSERT INTO","text":"<p>Add data to existing Lance tables using SQL or DataFrames.</p>"},{"location":"integrations/spark/operations/dml/insert-into/#basic-insert","title":"Basic Insert","text":"SQLPythonScalaJava <pre><code>INSERT INTO users VALUES\n    (4, 'David', 'david@example.com', '2024-01-15 10:30:00'),\n    (5, 'Eva', 'eva@example.com', '2024-01-15 11:45:00');\n</code></pre> <pre><code># Create new data\nnew_data = [\n    (8, \"Henry\", \"henry@example.com\"),\n    (9, \"Ivy\", \"ivy@example.com\")\n]\nnew_df = spark.createDataFrame(new_data, [\"id\", \"name\", \"email\"])\n\n# Append to existing table\nnew_df.writeTo(\"users\").append()\n\n# Alternative: use traditional write API with mode\nnew_df.write.mode(\"append\").saveAsTable(\"users\")\n</code></pre> <pre><code>// Create new data\nval newData = Seq(\n    (8, \"Henry\", \"henry@example.com\"),\n    (9, \"Ivy\", \"ivy@example.com\")\n)\nval newDF = newData.toDF(\"id\", \"name\", \"email\")\n\n// Append to existing table\nnewDF.writeTo(\"users\").append()\n\n// Alternative: use traditional write API with mode\nnewDF.write.mode(\"append\").saveAsTable(\"users\")\n</code></pre> <pre><code>// Create new data\nList&lt;Row&gt; newData = Arrays.asList(\n    RowFactory.create(8L, \"Henry\", \"henry@example.com\"),\n    RowFactory.create(9L, \"Ivy\", \"ivy@example.com\")\n);\nDataset&lt;Row&gt; newDF = spark.createDataFrame(newData, schema);\n\n// Append to existing table\nnewDF.writeTo(\"users\").append();\n\n// Alternative: use traditional write API with mode\nnewDF.write().mode(\"append\").saveAsTable(\"users\");\n</code></pre>"},{"location":"integrations/spark/operations/dml/insert-into/#insert-with-column-specification","title":"Insert with Column Specification","text":"SQL <pre><code>INSERT INTO users (id, name, email) VALUES\n    (6, 'Frank', 'frank@example.com'),\n    (7, 'Grace', 'grace@example.com');\n</code></pre>"},{"location":"integrations/spark/operations/dml/insert-into/#insert-from-select","title":"Insert from SELECT","text":"SQL <pre><code>INSERT INTO users\nSELECT user_id as id, username as name, email_address as email, signup_date as created_at\nFROM staging.user_signups\nWHERE signup_date &gt;= '2024-01-01';\n</code></pre>"},{"location":"integrations/spark/operations/dml/insert-into/#insert-with-complex-data-types","title":"Insert with Complex Data Types","text":"SQL <pre><code>INSERT INTO events VALUES (\n    1001,\n    123,\n    'page_view',\n    array('web', 'desktop'),\n    struct('web_app', 1, '2024-01-15 12:00:00'),\n    '2024-01-15 12:00:00'\n);\n</code></pre>"},{"location":"integrations/spark/operations/dml/insert-into/#writing-vector-data","title":"Writing Vector Data","text":"<p>If you created a table with the <code>arrow.fixed-size-list.size</code> property (see CREATE TABLE),  subsequent writes will automatically use <code>FixedSizeList</code>. No additional configuration is needed:</p> SQLPythonScalaJava <pre><code>-- Insert vector data (example with small vectors for clarity)\nINSERT INTO embeddings_table VALUES\n    (10, 'new text', array(0.1, 0.2, 0.3, ...)); -- 128 float values\n</code></pre> <pre><code># Table was created with: 'embeddings.arrow.fixed-size-list.size' = '128'\n# Subsequent writes automatically use FixedSizeList encoding\n\n# Plain schema WITHOUT metadata - it just works!\ndata = [(i, [float(j) for j in range(128)]) for i in range(10, 20)]\ndf = spark.createDataFrame(data, [\"id\", \"embeddings\"])\n\ndf.writeTo(\"embeddings_table\").append()\n</code></pre> <pre><code>// Table was created with: 'embeddings.arrow.fixed-size-list.size' = '128'\n// Subsequent writes automatically use FixedSizeList encoding\n\nval data = (10 until 20).map { i =&gt;\n  (i, Array.fill(128)(Random.nextFloat()))\n}\nval df = data.toDF(\"id\", \"embeddings\")\n\ndf.writeTo(\"embeddings_table\").append()\n</code></pre> <pre><code>// Table was created with: 'embeddings.arrow.fixed-size-list.size' = '128'\n// Subsequent writes automatically use FixedSizeList encoding\n\nList&lt;Row&gt; rows = new ArrayList&lt;&gt;();\nfor (int i = 10; i &lt; 20; i++) {\n    float[] vector = new float[128];\n    for (int j = 0; j &lt; 128; j++) {\n        vector[j] = random.nextFloat();\n    }\n    rows.add(RowFactory.create(i, vector));\n}\nDataset&lt;Row&gt; df = spark.createDataFrame(rows, schema);\n\ndf.writeTo(\"embeddings_table\").append();\n</code></pre>"},{"location":"integrations/spark/operations/dml/insert-into/#writing-blob-data","title":"Writing Blob Data","text":"<p>If you created a table with the <code>lance.encoding</code> property (see CREATE TABLE),  subsequent writes will automatically use blob encoding. No additional configuration is needed:</p> SQLPythonScalaJava <pre><code>-- Insert blob data (example with binary literals)\nINSERT INTO documents VALUES\n    (3, 'Document 3', X'48656C6C6F576F726C64');\n</code></pre> <pre><code># Table was created with: 'content.lance.encoding' = 'blob'\n# Subsequent writes automatically use blob encoding\n\n# Plain schema WITHOUT metadata - it just works!\ndata = [(i, f\"Document {i}\", bytearray(b\"Large content...\" * 10000)) for i in range(10, 20)]\ndf = spark.createDataFrame(data, [\"id\", \"title\", \"content\"])\n\ndf.writeTo(\"documents\").append()\n</code></pre> <pre><code>// Table was created with: 'content.lance.encoding' = 'blob'\n// Subsequent writes automatically use blob encoding\n\nval data = (10 until 20).map { i =&gt;\n  (i, s\"Document $i\", Array.fill[Byte](100000)(0x42))\n}\nval df = data.toDF(\"id\", \"title\", \"content\")\n\ndf.writeTo(\"documents\").append()\n</code></pre> <pre><code>// Table was created with: 'content.lance.encoding' = 'blob'\n// Subsequent writes automatically use blob encoding\n\nList&lt;Row&gt; data = new ArrayList&lt;&gt;();\nfor (int i = 10; i &lt; 20; i++) {\n    byte[] content = new byte[100000];\n    Arrays.fill(content, (byte) 0x42);\n    data.add(RowFactory.create(i, \"Document \" + i, content));\n}\nDataset&lt;Row&gt; df = spark.createDataFrame(data, schema);\n\ndf.writeTo(\"documents\").append();\n</code></pre>"},{"location":"integrations/spark/operations/dml/insert-into/#writing-large-string-data","title":"Writing Large String Data","text":"<p>If you created a table with the <code>arrow.large_var_char</code> property (see CREATE TABLE),  subsequent writes will automatically use <code>LargeVarCharVector</code>. No additional configuration is needed:</p> SQLPythonScalaJava <pre><code>-- Insert large string data\nINSERT INTO articles VALUES\n    (3, 'Article 3', 'Very long article content...');\n</code></pre> <pre><code># Table was created with: 'content.arrow.large_var_char' = 'true'\n# Subsequent writes automatically use large string encoding\n\ndata = [(i, f\"Article {i}\", \"Long content...\" * 100000) for i in range(10, 20)]\ndf = spark.createDataFrame(data, [\"id\", \"title\", \"content\"])\n\ndf.writeTo(\"articles\").append()\n</code></pre> <pre><code>// Table was created with: 'content.arrow.large_var_char' = 'true'\n// Subsequent writes automatically use large string encoding\n\nval data = (10 until 20).map { i =&gt;\n  (i, s\"Article $i\", \"Long content...\" * 100000)\n}\nval df = data.toDF(\"id\", \"title\", \"content\")\n\ndf.writeTo(\"articles\").append()\n</code></pre> <pre><code>// Table was created with: 'content.arrow.large_var_char' = 'true'\n// Subsequent writes automatically use large string encoding\n\nString longContent = String.join(\"\", Collections.nCopies(100000, \"Long content...\"));\nList&lt;Row&gt; data = new ArrayList&lt;&gt;();\nfor (int i = 10; i &lt; 20; i++) {\n    data.add(RowFactory.create(i, \"Article \" + i, longContent));\n}\nDataset&lt;Row&gt; df = spark.createDataFrame(data, schema);\n\ndf.writeTo(\"articles\").append();\n</code></pre>"},{"location":"integrations/spark/operations/dml/insert-into/#write-options","title":"Write Options","text":"<p>These options control how data is written to Lance datasets. They can be set using the <code>.option()</code> method when writing data.</p> Option Type Default Description <code>write_mode</code> String <code>append</code> Write mode: <code>append</code> to add to existing data, <code>overwrite</code> to replace existing data. <code>max_row_per_file</code> Integer - Maximum number of rows per Lance file. <code>max_rows_per_group</code> Integer - Maximum number of rows per row group within a file. <code>max_bytes_per_file</code> Long - Maximum size in bytes per Lance file. <code>data_storage_version</code> String - Lance file format version: <code>LEGACY</code> or <code>STABLE</code>. <code>batch_size</code> Integer <code>512</code> Number of rows per batch during writing. <code>use_queued_write_buffer</code> Boolean <code>false</code> Use pipelined write buffer for improved throughput. <code>queue_depth</code> Integer <code>8</code> Queue depth for pipelined writes (only used when <code>use_queued_write_buffer=true</code>)."},{"location":"integrations/spark/operations/dml/insert-into/#example-controlling-file-size","title":"Example: Controlling File Size","text":"PythonScalaJava <pre><code>df.write \\\n    .format(\"lance\") \\\n    .option(\"max_row_per_file\", \"100000\") \\\n    .option(\"max_rows_per_group\", \"10000\") \\\n    .save(\"/path/to/output.lance\")\n</code></pre> <pre><code>df.write\n    .format(\"lance\")\n    .option(\"max_row_per_file\", \"100000\")\n    .option(\"max_rows_per_group\", \"10000\")\n    .save(\"/path/to/output.lance\")\n</code></pre> <pre><code>df.write()\n    .format(\"lance\")\n    .option(\"max_row_per_file\", \"100000\")\n    .option(\"max_rows_per_group\", \"10000\")\n    .save(\"/path/to/output.lance\");\n</code></pre>"},{"location":"integrations/spark/operations/dml/insert-into/#example-using-stable-storage-format","title":"Example: Using Stable Storage Format","text":"PythonScala <pre><code>df.write \\\n    .format(\"lance\") \\\n    .option(\"data_storage_version\", \"STABLE\") \\\n    .save(\"/path/to/output.lance\")\n</code></pre> <pre><code>df.write\n    .format(\"lance\")\n    .option(\"data_storage_version\", \"STABLE\")\n    .save(\"/path/to/output.lance\")\n</code></pre>"},{"location":"integrations/spark/operations/dml/insert-into/#example-using-pipelined-writes","title":"Example: Using Pipelined Writes","text":"PythonScala <pre><code># Enable pipelined writes for improved throughput\ndf.write \\\n    .format(\"lance\") \\\n    .option(\"use_queued_write_buffer\", \"true\") \\\n    .option(\"queue_depth\", \"4\") \\\n    .save(\"/path/to/output.lance\")\n</code></pre> <pre><code>// Enable pipelined writes for improved throughput\ndf.write\n    .format(\"lance\")\n    .option(\"use_queued_write_buffer\", \"true\")\n    .option(\"queue_depth\", \"4\")\n    .save(\"/path/to/output.lance\")\n</code></pre>"},{"location":"integrations/spark/operations/dml/update/","title":"UPDATE SET","text":"<p>Currently, update only supports for Spark 3.5+.</p> <p>Update with condition:</p> <pre><code>UPDATE users \nSET name = 'Updated Name' \nWHERE id = 4;\n</code></pre> <p>Update with complex data types:</p> <pre><code>UPDATE events \nSET metadata = named_struct('source', 'ios', 'version', 1, 'processed_at', timestamp'2024-01-15 13:00:00') \nWHERE event_id = 1001;\n</code></pre> <p>Update struct's field:</p> <pre><code>UPDATE events \nSET metadata = named_struct('source', metadata.source, 'version', 2, 'processed_at', timestamp'2024-01-15 13:00:00') \nWHERE event_id = 1001;\n</code></pre> <p>Update array field:</p> <pre><code>UPDATE events\nSET tags = ARRAY('ios', 'mobile')\nWHERE event_id = 1001;\n</code></pre>"},{"location":"integrations/spark/operations/dql/select/","title":"SELECT","text":"<p>Query data from Lance tables using SQL or DataFrames.</p>"},{"location":"integrations/spark/operations/dql/select/#basic-queries","title":"Basic Queries","text":"SQLPythonScalaJava <pre><code>SELECT * FROM users;\n</code></pre> <pre><code># Load table as DataFrame\nusers_df = spark.table(\"users\")\n\n# Use DataFrame operations\nfiltered_users = users_df.filter(\"age &gt; 25\").select(\"name\", \"email\")\nfiltered_users.show()\n</code></pre> <pre><code>// Load table as DataFrame\nval usersDF = spark.table(\"users\")\n\n// Use DataFrame operations\nval filteredUsers = usersDF.filter(\"age &gt; 25\").select(\"name\", \"email\")\nfilteredUsers.show()\n</code></pre> <pre><code>// Load table as DataFrame\nDataset&lt;Row&gt; usersDF = spark.table(\"users\");\n\n// Use DataFrame operations\nDataset&lt;Row&gt; filteredUsers = usersDF.filter(\"age &gt; 25\").select(\"name\", \"email\");\nfilteredUsers.show();\n</code></pre>"},{"location":"integrations/spark/operations/dql/select/#select-specific-columns","title":"Select Specific Columns","text":"SQL <pre><code>SELECT id, name, email FROM users;\n</code></pre>"},{"location":"integrations/spark/operations/dql/select/#query-with-where-clause","title":"Query with WHERE Clause","text":"SQL <pre><code>SELECT * FROM users WHERE age &gt; 25;\n</code></pre>"},{"location":"integrations/spark/operations/dql/select/#aggregate-queries","title":"Aggregate Queries","text":"SQL <pre><code>SELECT department, COUNT(*) as employee_count\nFROM users\nGROUP BY department;\n</code></pre>"},{"location":"integrations/spark/operations/dql/select/#join-queries","title":"Join Queries","text":"SQL <pre><code>SELECT u.name, p.title\nFROM users u\nJOIN projects p ON u.id = p.user_id;\n</code></pre>"},{"location":"integrations/spark/operations/dql/select/#querying-blob-columns","title":"Querying Blob Columns","text":"<p>When querying tables with blob columns, the blob data itself is not materialized by default.  Instead, you can access blob metadata through virtual columns. For each blob column, Lance provides two virtual columns:</p> <ul> <li><code>&lt;column_name&gt;__blob_pos</code> - The byte position of the blob in the blob file</li> <li><code>&lt;column_name&gt;__blob_size</code> - The size of the blob in bytes</li> </ul> <p>These virtual columns can be used for:</p> <ul> <li>Monitoring blob storage statistics</li> <li>Filtering rows by blob size</li> <li>Implementing custom blob retrieval logic</li> <li>Verifying successful blob writes</li> </ul> SQLPythonScalaJava <pre><code>SELECT id, title, content__blob_pos, content__blob_size\nFROM documents\nWHERE id = 1;\n</code></pre> <pre><code># Read table with blob column\ndocuments_df = spark.table(\"documents\")\n\n# Access blob metadata using virtual columns\nblob_metadata = documents_df.select(\n    \"id\",\n    \"title\",\n    \"content__blob_pos\",\n    \"content__blob_size\"\n)\nblob_metadata.show()\n\n# Filter by blob size\nlarge_blobs = documents_df.filter(\"content__blob_size &gt; 1000000\")\nlarge_blobs.select(\"id\", \"title\", \"content__blob_size\").show()\n</code></pre> <pre><code>// Read table with blob column\nval documentsDF = spark.table(\"documents\")\n\n// Access blob metadata using virtual columns\nval blobMetadata = documentsDF.select(\n  \"id\",\n  \"title\",\n  \"content__blob_pos\",\n  \"content__blob_size\"\n)\nblobMetadata.show()\n\n// Filter by blob size\nval largeBlobs = documentsDF.filter(\"content__blob_size &gt; 1000000\")\nlargeBlobs.select(\"id\", \"title\", \"content__blob_size\").show()\n</code></pre> <pre><code>// Read table with blob column\nDataset&lt;Row&gt; documentsDF = spark.table(\"documents\");\n\n// Access blob metadata using virtual columns\nDataset&lt;Row&gt; blobMetadata = documentsDF.select(\n    \"id\",\n    \"title\",\n    \"content__blob_pos\",\n    \"content__blob_size\"\n);\nblobMetadata.show();\n\n// Filter by blob size\nDataset&lt;Row&gt; largeBlobs = documentsDF.filter(\"content__blob_size &gt; 1000000\");\nlargeBlobs.select(\"id\", \"title\", \"content__blob_size\").show();\n</code></pre>"},{"location":"integrations/spark/operations/dql/select/#read-options","title":"Read Options","text":"<p>These options control how data is read from Lance datasets. They can be set using the <code>.option()</code> method when reading data.</p> Option Type Default Description <code>batch_size</code> Integer <code>512</code> Number of rows to read per batch during scanning. Larger values may improve throughput but increase memory usage. <code>version</code> Integer Latest Specific dataset version to read. If not specified, reads the latest version. <code>block_size</code> Integer - Block size in bytes for reading data. <code>index_cache_size</code> Integer - Size of the index cache in number of entries. <code>metadata_cache_size</code> Integer - Size of the metadata cache in number of entries. <code>pushDownFilters</code> Boolean <code>true</code> Whether to push down filter predicates to the Lance reader for optimized scanning. <code>topN_push_down</code> Boolean <code>true</code> Whether to push down TopN (ORDER BY ... LIMIT) operations to Lance for optimized sorting."},{"location":"integrations/spark/operations/dql/select/#example-reading-with-options","title":"Example: Reading with Options","text":"SQLPythonScalaJava <pre><code>-- Read a specific version using table options\nSELECT * FROM lance.`/path/to/dataset.lance` VERSION AS OF 10;\n</code></pre> <pre><code># Reading with options\ndf = spark.read \\\n    .format(\"lance\") \\\n    .option(\"batch_size\", \"1024\") \\\n    .option(\"version\", \"5\") \\\n    .load(\"/path/to/dataset.lance\")\n</code></pre> <pre><code>// Reading with options\nval df = spark.read\n    .format(\"lance\")\n    .option(\"batch_size\", \"1024\")\n    .option(\"version\", \"5\")\n    .load(\"/path/to/dataset.lance\")\n</code></pre> <pre><code>// Reading with options\nDataset&lt;Row&gt; df = spark.read()\n    .format(\"lance\")\n    .option(\"batch_size\", \"1024\")\n    .option(\"version\", \"5\")\n    .load(\"/path/to/dataset.lance\");\n</code></pre>"},{"location":"integrations/spark/operations/dql/select/#example-tuning-batch-size-for-performance","title":"Example: Tuning Batch Size for Performance","text":"PythonScala <pre><code># Larger batch size for better throughput on large scans\ndf = spark.read \\\n    .format(\"lance\") \\\n    .option(\"batch_size\", \"4096\") \\\n    .load(\"/path/to/dataset.lance\")\n</code></pre> <pre><code>// Larger batch size for better throughput on large scans\nval df = spark.read\n    .format(\"lance\")\n    .option(\"batch_size\", \"4096\")\n    .load(\"/path/to/dataset.lance\")\n</code></pre>"},{"location":"quickstart/","title":"Getting Started with Lance Tables","text":"<p>This quickstart guide will walk you through the core features of Lance including creating datasets, versioning, and vector search.</p> <p>By the end of this tutorial, you'll be able to create Lance datasets from pandas DataFrames and convert existing Parquet files to Lance format. You'll also understand the basic workflow for working with Lance datasets and be prepared to explore advanced features like versioning and vector search.</p>"},{"location":"quickstart/#install-the-python-sdk","title":"Install the Python SDK","text":"<p>The easiest way to get started with Lance is via our Python SDK <code>pylance</code>:</p> <pre><code>pip install pylance\n</code></pre> <p>For the latest features and bug fixes, you can install the preview version:</p> <pre><code>pip install --pre --extra-index-url https://pypi.fury.io/lance-format/pylance\n</code></pre> <p>Note: Preview releases receive the same level of testing as regular releases.</p> <p>Note: For versions prior to 1.0.0-beta.4, you can find them at https://pypi.fury.io/lancedb/pylance</p>"},{"location":"quickstart/#set-up-your-environment","title":"Set Up Your Environment","text":"<p>First, let's import the necessary libraries:</p> <pre><code>import shutil\nimport lance\nimport numpy as np\nimport pandas as pd\nimport pyarrow as pa\n</code></pre>"},{"location":"quickstart/#create-your-first-dataset","title":"Create Your First Dataset","text":"<p>Lance is built on top of Apache Arrow, making it incredibly easy to work with pandas DataFrames and Arrow tables. You can create Lance datasets from various data sources including pandas DataFrames, Arrow tables, and existing Parquet files. Lance automatically handles the conversion and optimization for you.</p>"},{"location":"quickstart/#create-a-simple-dataset","title":"Create a Simple Dataset","text":"<p>You'll create a simple dataframe and then write it to Lance format. This demonstrates the basic workflow for creating Lance datasets.</p> <p>Create a simple dataframe:</p> <pre><code>df = pd.DataFrame({\"a\": [5]})\ndf\n</code></pre> <p>Now you'll write this dataframe to Lance format and verify the data was saved correctly:</p> <pre><code>shutil.rmtree(\"/tmp/test.lance\", ignore_errors=True)\n\ndataset = lance.write_dataset(df, \"/tmp/test.lance\")\ndataset.to_table().to_pandas()\n</code></pre>"},{"location":"quickstart/#convert-your-existing-parquet-files","title":"Convert Your Existing Parquet Files","text":"<p>You'll convert an existing Parquet file to Lance format. This shows how to migrate your existing data to Lance.</p> <p>First, you'll create a Parquet file and then convert it to Lance:</p> <pre><code>shutil.rmtree(\"/tmp/test.parquet\", ignore_errors=True)\nshutil.rmtree(\"/tmp/test.lance\", ignore_errors=True)\n\ntbl = pa.Table.from_pandas(df)\npa.dataset.write_dataset(tbl, \"/tmp/test.parquet\", format='parquet')\n\nparquet = pa.dataset.dataset(\"/tmp/test.parquet\")\nparquet.to_table().to_pandas()\n</code></pre> <p>Now you'll convert the Parquet dataset to Lance format in a single line:</p> <pre><code>dataset = lance.write_dataset(parquet, \"/tmp/test.lance\")\n\n# Make sure it's the same\ndataset.to_table().to_pandas()\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<p>Now that you've mastered the basics of creating Lance datasets, here's what you can explore next:</p> <ul> <li>Versioning Your Datasets with Lance - Learn how to track changes over time with native versioning</li> <li>Vector Indexing and Vector Search With Lance - Build high-performance vector search capabilities with ANN indexes</li> </ul>"},{"location":"quickstart/vector-search/","title":"Vector Indexing and Vector Search With Lance","text":"<p>Lance provides high-performance vector search capabilities with ANN (Approximate Nearest Neighbor) indexes.</p> <p>By the end of this tutorial, you'll be able to build and use ANN indexes to dramatically speed up vector search operations while maintaining high accuracy. You'll also learn how to tune search parameters for optimal performance and combine vector search with metadata queries in a single operation.</p>"},{"location":"quickstart/vector-search/#install-the-python-sdk","title":"Install the Python SDK","text":"<pre><code>pip install pylance\n</code></pre>"},{"location":"quickstart/vector-search/#set-up-your-environment","title":"Set Up Your Environment","text":"<p>First, import the necessary libraries:</p> <pre><code>import shutil\nimport lance\nimport numpy as np\nimport pandas as pd\nimport pyarrow as pa\nimport duckdb\n</code></pre>"},{"location":"quickstart/vector-search/#prepare-your-vector-embeddings","title":"Prepare Your Vector Embeddings","text":"<p>For this tutorial, download and prepare the SIFT 1M dataset for vector search experiments.</p> <ul> <li>Download <code>ANN_SIFT1M</code> from: http://corpus-texmex.irisa.fr/</li> <li>Direct link: <code>ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz</code></li> </ul> <p>You can just use <code>wget</code>:</p> <pre><code>rm -rf sift* vec_data.lance\nwget ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz\ntar -xzf sift.tar.gz\n</code></pre>"},{"location":"quickstart/vector-search/#convert-your-data-to-lance-format","title":"Convert Your Data to Lance Format","text":"<p>Then, convert the raw vector data into Lance format for efficient storage and querying.</p> <pre><code>from lance.vector import vec_to_table\nimport struct\n\nuri = \"vec_data.lance\"\n\nwith open(\"sift/sift_base.fvecs\", mode=\"rb\") as fobj:\n    buf = fobj.read()\n    data = np.array(struct.unpack(\"&lt;128000000f\", buf[4 : 4 + 4 * 1000000 * 128])).reshape((1000000, 128))\n    dd = dict(zip(range(1000000), data))\n\ntable = vec_to_table(dd)\nlance.write_dataset(table, uri, max_rows_per_group=8192, max_rows_per_file=1024*1024)\n</code></pre> <p>Now you can load the dataset:</p> <pre><code>uri = \"vec_data.lance\"\nsift1m = lance.dataset(uri)\n</code></pre>"},{"location":"quickstart/vector-search/#search-without-an-index","title":"Search Without an Index","text":"<p>You'll perform vector search without an index to see the baseline performance, then compare it with indexed search.</p> <p>First, let's sample some query vectors:</p> <pre><code>import duckdb\n# Make sure DuckDB v0.7+ is installed\nsamples = duckdb.query(\"SELECT vector FROM sift1m USING SAMPLE 100\").to_df().vector\n</code></pre> <pre><code>0     [29.0, 10.0, 1.0, 50.0, 7.0, 89.0, 95.0, 51.0,...\n1     [7.0, 5.0, 39.0, 49.0, 17.0, 12.0, 83.0, 117.0...\n2     [0.0, 0.0, 0.0, 10.0, 12.0, 31.0, 6.0, 0.0, 0....\n3     [0.0, 2.0, 9.0, 1.793662034335766e-43, 30.0, 1...\n4     [54.0, 112.0, 16.0, 0.0, 0.0, 7.0, 112.0, 44.0...\n                            ...\n95    [1.793662034335766e-43, 33.0, 47.0, 28.0, 0.0,...\n96    [1.0, 4.0, 2.0, 32.0, 3.0, 7.0, 119.0, 116.0, ...\n97    [17.0, 46.0, 12.0, 0.0, 0.0, 3.0, 23.0, 58.0, ...\n98    [0.0, 11.0, 30.0, 14.0, 34.0, 7.0, 0.0, 0.0, 1...\n99    [20.0, 8.0, 121.0, 98.0, 37.0, 77.0, 9.0, 18.0...\nName: vector, Length: 100, dtype: object\n</code></pre> <p>Now, perform nearest neighbor search without an index:</p> <pre><code>import time\n\nstart = time.time()\ntbl = sift1m.to_table(columns=[\"id\"], nearest={\"column\": \"vector\", \"q\": samples[0], \"k\": 10})\nend = time.time()\n\nprint(f\"Time(sec): {end-start}\")\nprint(tbl.to_pandas())\n</code></pre> <p>Expected output: <pre><code>Time(sec): 0.10735273361206055\n       id                                             vector    score\n0  144678  [29.0, 10.0, 1.0, 50.0, 7.0, 89.0, 95.0, 51.0,...      0.0\n1  575538  [2.0, 0.0, 1.0, 42.0, 3.0, 38.0, 152.0, 27.0, ...  76908.0\n2  241428  [11.0, 0.0, 2.0, 118.0, 11.0, 108.0, 116.0, 21...  92877.0\n...\n</code></pre></p> <p>Without the index, the search will scan throughout the whole dataset to compute the distance between each data point. For practical real-time performance with, you will get much better performance with an ANN index.</p>"},{"location":"quickstart/vector-search/#build-the-search-index","title":"Build the Search Index","text":"<p>If you build an ANN index - you can dramatically speed up vector search operations while maintaining high accuracy. In this example, we will build the <code>IVF_PQ</code> index: </p> <pre><code>sift1m.create_index(\n    \"vector\",\n    index_type=\"IVF_PQ\", # specify the IVF_PQ index type\n    num_partitions=256,  # IVF\n    num_sub_vectors=16,  # PQ\n)\n</code></pre> <p>The sample response should look like this:</p> <pre><code>Building vector index: IVF256,PQ16\nCPU times: user 2min 23s, sys: 2.77 s, total: 2min 26s\nWall time: 22.7 s\nSample 65536 out of 1000000 to train kmeans of 128 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\n</code></pre> <p>Index Creation Performance</p> <p>If you're trying this on your own data, make sure your vector (dimensions / num_sub_vectors) % 8 == 0, or else index creation will take much longer than expected due to SIMD misalignment.</p>"},{"location":"quickstart/vector-search/#vector-search-with-the-ann-index","title":"Vector Search with the ANN Index","text":"<p>You can now perform the same search operation using your newly created index and see the dramatic performance improvement.</p> <pre><code>sift1m = lance.dataset(uri)\n\nimport time\n\ntot = 0\nfor q in samples:\n    start = time.time()\n    tbl = sift1m.to_table(nearest={\"column\": \"vector\", \"q\": q, \"k\": 10})\n    end = time.time()\n    tot += (end - start)\n\nprint(f\"Avg(sec): {tot / len(samples)}\")\nprint(tbl.to_pandas())\n</code></pre> <p>Expected output: <pre><code>Avg(sec): 0.0009334301948547364\n       id                                             vector         score\n0  378825  [20.0, 8.0, 121.0, 98.0, 37.0, 77.0, 9.0, 18.0...  16560.197266\n1  143787  [11.0, 24.0, 122.0, 122.0, 53.0, 4.0, 0.0, 3.0...  61714.941406\n2  356895  [0.0, 14.0, 67.0, 122.0, 83.0, 23.0, 1.0, 0.0,...  64147.218750\n3  535431  [9.0, 22.0, 118.0, 118.0, 4.0, 5.0, 4.0, 4.0, ...  69092.593750\n4  308778  [1.0, 7.0, 48.0, 123.0, 73.0, 36.0, 8.0, 4.0, ...  69131.812500\n5  222477  [14.0, 73.0, 39.0, 4.0, 16.0, 94.0, 19.0, 8.0,...  69244.195312\n6  672558  [2.0, 1.0, 0.0, 11.0, 36.0, 23.0, 7.0, 10.0, 0...  70264.828125\n7  365538  [54.0, 43.0, 97.0, 59.0, 34.0, 17.0, 10.0, 15....  70273.710938\n8  659787  [10.0, 9.0, 23.0, 121.0, 38.0, 26.0, 38.0, 9.0...  70374.703125\n9  603930  [32.0, 32.0, 122.0, 122.0, 70.0, 4.0, 15.0, 12...  70583.375000\n</code></pre></p> <p>Performance Note</p> <p>Your actual numbers will vary by your storage. These numbers are from local disk on an M2 MacBook Air. If you're querying S3 directly, HDD, or network drives, performance will be slower.</p>"},{"location":"quickstart/vector-search/#tune-the-search-parameters","title":"Tune the Search Parameters","text":"<p>You need to adjust search parameters to balance between speed and accuracy, finding the optimal settings for your use case.</p> <p>The latency vs recall is tunable via: - nprobes: how many IVF partitions to search - refine_factor: determines how many vectors are retrieved during re-ranking</p> <pre><code>%%time\n\nsift1m.to_table(\n    nearest={\n        \"column\": \"vector\",\n        \"q\": samples[0],\n        \"k\": 10,\n        \"nprobes\": 10,\n        \"refine_factor\": 5,\n    }\n).to_pandas()\n</code></pre> <p>Parameter Explanation: - <code>q</code> =&gt; sample vector - <code>k</code> =&gt; how many neighbors to return - <code>nprobes</code> =&gt; how many partitions (in the coarse quantizer) to probe - <code>refine_factor</code> =&gt; controls \"re-ranking\". If k=10 and refine_factor=5 then retrieve 50 nearest neighbors by ANN and re-sort using actual distances then return top 10. This improves recall without sacrificing performance too much</p> <p>Memory Usage</p> <p>The latencies above include file I/O as Lance currently doesn't hold anything in memory. Along with index building speed, creating a purely in-memory version of the dataset would make the biggest impact on performance.</p>"},{"location":"quickstart/vector-search/#combine-features-and-vectors","title":"Combine Features and Vectors","text":"<p>You can add metadata columns to your vector dataset and query both vectors and features together in a single operation.</p> <p>In real-life situations, users have other feature or metadata columns that need to be stored and fetched together. If you're managing data and the index separately, you have to do a bunch of annoying plumbing to put stuff together. </p> <p>With Lance, you can add columns directly to the dataset using <code>add_columns()</code>. For basic use cases, you can use SQL:</p> <p><pre><code>sift1m.add_columns(\n    {\n        \"item_id\": \"id + 1000000\",\n        \"revenue\": \"random() * 1000 + 5000\",\n    }\n)\n</code></pre> For more complex columns, you can provide a Python function to generate the new column data: <pre><code>@lance.batch_udf()\ndef add_columns_func(batch: pa.Table) -&gt; pd.DataFrame:\n    \"\"\"Add item_id and revenue columns to a batch of data.\n\n    Args:\n        batch: PyArrow Table containing the original data\n\n    Returns:\n        Pandas DataFrame with added item_id and revenue columns\n    \"\"\"\n    item_ids: np.ndarray = np.arange(batch.num_rows)\n    revenue: np.ndarray = (np.random.randn(batch.num_rows) + 5) * 1000\n    return pd.DataFrame({\"item_id\": item_ids, \"revenue\": revenue})\n\n\nsift1m.add_columns(add_columns_func)\n</code></pre> You can then query both vectors and metadata together:</p> <pre><code># Get vectors and metadata together\nresult = sift1m.to_table(\n    columns=[\"item_id\", \"revenue\"],\n    nearest={\"column\": \"vector\", \"q\": samples[0], \"k\": 10}\n)\nprint(result.to_pandas())\n</code></pre>"},{"location":"quickstart/vector-search/#next-steps","title":"Next Steps","text":"<p>You should check out Versioning Your Datasets with Lance. We'll show you how to version your vector datasets and track changes over time.</p>"},{"location":"quickstart/versioning/","title":"Versioning Your Datasets with Lance","text":"<p>Lance supports versioning natively, allowing you to track changes over time. </p> <p>In this tutorial, you'll learn how to append new data to existing datasets while preserving historical versions and access specific versions using version numbers or meaningful tags. You'll also understand how to implement proper data governance practices with Lance's native versioning capabilities.</p>"},{"location":"quickstart/versioning/#install-the-python-sdk","title":"Install the Python SDK","text":"<pre><code>pip install pylance\n</code></pre>"},{"location":"quickstart/versioning/#set-up-your-environment","title":"Set Up Your Environment","text":"<p>First, you should import the necessary libraries:</p> <pre><code>import shutil\nimport lance\nimport numpy as np\nimport pandas as pd\nimport pyarrow as pa\n</code></pre>"},{"location":"quickstart/versioning/#append-new-data-to-your-dataset","title":"Append New Data to Your Dataset","text":"<p>You can add new rows to your existing dataset, creating a new version while preserving the original data. Here is how to append rows:</p> <pre><code>df = pd.DataFrame({\"a\": [10]})\ntbl = pa.Table.from_pandas(df)\ndataset = lance.write_dataset(tbl, \"/tmp/test.lance\", mode=\"append\")\n\ndataset.to_table().to_pandas()\n</code></pre>"},{"location":"quickstart/versioning/#overwrite-your-dataset","title":"Overwrite Your Dataset","text":"<p>You can completely replace your dataset with new data, creating a new version while keeping the old version accessible.</p> <p>Here is how to overwrite the data and create a new version:</p> <pre><code>df = pd.DataFrame({\"a\": [50, 100]})\ntbl = pa.Table.from_pandas(df)\ndataset = lance.write_dataset(tbl, \"/tmp/test.lance\", mode=\"overwrite\")\n\ndataset.to_table().to_pandas()\n</code></pre>"},{"location":"quickstart/versioning/#access-previous-dataset-versions","title":"Access Previous Dataset Versions","text":"<p>You can also check what versions are available and then access specific versions of your dataset.</p> <p>List all versions of a dataset with this request:</p> <pre><code>dataset.versions()\n</code></pre> <p>You can also access any available version:</p> <pre><code># Version 1\nlance.dataset('/tmp/test.lance', version=1).to_table().to_pandas()\n\n# Version 2\nlance.dataset('/tmp/test.lance', version=2).to_table().to_pandas()\n</code></pre>"},{"location":"quickstart/versioning/#tag-your-important-versions","title":"Tag Your Important Versions","text":"<p>Create named tags for important versions, making it easier to reference specific versions by meaningful names. To create tags for relevant versions, do this:</p> <pre><code>dataset.tags.create(\"stable\", 2)\ndataset.tags.create(\"nightly\", 3)\ndataset.tags.list()\n</code></pre> <p>Tags can be checked out like versions:</p> <pre><code>lance.dataset('/tmp/test.lance', version=\"stable\").to_table().to_pandas()\n</code></pre>"},{"location":"quickstart/versioning/#next-steps","title":"Next Steps","text":"<p>Now that you've mastered dataset versioning with Lance, check out Vector Indexing and Vector Search With Lance. You can learn how to build high-performance vector search capabilities on top of your Lance tables.</p> <p>This will teach you how to build fast, scalable search capabilities for your versioned datasets. </p>"}]}